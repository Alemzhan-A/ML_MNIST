{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2WZJ7vl0gafx+XDC/CNrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alemzhan-A/ML_MNIST/blob/main/MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1) Скачивание датасета mnist\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTUW01P-ybQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hlo8Kah_VY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = tv.datasets.MNIST(root='mnist',train=True, download=True,transform = tv.transforms.ToTensor())\n",
        "mnist_test = tv.datasets.MNIST(root='mnist',train=False, download=True,transform = tv.transforms.ToTensor())\n",
        "plt.imshow(mnist_train[120][0].numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "5TNuawPh2va-",
        "outputId": "a7768651-4b7e-482d-e553-559aafd14727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 128195066.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 96215801.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46290487.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21261750.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7912619a3040>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(mnist_train, batch_size=50, shuffle=True)\n",
        "for i, (X, y) in enumerate(dataloader):\n",
        "    print(f'Batch {i}')\n",
        "    print(f'X.shape = {X.shape}')\n",
        "    print(f'y.shape = {y.shape}')\n",
        "    break\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9wrpINZ2kQe",
        "outputId": "c69da9b0-7775-4b67-f0fc-ba6618236805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0\n",
            "X.shape = torch.Size([50, 1, 28, 28])\n",
            "y.shape = torch.Size([50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Напишите модель, подобную модели с первой части семинара."
      ],
      "metadata": {
        "id": "IB4ufDg_B8in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.linear2 = nn.Linear(100, 10)\n",
        "    self.act = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear2(out)\n",
        "\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "15jBsgYs63RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_count = 0\n",
        "for param in model.parameters():\n",
        "    params_count += torch.numel(param)\n",
        "params_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8T3TMipZQay",
        "outputId": "6870ddfb-3965-4cb8-d1a3-f110052df23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79510"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Выведите summary модели."
      ],
      "metadata": {
        "id": "WIjW5ouWCKcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda')\n",
        "summary(model,(1,28,28))"
      ],
      "metadata": {
        "id": "E-VTv8otccWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1f8ea3-a6ae-485c-d14f-cc2d79987953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 100]          78,500\n",
            "           Sigmoid-3                  [-1, 100]               0\n",
            "            Linear-4                   [-1, 10]           1,010\n",
            "================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.30\n",
            "Estimated Total Size (MB): 0.31\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Обучите модель, добавив коллбек построения графиков обучения, напишите выводы: сошлась ли модель по итогам обучения."
      ],
      "metadata": {
        "id": "A5OflTAlCW-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "metadata": {
        "id": "b-WvqydvdiX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model, dataloader, loss_function, optimizer=None):\n",
        "    if optimizer == None:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        pred = model(X)\n",
        "        loss = loss_function(pred, y)\n",
        "        total_loss += loss.item()\n",
        "        if optimizer != None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "037-zcI6eKaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_losses(train_loss_hist, test_loss_hist):\n",
        "    clear_output()\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title('Train Loss')\n",
        "    plt.plot(np.arange(len(train_loss_hist)), train_loss_hist)\n",
        "    plt.yscale('log')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title('Test Loss')\n",
        "    plt.plot(np.arange(len(test_loss_hist)), test_loss_hist)\n",
        "    plt.yscale('log')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bfPb2gS6-WZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 160\n",
        "\n",
        "# your code here\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "    train_loss = run(model, train_loader, loss_fn, optimizer)\n",
        "    train_loss_hist.append(train_loss)\n",
        "    test_loss = run(model, test_loader, loss_fn)\n",
        "    test_loss_hist.append(test_loss)\n",
        "\n",
        "    if i % 10 == 9:\n",
        "        show_losses(train_loss_hist, test_loss_hist)"
      ],
      "metadata": {
        "id": "YZnSzaTAeNWr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "eb8a8346-7a5b-4372-b1f3-8b91c9ea9cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_loss_hist[-1])\n",
        "print(test_loss_hist[-2])"
      ],
      "metadata": {
        "id": "b0FXUjXSLqi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e50134-23ed-4aa6-c201-0a4cb2b2bfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06631109816982644\n",
            "0.06636795289203291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_hist, label = 'Training Loss')\n",
        "plt.plot(test_loss_hist, label = 'Validation Loss')\n",
        "plt.legend(frameon = False)"
      ],
      "metadata": {
        "id": "PHZHhdIdN_Yk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "792900e5-bdc1-46da-e061-fbc7f56950b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7c7251d3b820>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вывод: Модель сошлась. Это видно на тестовом лоссе, потому что изменение стало очень маленьким"
      ],
      "metadata": {
        "id": "MSXKF8Q9Lajq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Сравните несколько архитектур, в том числе с различным количеством слоёв и подберите оптимальную. Обязательно используйте dropout и batchnorm и поясните свои выводы. Также можно попробовать различные активации в скрытых слоях.\n",
        "\n",
        "## Я попробую 6 различных архитектур и проверю на 30 эпохах. В каждой будет dropout И batchnorm. Также попробую одну модель без droupout и batchnorm, чтобы посмотреть насколько они влияют на модель"
      ],
      "metadata": {
        "id": "79gtVukHMdr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.linear2 = nn.Linear(100, 10)\n",
        "    self.act = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear2(out)\n",
        "\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "jBaL-LQeW875"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "KcDjAXxudL70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_list.extendnp.squeeze((top_class.cpu().numpy()))\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbKs3cEIbwy6",
        "outputId": "2629e1b9-1294-4421-bb57-2b4e022cb832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.887 Loss: 0.424\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.94      0.95       998\n",
            "           1       0.98      0.94      0.96      1172\n",
            "           2       0.88      0.87      0.88      1034\n",
            "           3       0.88      0.86      0.87      1029\n",
            "           4       0.80      0.94      0.86       828\n",
            "           5       0.81      0.84      0.82       865\n",
            "           6       0.92      0.93      0.92       949\n",
            "           7       0.88      0.93      0.90       972\n",
            "           8       0.84      0.84      0.84       975\n",
            "           9       0.91      0.78      0.84      1178\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.88      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.904 Loss: 0.336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      2053\n",
            "           1       0.98      0.95      0.96      2327\n",
            "           2       0.88      0.88      0.88      2058\n",
            "           3       0.90      0.85      0.88      2145\n",
            "           4       0.84      0.93      0.88      1775\n",
            "           5       0.80      0.88      0.84      1616\n",
            "           6       0.93      0.93      0.93      1906\n",
            "           7       0.88      0.94      0.91      1917\n",
            "           8       0.84      0.86      0.85      1911\n",
            "           9       0.92      0.81      0.86      2292\n",
            "\n",
            "    accuracy                           0.90     20000\n",
            "   macro avg       0.89      0.90      0.89     20000\n",
            "weighted avg       0.90      0.90      0.90     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.912 Loss: 0.309\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      3062\n",
            "           1       0.98      0.95      0.96      3495\n",
            "           2       0.86      0.91      0.88      2945\n",
            "           3       0.89      0.87      0.88      3112\n",
            "           4       0.86      0.93      0.89      2748\n",
            "           5       0.84      0.86      0.85      2616\n",
            "           6       0.93      0.93      0.93      2857\n",
            "           7       0.89      0.94      0.92      2912\n",
            "           8       0.85      0.86      0.86      2914\n",
            "           9       0.91      0.83      0.87      3339\n",
            "\n",
            "    accuracy                           0.90     30000\n",
            "   macro avg       0.90      0.90      0.90     30000\n",
            "weighted avg       0.90      0.90      0.90     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.926 Loss: 0.269\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.96      4068\n",
            "           1       0.98      0.96      0.97      4645\n",
            "           2       0.88      0.90      0.89      4014\n",
            "           3       0.90      0.88      0.89      4127\n",
            "           4       0.87      0.93      0.90      3682\n",
            "           5       0.85      0.87      0.86      3496\n",
            "           6       0.93      0.94      0.94      3817\n",
            "           7       0.90      0.94      0.92      3912\n",
            "           8       0.86      0.87      0.87      3826\n",
            "           9       0.92      0.84      0.88      4413\n",
            "\n",
            "    accuracy                           0.91     40000\n",
            "   macro avg       0.91      0.91      0.91     40000\n",
            "weighted avg       0.91      0.91      0.91     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.927 Loss: 0.259\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      5113\n",
            "           1       0.98      0.96      0.97      5798\n",
            "           2       0.89      0.90      0.90      5063\n",
            "           3       0.91      0.88      0.89      5235\n",
            "           4       0.88      0.93      0.90      4634\n",
            "           5       0.85      0.88      0.86      4282\n",
            "           6       0.94      0.94      0.94      4773\n",
            "           7       0.90      0.95      0.92      4891\n",
            "           8       0.86      0.88      0.87      4742\n",
            "           9       0.92      0.85      0.88      5469\n",
            "\n",
            "    accuracy                           0.91     50000\n",
            "   macro avg       0.91      0.91      0.91     50000\n",
            "weighted avg       0.91      0.91      0.91     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.932 Loss: 0.238\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      6129\n",
            "           1       0.98      0.96      0.97      6957\n",
            "           2       0.89      0.91      0.90      6007\n",
            "           3       0.91      0.88      0.90      6251\n",
            "           4       0.89      0.93      0.91      5595\n",
            "           5       0.86      0.88      0.87      5210\n",
            "           6       0.94      0.94      0.94      5732\n",
            "           7       0.91      0.95      0.93      5893\n",
            "           8       0.87      0.89      0.88      5718\n",
            "           9       0.92      0.86      0.89      6508\n",
            "\n",
            "    accuracy                           0.91     60000\n",
            "   macro avg       0.91      0.91      0.91     60000\n",
            "weighted avg       0.92      0.91      0.91     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.939 Loss: 0.214\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      7136\n",
            "           1       0.98      0.96      0.97      8105\n",
            "           2       0.89      0.91      0.90      7045\n",
            "           3       0.91      0.89      0.90      7258\n",
            "           4       0.89      0.93      0.91      6587\n",
            "           5       0.87      0.89      0.88      6077\n",
            "           6       0.94      0.94      0.94      6704\n",
            "           7       0.91      0.95      0.93      6907\n",
            "           8       0.87      0.90      0.88      6650\n",
            "           9       0.92      0.87      0.89      7531\n",
            "\n",
            "    accuracy                           0.92     70000\n",
            "   macro avg       0.92      0.92      0.92     70000\n",
            "weighted avg       0.92      0.92      0.92     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.939 Loss: 0.213\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      8162\n",
            "           1       0.98      0.96      0.97      9255\n",
            "           2       0.90      0.92      0.91      8108\n",
            "           3       0.92      0.89      0.90      8334\n",
            "           4       0.90      0.93      0.92      7546\n",
            "           5       0.87      0.90      0.88      6895\n",
            "           6       0.94      0.94      0.94      7654\n",
            "           7       0.91      0.95      0.93      7897\n",
            "           8       0.88      0.90      0.89      7582\n",
            "           9       0.92      0.87      0.90      8567\n",
            "\n",
            "    accuracy                           0.92     80000\n",
            "   macro avg       0.92      0.92      0.92     80000\n",
            "weighted avg       0.92      0.92      0.92     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.944 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      9175\n",
            "           1       0.98      0.96      0.97     10414\n",
            "           2       0.90      0.92      0.91      9087\n",
            "           3       0.92      0.89      0.91      9375\n",
            "           4       0.90      0.94      0.92      8505\n",
            "           5       0.87      0.90      0.89      7791\n",
            "           6       0.94      0.94      0.94      8618\n",
            "           7       0.91      0.95      0.93      8904\n",
            "           8       0.88      0.91      0.89      8532\n",
            "           9       0.93      0.88      0.90      9599\n",
            "\n",
            "    accuracy                           0.92     90000\n",
            "   macro avg       0.92      0.92      0.92     90000\n",
            "weighted avg       0.92      0.92      0.92     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.947 Loss: 0.180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     10179\n",
            "           1       0.98      0.96      0.97     11557\n",
            "           2       0.91      0.92      0.91     10133\n",
            "           3       0.92      0.90      0.91     10385\n",
            "           4       0.91      0.94      0.92      9508\n",
            "           5       0.88      0.91      0.89      8649\n",
            "           6       0.94      0.94      0.94      9588\n",
            "           7       0.92      0.95      0.93      9915\n",
            "           8       0.89      0.91      0.90      9480\n",
            "           9       0.93      0.88      0.90     10606\n",
            "\n",
            "    accuracy                           0.93    100000\n",
            "   macro avg       0.92      0.93      0.92    100000\n",
            "weighted avg       0.93      0.93      0.93    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.947 Loss: 0.180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     11203\n",
            "           1       0.98      0.96      0.97     12705\n",
            "           2       0.91      0.92      0.92     11202\n",
            "           3       0.92      0.90      0.91     11447\n",
            "           4       0.91      0.94      0.92     10469\n",
            "           5       0.88      0.91      0.90      9476\n",
            "           6       0.94      0.94      0.94     10540\n",
            "           7       0.92      0.95      0.93     10910\n",
            "           8       0.89      0.91      0.90     10419\n",
            "           9       0.93      0.89      0.91     11629\n",
            "\n",
            "    accuracy                           0.93    110000\n",
            "   macro avg       0.93      0.93      0.93    110000\n",
            "weighted avg       0.93      0.93      0.93    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.949 Loss: 0.170\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     12213\n",
            "           1       0.98      0.96      0.97     13857\n",
            "           2       0.91      0.93      0.92     12192\n",
            "           3       0.93      0.90      0.91     12495\n",
            "           4       0.91      0.94      0.92     11435\n",
            "           5       0.89      0.91      0.90     10370\n",
            "           6       0.95      0.94      0.94     11502\n",
            "           7       0.92      0.95      0.94     11922\n",
            "           8       0.89      0.92      0.90     11369\n",
            "           9       0.93      0.89      0.91     12645\n",
            "\n",
            "    accuracy                           0.93    120000\n",
            "   macro avg       0.93      0.93      0.93    120000\n",
            "weighted avg       0.93      0.93      0.93    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.955 Loss: 0.156\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     13214\n",
            "           1       0.98      0.96      0.97     14996\n",
            "           2       0.91      0.93      0.92     13231\n",
            "           3       0.93      0.90      0.92     13502\n",
            "           4       0.92      0.94      0.93     12443\n",
            "           5       0.89      0.92      0.90     11243\n",
            "           6       0.95      0.95      0.95     12466\n",
            "           7       0.92      0.95      0.94     12943\n",
            "           8       0.90      0.92      0.91     12323\n",
            "           9       0.93      0.89      0.91     13639\n",
            "\n",
            "    accuracy                           0.93    130000\n",
            "   macro avg       0.93      0.93      0.93    130000\n",
            "weighted avg       0.93      0.93      0.93    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.954 Loss: 0.157\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     14235\n",
            "           1       0.98      0.97      0.97     16144\n",
            "           2       0.92      0.93      0.92     14302\n",
            "           3       0.93      0.90      0.92     14546\n",
            "           4       0.92      0.94      0.93     13412\n",
            "           5       0.89      0.92      0.91     12089\n",
            "           6       0.95      0.95      0.95     13414\n",
            "           7       0.92      0.95      0.94     13951\n",
            "           8       0.90      0.92      0.91     13264\n",
            "           9       0.93      0.90      0.91     14643\n",
            "\n",
            "    accuracy                           0.93    140000\n",
            "   macro avg       0.93      0.93      0.93    140000\n",
            "weighted avg       0.93      0.93      0.93    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.956 Loss: 0.149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     15244\n",
            "           1       0.98      0.97      0.97     17296\n",
            "           2       0.92      0.93      0.92     15308\n",
            "           3       0.93      0.91      0.92     15587\n",
            "           4       0.92      0.94      0.93     14380\n",
            "           5       0.89      0.92      0.91     12977\n",
            "           6       0.95      0.95      0.95     14382\n",
            "           7       0.93      0.95      0.94     14971\n",
            "           8       0.90      0.93      0.91     14211\n",
            "           9       0.93      0.90      0.92     15644\n",
            "\n",
            "    accuracy                           0.93    150000\n",
            "   macro avg       0.93      0.93      0.93    150000\n",
            "weighted avg       0.94      0.93      0.93    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.960 Loss: 0.139\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     16244\n",
            "           1       0.98      0.97      0.97     18433\n",
            "           2       0.92      0.93      0.93     16350\n",
            "           3       0.93      0.91      0.92     16595\n",
            "           4       0.92      0.94      0.93     15384\n",
            "           5       0.90      0.92      0.91     13855\n",
            "           6       0.95      0.95      0.95     15342\n",
            "           7       0.93      0.95      0.94     15991\n",
            "           8       0.90      0.93      0.92     15166\n",
            "           9       0.93      0.90      0.92     16640\n",
            "\n",
            "    accuracy                           0.94    160000\n",
            "   macro avg       0.94      0.94      0.94    160000\n",
            "weighted avg       0.94      0.94      0.94    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.960 Loss: 0.140\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     17259\n",
            "           1       0.98      0.97      0.97     19575\n",
            "           2       0.93      0.93      0.93     17423\n",
            "           3       0.94      0.91      0.92     17634\n",
            "           4       0.92      0.94      0.93     16354\n",
            "           5       0.90      0.93      0.91     14712\n",
            "           6       0.95      0.95      0.95     16287\n",
            "           7       0.93      0.95      0.94     17006\n",
            "           8       0.91      0.93      0.92     16113\n",
            "           9       0.93      0.91      0.92     17637\n",
            "\n",
            "    accuracy                           0.94    170000\n",
            "   macro avg       0.94      0.94      0.94    170000\n",
            "weighted avg       0.94      0.94      0.94    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.960 Loss: 0.134\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     18271\n",
            "           1       0.98      0.97      0.97     20724\n",
            "           2       0.93      0.93      0.93     18444\n",
            "           3       0.94      0.91      0.93     18664\n",
            "           4       0.93      0.94      0.93     17327\n",
            "           5       0.90      0.93      0.92     15601\n",
            "           6       0.95      0.95      0.95     17249\n",
            "           7       0.93      0.95      0.94     18030\n",
            "           8       0.91      0.93      0.92     17056\n",
            "           9       0.93      0.91      0.92     18634\n",
            "\n",
            "    accuracy                           0.94    180000\n",
            "   macro avg       0.94      0.94      0.94    180000\n",
            "weighted avg       0.94      0.94      0.94    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.964 Loss: 0.126\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     19271\n",
            "           1       0.98      0.97      0.98     21860\n",
            "           2       0.93      0.94      0.93     19490\n",
            "           3       0.94      0.92      0.93     19664\n",
            "           4       0.93      0.94      0.94     18327\n",
            "           5       0.91      0.93      0.92     16485\n",
            "           6       0.95      0.95      0.95     18212\n",
            "           7       0.93      0.96      0.94     19050\n",
            "           8       0.91      0.93      0.92     18013\n",
            "           9       0.93      0.91      0.92     19628\n",
            "\n",
            "    accuracy                           0.94    190000\n",
            "   macro avg       0.94      0.94      0.94    190000\n",
            "weighted avg       0.94      0.94      0.94    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.965 Loss: 0.128\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     20280\n",
            "           1       0.98      0.97      0.98     23000\n",
            "           2       0.93      0.94      0.93     20560\n",
            "           3       0.94      0.92      0.93     20698\n",
            "           4       0.93      0.95      0.94     19299\n",
            "           5       0.91      0.93      0.92     17350\n",
            "           6       0.95      0.95      0.95     19157\n",
            "           7       0.93      0.96      0.94     20065\n",
            "           8       0.91      0.94      0.92     18967\n",
            "           9       0.93      0.91      0.92     20624\n",
            "\n",
            "    accuracy                           0.94    200000\n",
            "   macro avg       0.94      0.94      0.94    200000\n",
            "weighted avg       0.94      0.94      0.94    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.963 Loss: 0.124\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     21286\n",
            "           1       0.98      0.97      0.98     24145\n",
            "           2       0.93      0.94      0.94     21590\n",
            "           3       0.94      0.92      0.93     21730\n",
            "           4       0.93      0.95      0.94     20273\n",
            "           5       0.91      0.93      0.92     18244\n",
            "           6       0.95      0.95      0.95     20122\n",
            "           7       0.93      0.96      0.95     21085\n",
            "           8       0.91      0.94      0.93     19904\n",
            "           9       0.93      0.92      0.92     21621\n",
            "\n",
            "    accuracy                           0.94    210000\n",
            "   macro avg       0.94      0.94      0.94    210000\n",
            "weighted avg       0.94      0.94      0.94    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.967 Loss: 0.116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     22284\n",
            "           1       0.98      0.97      0.98     25283\n",
            "           2       0.94      0.94      0.94     22641\n",
            "           3       0.94      0.92      0.93     22727\n",
            "           4       0.93      0.95      0.94     21272\n",
            "           5       0.91      0.94      0.92     19130\n",
            "           6       0.95      0.95      0.95     21086\n",
            "           7       0.94      0.96      0.95     22100\n",
            "           8       0.91      0.94      0.93     20859\n",
            "           9       0.93      0.92      0.93     22618\n",
            "\n",
            "    accuracy                           0.94    220000\n",
            "   macro avg       0.94      0.94      0.94    220000\n",
            "weighted avg       0.94      0.94      0.94    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.967 Loss: 0.119\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     23293\n",
            "           1       0.98      0.97      0.98     26425\n",
            "           2       0.94      0.94      0.94     23711\n",
            "           3       0.94      0.92      0.93     23760\n",
            "           4       0.93      0.95      0.94     22246\n",
            "           5       0.91      0.94      0.93     19995\n",
            "           6       0.95      0.95      0.95     22035\n",
            "           7       0.94      0.96      0.95     23114\n",
            "           8       0.92      0.94      0.93     21811\n",
            "           9       0.94      0.92      0.93     23610\n",
            "\n",
            "    accuracy                           0.94    230000\n",
            "   macro avg       0.94      0.94      0.94    230000\n",
            "weighted avg       0.94      0.94      0.94    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.966 Loss: 0.116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     24297\n",
            "           1       0.98      0.97      0.98     27570\n",
            "           2       0.94      0.94      0.94     24748\n",
            "           3       0.94      0.92      0.93     24791\n",
            "           4       0.93      0.95      0.94     23220\n",
            "           5       0.92      0.94      0.93     20892\n",
            "           6       0.95      0.95      0.95     22998\n",
            "           7       0.94      0.96      0.95     24135\n",
            "           8       0.92      0.94      0.93     22743\n",
            "           9       0.94      0.92      0.93     24606\n",
            "\n",
            "    accuracy                           0.95    240000\n",
            "   macro avg       0.94      0.95      0.94    240000\n",
            "weighted avg       0.95      0.95      0.95    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.968 Loss: 0.109\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     25297\n",
            "           1       0.98      0.97      0.98     28707\n",
            "           2       0.94      0.94      0.94     25793\n",
            "           3       0.95      0.93      0.94     25793\n",
            "           4       0.94      0.95      0.94     24215\n",
            "           5       0.92      0.94      0.93     21774\n",
            "           6       0.95      0.95      0.95     23962\n",
            "           7       0.94      0.96      0.95     25156\n",
            "           8       0.92      0.94      0.93     23700\n",
            "           9       0.94      0.92      0.93     25603\n",
            "\n",
            "    accuracy                           0.95    250000\n",
            "   macro avg       0.95      0.95      0.95    250000\n",
            "weighted avg       0.95      0.95      0.95    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.968 Loss: 0.112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     26302\n",
            "           1       0.98      0.97      0.98     29851\n",
            "           2       0.94      0.94      0.94     26864\n",
            "           3       0.95      0.93      0.94     26822\n",
            "           4       0.94      0.95      0.94     25187\n",
            "           5       0.92      0.94      0.93     22640\n",
            "           6       0.96      0.96      0.96     24913\n",
            "           7       0.94      0.96      0.95     26172\n",
            "           8       0.92      0.95      0.93     24652\n",
            "           9       0.94      0.92      0.93     26597\n",
            "\n",
            "    accuracy                           0.95    260000\n",
            "   macro avg       0.95      0.95      0.95    260000\n",
            "weighted avg       0.95      0.95      0.95    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.968 Loss: 0.110\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     27304\n",
            "           1       0.98      0.97      0.98     30999\n",
            "           2       0.94      0.94      0.94     27900\n",
            "           3       0.95      0.93      0.94     27856\n",
            "           4       0.94      0.95      0.94     26162\n",
            "           5       0.92      0.94      0.93     23531\n",
            "           6       0.96      0.96      0.96     25876\n",
            "           7       0.94      0.96      0.95     27193\n",
            "           8       0.92      0.95      0.93     25585\n",
            "           9       0.94      0.93      0.93     27594\n",
            "\n",
            "    accuracy                           0.95    270000\n",
            "   macro avg       0.95      0.95      0.95    270000\n",
            "weighted avg       0.95      0.95      0.95    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.970 Loss: 0.103\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     28302\n",
            "           1       0.98      0.97      0.98     32141\n",
            "           2       0.94      0.94      0.94     28945\n",
            "           3       0.95      0.93      0.94     28859\n",
            "           4       0.94      0.95      0.95     27154\n",
            "           5       0.92      0.94      0.93     24416\n",
            "           6       0.96      0.96      0.96     26838\n",
            "           7       0.94      0.96      0.95     28209\n",
            "           8       0.92      0.95      0.93     26544\n",
            "           9       0.94      0.93      0.93     28592\n",
            "\n",
            "    accuracy                           0.95    280000\n",
            "   macro avg       0.95      0.95      0.95    280000\n",
            "weighted avg       0.95      0.95      0.95    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.969 Loss: 0.107\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     29307\n",
            "           1       0.98      0.97      0.98     33287\n",
            "           2       0.95      0.94      0.94     30020\n",
            "           3       0.95      0.93      0.94     29890\n",
            "           4       0.94      0.95      0.95     28120\n",
            "           5       0.92      0.94      0.93     25281\n",
            "           6       0.96      0.96      0.96     27789\n",
            "           7       0.94      0.96      0.95     29221\n",
            "           8       0.92      0.95      0.94     27495\n",
            "           9       0.94      0.93      0.93     29590\n",
            "\n",
            "    accuracy                           0.95    290000\n",
            "   macro avg       0.95      0.95      0.95    290000\n",
            "weighted avg       0.95      0.95      0.95    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.969 Loss: 0.105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     30310\n",
            "           1       0.98      0.97      0.98     34435\n",
            "           2       0.95      0.94      0.94     31056\n",
            "           3       0.95      0.93      0.94     30924\n",
            "           4       0.94      0.95      0.95     29092\n",
            "           5       0.92      0.94      0.93     26174\n",
            "           6       0.96      0.96      0.96     28748\n",
            "           7       0.94      0.96      0.95     30243\n",
            "           8       0.92      0.95      0.94     28429\n",
            "           9       0.94      0.93      0.93     30589\n",
            "\n",
            "    accuracy                           0.95    300000\n",
            "   macro avg       0.95      0.95      0.95    300000\n",
            "weighted avg       0.95      0.95      0.95    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.972 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     31308\n",
            "           1       0.98      0.97      0.98     35576\n",
            "           2       0.95      0.94      0.95     32097\n",
            "           3       0.95      0.93      0.94     31932\n",
            "           4       0.94      0.95      0.95     30084\n",
            "           5       0.93      0.95      0.94     27057\n",
            "           6       0.96      0.96      0.96     29709\n",
            "           7       0.94      0.96      0.95     31261\n",
            "           8       0.93      0.95      0.94     29390\n",
            "           9       0.94      0.93      0.93     31586\n",
            "\n",
            "    accuracy                           0.95    310000\n",
            "   macro avg       0.95      0.95      0.95    310000\n",
            "weighted avg       0.95      0.95      0.95    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.970 Loss: 0.102\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     32311\n",
            "           1       0.98      0.97      0.98     36722\n",
            "           2       0.95      0.94      0.95     33169\n",
            "           3       0.95      0.93      0.94     32963\n",
            "           4       0.94      0.95      0.95     31049\n",
            "           5       0.93      0.95      0.94     27922\n",
            "           6       0.96      0.96      0.96     30663\n",
            "           7       0.94      0.96      0.95     32274\n",
            "           8       0.93      0.95      0.94     30342\n",
            "           9       0.94      0.93      0.94     32585\n",
            "\n",
            "    accuracy                           0.95    320000\n",
            "   macro avg       0.95      0.95      0.95    320000\n",
            "weighted avg       0.95      0.95      0.95    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.970 Loss: 0.101\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     33311\n",
            "           1       0.98      0.97      0.98     37870\n",
            "           2       0.95      0.94      0.95     34208\n",
            "           3       0.95      0.93      0.94     33996\n",
            "           4       0.94      0.95      0.95     32024\n",
            "           5       0.93      0.95      0.94     28814\n",
            "           6       0.96      0.96      0.96     31623\n",
            "           7       0.94      0.96      0.95     33296\n",
            "           8       0.93      0.95      0.94     31276\n",
            "           9       0.94      0.93      0.94     33582\n",
            "\n",
            "    accuracy                           0.95    330000\n",
            "   macro avg       0.95      0.95      0.95    330000\n",
            "weighted avg       0.95      0.95      0.95    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.973 Loss: 0.093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     34309\n",
            "           1       0.99      0.97      0.98     39011\n",
            "           2       0.95      0.95      0.95     35249\n",
            "           3       0.95      0.93      0.94     35002\n",
            "           4       0.94      0.95      0.95     33014\n",
            "           5       0.93      0.95      0.94     29698\n",
            "           6       0.96      0.96      0.96     32584\n",
            "           7       0.94      0.96      0.95     34314\n",
            "           8       0.93      0.95      0.94     32238\n",
            "           9       0.94      0.93      0.94     34581\n",
            "\n",
            "    accuracy                           0.95    340000\n",
            "   macro avg       0.95      0.95      0.95    340000\n",
            "weighted avg       0.95      0.95      0.95    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.972 Loss: 0.097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     35311\n",
            "           1       0.99      0.97      0.98     40157\n",
            "           2       0.95      0.95      0.95     36319\n",
            "           3       0.95      0.93      0.94     36031\n",
            "           4       0.94      0.96      0.95     33979\n",
            "           5       0.93      0.95      0.94     30567\n",
            "           6       0.96      0.96      0.96     33539\n",
            "           7       0.94      0.96      0.95     35324\n",
            "           8       0.93      0.95      0.94     33193\n",
            "           9       0.94      0.93      0.94     35580\n",
            "\n",
            "    accuracy                           0.95    350000\n",
            "   macro avg       0.95      0.95      0.95    350000\n",
            "weighted avg       0.95      0.95      0.95    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.971 Loss: 0.097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     36311\n",
            "           1       0.99      0.97      0.98     41303\n",
            "           2       0.95      0.95      0.95     37357\n",
            "           3       0.95      0.94      0.94     37063\n",
            "           4       0.95      0.96      0.95     34956\n",
            "           5       0.93      0.95      0.94     31457\n",
            "           6       0.96      0.96      0.96     34499\n",
            "           7       0.94      0.96      0.95     36346\n",
            "           8       0.93      0.95      0.94     34129\n",
            "           9       0.94      0.94      0.94     36579\n",
            "\n",
            "    accuracy                           0.95    360000\n",
            "   macro avg       0.95      0.95      0.95    360000\n",
            "weighted avg       0.95      0.95      0.95    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.974 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     37309\n",
            "           1       0.99      0.98      0.98     42446\n",
            "           2       0.95      0.95      0.95     38394\n",
            "           3       0.95      0.94      0.95     38071\n",
            "           4       0.95      0.96      0.95     35945\n",
            "           5       0.93      0.95      0.94     32341\n",
            "           6       0.96      0.96      0.96     35461\n",
            "           7       0.95      0.96      0.95     37363\n",
            "           8       0.93      0.96      0.94     35092\n",
            "           9       0.94      0.94      0.94     37578\n",
            "\n",
            "    accuracy                           0.95    370000\n",
            "   macro avg       0.95      0.95      0.95    370000\n",
            "weighted avg       0.95      0.95      0.95    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.972 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     38313\n",
            "           1       0.99      0.98      0.98     43592\n",
            "           2       0.95      0.95      0.95     39464\n",
            "           3       0.95      0.94      0.95     39099\n",
            "           4       0.95      0.96      0.95     36908\n",
            "           5       0.93      0.95      0.94     33211\n",
            "           6       0.96      0.96      0.96     36415\n",
            "           7       0.95      0.96      0.95     38375\n",
            "           8       0.93      0.96      0.94     36046\n",
            "           9       0.94      0.94      0.94     38577\n",
            "\n",
            "    accuracy                           0.95    380000\n",
            "   macro avg       0.95      0.95      0.95    380000\n",
            "weighted avg       0.95      0.95      0.95    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.972 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     39314\n",
            "           1       0.99      0.98      0.98     44737\n",
            "           2       0.95      0.95      0.95     40500\n",
            "           3       0.95      0.94      0.95     40135\n",
            "           4       0.95      0.96      0.95     37886\n",
            "           5       0.93      0.95      0.94     34100\n",
            "           6       0.96      0.96      0.96     37375\n",
            "           7       0.95      0.96      0.95     39397\n",
            "           8       0.93      0.96      0.94     36980\n",
            "           9       0.94      0.94      0.94     39576\n",
            "\n",
            "    accuracy                           0.96    390000\n",
            "   macro avg       0.95      0.95      0.95    390000\n",
            "weighted avg       0.96      0.96      0.96    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.974 Loss: 0.087\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     40308\n",
            "           1       0.99      0.98      0.98     45880\n",
            "           2       0.95      0.95      0.95     41535\n",
            "           3       0.96      0.94      0.95     41142\n",
            "           4       0.95      0.96      0.95     38875\n",
            "           5       0.93      0.95      0.94     34987\n",
            "           6       0.96      0.96      0.96     38338\n",
            "           7       0.95      0.96      0.95     40416\n",
            "           8       0.93      0.96      0.94     37945\n",
            "           9       0.94      0.94      0.94     40574\n",
            "\n",
            "    accuracy                           0.96    400000\n",
            "   macro avg       0.95      0.96      0.96    400000\n",
            "weighted avg       0.96      0.96      0.96    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.972 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     41309\n",
            "           1       0.99      0.98      0.98     47025\n",
            "           2       0.95      0.95      0.95     42604\n",
            "           3       0.96      0.94      0.95     42173\n",
            "           4       0.95      0.96      0.95     39839\n",
            "           5       0.93      0.95      0.94     35856\n",
            "           6       0.96      0.96      0.96     39293\n",
            "           7       0.95      0.96      0.96     41426\n",
            "           8       0.93      0.96      0.95     38900\n",
            "           9       0.94      0.94      0.94     41575\n",
            "\n",
            "    accuracy                           0.96    410000\n",
            "   macro avg       0.96      0.96      0.96    410000\n",
            "weighted avg       0.96      0.96      0.96    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.973 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     42310\n",
            "           1       0.99      0.98      0.98     48170\n",
            "           2       0.96      0.95      0.95     43642\n",
            "           3       0.96      0.94      0.95     43209\n",
            "           4       0.95      0.96      0.95     40814\n",
            "           5       0.94      0.95      0.94     36748\n",
            "           6       0.96      0.96      0.96     40251\n",
            "           7       0.95      0.96      0.96     42446\n",
            "           8       0.93      0.96      0.95     39835\n",
            "           9       0.94      0.94      0.94     42575\n",
            "\n",
            "    accuracy                           0.96    420000\n",
            "   macro avg       0.96      0.96      0.96    420000\n",
            "weighted avg       0.96      0.96      0.96    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.975 Loss: 0.085\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     43303\n",
            "           1       0.99      0.98      0.98     49312\n",
            "           2       0.96      0.95      0.95     44676\n",
            "           3       0.96      0.94      0.95     44220\n",
            "           4       0.95      0.96      0.95     41801\n",
            "           5       0.94      0.95      0.95     37634\n",
            "           6       0.96      0.96      0.96     41214\n",
            "           7       0.95      0.96      0.96     43466\n",
            "           8       0.93      0.96      0.95     40800\n",
            "           9       0.94      0.94      0.94     43574\n",
            "\n",
            "    accuracy                           0.96    430000\n",
            "   macro avg       0.96      0.96      0.96    430000\n",
            "weighted avg       0.96      0.96      0.96    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.973 Loss: 0.089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     44306\n",
            "           1       0.99      0.98      0.98     50457\n",
            "           2       0.96      0.95      0.95     45742\n",
            "           3       0.96      0.94      0.95     45249\n",
            "           4       0.95      0.96      0.95     42765\n",
            "           5       0.94      0.96      0.95     38505\n",
            "           6       0.96      0.96      0.96     42168\n",
            "           7       0.95      0.96      0.96     44477\n",
            "           8       0.93      0.96      0.95     41757\n",
            "           9       0.94      0.94      0.94     44574\n",
            "\n",
            "    accuracy                           0.96    440000\n",
            "   macro avg       0.96      0.96      0.96    440000\n",
            "weighted avg       0.96      0.96      0.96    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.973 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     45308\n",
            "           1       0.99      0.98      0.98     51602\n",
            "           2       0.96      0.95      0.95     46776\n",
            "           3       0.96      0.94      0.95     46283\n",
            "           4       0.95      0.96      0.95     43738\n",
            "           5       0.94      0.96      0.95     39399\n",
            "           6       0.96      0.96      0.96     43126\n",
            "           7       0.95      0.96      0.96     45497\n",
            "           8       0.94      0.96      0.95     42694\n",
            "           9       0.94      0.94      0.94     45577\n",
            "\n",
            "    accuracy                           0.96    450000\n",
            "   macro avg       0.96      0.96      0.96    450000\n",
            "weighted avg       0.96      0.96      0.96    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.976 Loss: 0.083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     46301\n",
            "           1       0.99      0.98      0.98     52742\n",
            "           2       0.96      0.95      0.95     47810\n",
            "           3       0.96      0.94      0.95     47297\n",
            "           4       0.95      0.96      0.96     44722\n",
            "           5       0.94      0.96      0.95     40285\n",
            "           6       0.96      0.96      0.96     44090\n",
            "           7       0.95      0.96      0.96     46515\n",
            "           8       0.94      0.96      0.95     43662\n",
            "           9       0.95      0.94      0.94     46576\n",
            "\n",
            "    accuracy                           0.96    460000\n",
            "   macro avg       0.96      0.96      0.96    460000\n",
            "weighted avg       0.96      0.96      0.96    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.974 Loss: 0.086\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     47303\n",
            "           1       0.99      0.98      0.98     53886\n",
            "           2       0.96      0.95      0.95     48873\n",
            "           3       0.96      0.94      0.95     48326\n",
            "           4       0.95      0.96      0.96     45685\n",
            "           5       0.94      0.96      0.95     41158\n",
            "           6       0.96      0.96      0.96     45045\n",
            "           7       0.95      0.96      0.96     47524\n",
            "           8       0.94      0.96      0.95     44622\n",
            "           9       0.95      0.94      0.94     47578\n",
            "\n",
            "    accuracy                           0.96    470000\n",
            "   macro avg       0.96      0.96      0.96    470000\n",
            "weighted avg       0.96      0.96      0.96    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     48305\n",
            "           1       0.99      0.98      0.98     55031\n",
            "           2       0.96      0.95      0.95     49907\n",
            "           3       0.96      0.94      0.95     49357\n",
            "           4       0.95      0.96      0.96     46659\n",
            "           5       0.94      0.96      0.95     42053\n",
            "           6       0.96      0.96      0.96     46001\n",
            "           7       0.95      0.97      0.96     48545\n",
            "           8       0.94      0.96      0.95     45562\n",
            "           9       0.95      0.94      0.94     48580\n",
            "\n",
            "    accuracy                           0.96    480000\n",
            "   macro avg       0.96      0.96      0.96    480000\n",
            "weighted avg       0.96      0.96      0.96    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.976 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     49297\n",
            "           1       0.99      0.98      0.98     56171\n",
            "           2       0.96      0.95      0.96     50941\n",
            "           3       0.96      0.94      0.95     50372\n",
            "           4       0.95      0.96      0.96     47641\n",
            "           5       0.94      0.96      0.95     42937\n",
            "           6       0.97      0.96      0.96     46968\n",
            "           7       0.95      0.97      0.96     49562\n",
            "           8       0.94      0.96      0.95     46528\n",
            "           9       0.95      0.94      0.94     49583\n",
            "\n",
            "    accuracy                           0.96    490000\n",
            "   macro avg       0.96      0.96      0.96    490000\n",
            "weighted avg       0.96      0.96      0.96    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.975 Loss: 0.084\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     50295\n",
            "           1       0.99      0.98      0.98     57313\n",
            "           2       0.96      0.95      0.96     52000\n",
            "           3       0.96      0.94      0.95     51401\n",
            "           4       0.95      0.96      0.96     48601\n",
            "           5       0.94      0.96      0.95     43814\n",
            "           6       0.97      0.96      0.97     47926\n",
            "           7       0.95      0.97      0.96     50571\n",
            "           8       0.94      0.96      0.95     47490\n",
            "           9       0.95      0.94      0.95     50589\n",
            "\n",
            "    accuracy                           0.96    500000\n",
            "   macro avg       0.96      0.96      0.96    500000\n",
            "weighted avg       0.96      0.96      0.96    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.975 Loss: 0.086\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     51298\n",
            "           1       0.99      0.98      0.98     58456\n",
            "           2       0.96      0.95      0.96     53037\n",
            "           3       0.96      0.94      0.95     52429\n",
            "           4       0.95      0.96      0.96     49575\n",
            "           5       0.94      0.96      0.95     44708\n",
            "           6       0.97      0.97      0.97     48880\n",
            "           7       0.95      0.97      0.96     51593\n",
            "           8       0.94      0.96      0.95     48433\n",
            "           9       0.95      0.94      0.95     51591\n",
            "\n",
            "    accuracy                           0.96    510000\n",
            "   macro avg       0.96      0.96      0.96    510000\n",
            "weighted avg       0.96      0.96      0.96    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.976 Loss: 0.079\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     52292\n",
            "           1       0.99      0.98      0.98     59594\n",
            "           2       0.96      0.95      0.96     54067\n",
            "           3       0.96      0.94      0.95     53447\n",
            "           4       0.95      0.96      0.96     50557\n",
            "           5       0.94      0.96      0.95     45590\n",
            "           6       0.97      0.97      0.97     49849\n",
            "           7       0.95      0.97      0.96     52611\n",
            "           8       0.94      0.96      0.95     49400\n",
            "           9       0.95      0.94      0.95     52593\n",
            "\n",
            "    accuracy                           0.96    520000\n",
            "   macro avg       0.96      0.96      0.96    520000\n",
            "weighted avg       0.96      0.96      0.96    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.975 Loss: 0.082\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     53290\n",
            "           1       0.99      0.98      0.98     60736\n",
            "           2       0.96      0.95      0.96     55126\n",
            "           3       0.96      0.94      0.95     54476\n",
            "           4       0.95      0.96      0.96     51517\n",
            "           5       0.94      0.96      0.95     46465\n",
            "           6       0.97      0.97      0.97     50807\n",
            "           7       0.95      0.97      0.96     53621\n",
            "           8       0.94      0.96      0.95     50363\n",
            "           9       0.95      0.95      0.95     53599\n",
            "\n",
            "    accuracy                           0.96    530000\n",
            "   macro avg       0.96      0.96      0.96    530000\n",
            "weighted avg       0.96      0.96      0.96    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.975 Loss: 0.084\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     54293\n",
            "           1       0.99      0.98      0.98     61877\n",
            "           2       0.96      0.95      0.96     56163\n",
            "           3       0.96      0.94      0.95     55504\n",
            "           4       0.95      0.96      0.96     52494\n",
            "           5       0.94      0.96      0.95     47359\n",
            "           6       0.97      0.97      0.97     51762\n",
            "           7       0.95      0.97      0.96     54645\n",
            "           8       0.94      0.96      0.95     51306\n",
            "           9       0.95      0.95      0.95     54597\n",
            "\n",
            "    accuracy                           0.96    540000\n",
            "   macro avg       0.96      0.96      0.96    540000\n",
            "weighted avg       0.96      0.96      0.96    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.976 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     55286\n",
            "           1       0.99      0.98      0.98     63013\n",
            "           2       0.96      0.95      0.96     57194\n",
            "           3       0.96      0.94      0.95     56524\n",
            "           4       0.95      0.96      0.96     53476\n",
            "           5       0.94      0.96      0.95     48240\n",
            "           6       0.97      0.97      0.97     52731\n",
            "           7       0.95      0.97      0.96     55663\n",
            "           8       0.94      0.96      0.95     52274\n",
            "           9       0.95      0.95      0.95     55599\n",
            "\n",
            "    accuracy                           0.96    550000\n",
            "   macro avg       0.96      0.96      0.96    550000\n",
            "weighted avg       0.96      0.96      0.96    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.976 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     56284\n",
            "           1       0.99      0.98      0.98     64155\n",
            "           2       0.96      0.95      0.96     58252\n",
            "           3       0.96      0.95      0.95     57554\n",
            "           4       0.95      0.96      0.96     54436\n",
            "           5       0.94      0.96      0.95     49114\n",
            "           6       0.97      0.97      0.97     53691\n",
            "           7       0.95      0.97      0.96     56673\n",
            "           8       0.94      0.96      0.95     53237\n",
            "           9       0.95      0.95      0.95     56604\n",
            "\n",
            "    accuracy                           0.96    560000\n",
            "   macro avg       0.96      0.96      0.96    560000\n",
            "weighted avg       0.96      0.96      0.96    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.976 Loss: 0.083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     57288\n",
            "           1       0.99      0.98      0.98     65296\n",
            "           2       0.96      0.95      0.96     59289\n",
            "           3       0.96      0.95      0.95     58578\n",
            "           4       0.95      0.96      0.96     55413\n",
            "           5       0.94      0.96      0.95     50009\n",
            "           6       0.97      0.97      0.97     54645\n",
            "           7       0.95      0.97      0.96     57696\n",
            "           8       0.94      0.96      0.95     54185\n",
            "           9       0.95      0.95      0.95     57601\n",
            "\n",
            "    accuracy                           0.96    570000\n",
            "   macro avg       0.96      0.96      0.96    570000\n",
            "weighted avg       0.96      0.96      0.96    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.976 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     58281\n",
            "           1       0.99      0.98      0.98     66432\n",
            "           2       0.96      0.95      0.96     60321\n",
            "           3       0.96      0.95      0.95     59601\n",
            "           4       0.95      0.96      0.96     56396\n",
            "           5       0.95      0.96      0.95     50889\n",
            "           6       0.97      0.97      0.97     55613\n",
            "           7       0.95      0.97      0.96     58713\n",
            "           8       0.94      0.96      0.95     55154\n",
            "           9       0.95      0.95      0.95     58600\n",
            "\n",
            "    accuracy                           0.96    580000\n",
            "   macro avg       0.96      0.96      0.96    580000\n",
            "weighted avg       0.96      0.96      0.96    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.977 Loss: 0.080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     59278\n",
            "           1       0.99      0.98      0.98     67574\n",
            "           2       0.96      0.95      0.96     61379\n",
            "           3       0.96      0.95      0.95     60629\n",
            "           4       0.95      0.96      0.96     57358\n",
            "           5       0.95      0.96      0.95     51767\n",
            "           6       0.97      0.97      0.97     56573\n",
            "           7       0.95      0.97      0.96     59722\n",
            "           8       0.94      0.96      0.95     56116\n",
            "           9       0.95      0.95      0.95     59604\n",
            "\n",
            "    accuracy                           0.96    590000\n",
            "   macro avg       0.96      0.96      0.96    590000\n",
            "weighted avg       0.96      0.96      0.96    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.977 Loss: 0.082\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     60283\n",
            "           1       0.99      0.98      0.98     68715\n",
            "           2       0.96      0.95      0.96     62416\n",
            "           3       0.96      0.95      0.95     61654\n",
            "           4       0.96      0.96      0.96     58335\n",
            "           5       0.95      0.96      0.95     52662\n",
            "           6       0.97      0.97      0.97     57525\n",
            "           7       0.95      0.97      0.96     60743\n",
            "           8       0.94      0.96      0.95     57066\n",
            "           9       0.95      0.95      0.95     60601\n",
            "\n",
            "    accuracy                           0.96    600000\n",
            "   macro avg       0.96      0.96      0.96    600000\n",
            "weighted avg       0.96      0.96      0.96    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.977 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     61277\n",
            "           1       0.99      0.98      0.98     69852\n",
            "           2       0.96      0.95      0.96     63446\n",
            "           3       0.96      0.95      0.95     62677\n",
            "           4       0.96      0.96      0.96     59318\n",
            "           5       0.95      0.96      0.95     53542\n",
            "           6       0.97      0.97      0.97     58492\n",
            "           7       0.95      0.97      0.96     61759\n",
            "           8       0.94      0.97      0.95     58034\n",
            "           9       0.95      0.95      0.95     61603\n",
            "\n",
            "    accuracy                           0.96    610000\n",
            "   macro avg       0.96      0.96      0.96    610000\n",
            "weighted avg       0.96      0.96      0.96    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.977 Loss: 0.079\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     62274\n",
            "           1       0.99      0.98      0.98     70994\n",
            "           2       0.96      0.95      0.96     64503\n",
            "           3       0.96      0.95      0.96     63704\n",
            "           4       0.96      0.97      0.96     60281\n",
            "           5       0.95      0.96      0.95     54420\n",
            "           6       0.97      0.97      0.97     59455\n",
            "           7       0.95      0.97      0.96     62764\n",
            "           8       0.94      0.97      0.95     58999\n",
            "           9       0.95      0.95      0.95     62606\n",
            "\n",
            "    accuracy                           0.96    620000\n",
            "   macro avg       0.96      0.96      0.96    620000\n",
            "weighted avg       0.96      0.96      0.96    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.977 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     63278\n",
            "           1       0.99      0.98      0.98     72136\n",
            "           2       0.96      0.96      0.96     65538\n",
            "           3       0.96      0.95      0.96     64729\n",
            "           4       0.96      0.97      0.96     61256\n",
            "           5       0.95      0.96      0.95     55316\n",
            "           6       0.97      0.97      0.97     60407\n",
            "           7       0.95      0.97      0.96     63788\n",
            "           8       0.94      0.97      0.95     59948\n",
            "           9       0.95      0.95      0.95     63604\n",
            "\n",
            "    accuracy                           0.96    630000\n",
            "   macro avg       0.96      0.96      0.96    630000\n",
            "weighted avg       0.96      0.96      0.96    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.977 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     64271\n",
            "           1       0.99      0.98      0.98     73272\n",
            "           2       0.96      0.96      0.96     66568\n",
            "           3       0.96      0.95      0.96     65754\n",
            "           4       0.96      0.97      0.96     62237\n",
            "           5       0.95      0.96      0.96     56194\n",
            "           6       0.97      0.97      0.97     61377\n",
            "           7       0.95      0.97      0.96     64805\n",
            "           8       0.94      0.97      0.95     60916\n",
            "           9       0.95      0.95      0.95     64606\n",
            "\n",
            "    accuracy                           0.96    640000\n",
            "   macro avg       0.96      0.96      0.96    640000\n",
            "weighted avg       0.96      0.96      0.96    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.977 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     65267\n",
            "           1       0.99      0.98      0.98     74413\n",
            "           2       0.96      0.96      0.96     67623\n",
            "           3       0.96      0.95      0.96     66781\n",
            "           4       0.96      0.97      0.96     63201\n",
            "           5       0.95      0.96      0.96     57071\n",
            "           6       0.97      0.97      0.97     62341\n",
            "           7       0.95      0.97      0.96     65809\n",
            "           8       0.94      0.97      0.95     61882\n",
            "           9       0.95      0.95      0.95     65612\n",
            "\n",
            "    accuracy                           0.96    650000\n",
            "   macro avg       0.96      0.96      0.96    650000\n",
            "weighted avg       0.96      0.96      0.96    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.977 Loss: 0.080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     66270\n",
            "           1       0.99      0.98      0.98     75553\n",
            "           2       0.96      0.96      0.96     68661\n",
            "           3       0.96      0.95      0.96     67804\n",
            "           4       0.96      0.97      0.96     64177\n",
            "           5       0.95      0.96      0.96     57971\n",
            "           6       0.97      0.97      0.97     63292\n",
            "           7       0.95      0.97      0.96     66832\n",
            "           8       0.94      0.97      0.96     62830\n",
            "           9       0.95      0.95      0.95     66610\n",
            "\n",
            "    accuracy                           0.96    660000\n",
            "   macro avg       0.96      0.96      0.96    660000\n",
            "weighted avg       0.96      0.96      0.96    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.978 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     67263\n",
            "           1       0.99      0.98      0.98     76689\n",
            "           2       0.96      0.96      0.96     69689\n",
            "           3       0.96      0.95      0.96     68830\n",
            "           4       0.96      0.97      0.96     65158\n",
            "           5       0.95      0.96      0.96     58850\n",
            "           6       0.97      0.97      0.97     64261\n",
            "           7       0.95      0.97      0.96     67852\n",
            "           8       0.94      0.97      0.96     63797\n",
            "           9       0.95      0.95      0.95     67611\n",
            "\n",
            "    accuracy                           0.96    670000\n",
            "   macro avg       0.96      0.96      0.96    670000\n",
            "weighted avg       0.96      0.96      0.96    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.977 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     68258\n",
            "           1       0.99      0.98      0.98     77829\n",
            "           2       0.96      0.96      0.96     70742\n",
            "           3       0.97      0.95      0.96     69856\n",
            "           4       0.96      0.97      0.96     66125\n",
            "           5       0.95      0.96      0.96     59730\n",
            "           6       0.97      0.97      0.97     65227\n",
            "           7       0.95      0.97      0.96     68857\n",
            "           8       0.95      0.97      0.96     64762\n",
            "           9       0.95      0.95      0.95     68614\n",
            "\n",
            "    accuracy                           0.96    680000\n",
            "   macro avg       0.96      0.96      0.96    680000\n",
            "weighted avg       0.96      0.96      0.96    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.978 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     69259\n",
            "           1       0.99      0.98      0.98     78968\n",
            "           2       0.96      0.96      0.96     71780\n",
            "           3       0.97      0.95      0.96     70875\n",
            "           4       0.96      0.97      0.96     67101\n",
            "           5       0.95      0.96      0.96     60628\n",
            "           6       0.97      0.97      0.97     66180\n",
            "           7       0.95      0.97      0.96     69883\n",
            "           8       0.95      0.97      0.96     65713\n",
            "           9       0.95      0.95      0.95     69613\n",
            "\n",
            "    accuracy                           0.96    690000\n",
            "   macro avg       0.96      0.96      0.96    690000\n",
            "weighted avg       0.96      0.96      0.96    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     70251\n",
            "           1       0.99      0.98      0.98     80103\n",
            "           2       0.96      0.96      0.96     72809\n",
            "           3       0.97      0.95      0.96     71899\n",
            "           4       0.96      0.97      0.96     68081\n",
            "           5       0.95      0.96      0.96     61508\n",
            "           6       0.97      0.97      0.97     67151\n",
            "           7       0.96      0.97      0.96     70903\n",
            "           8       0.95      0.97      0.96     66679\n",
            "           9       0.95      0.95      0.95     70616\n",
            "\n",
            "    accuracy                           0.96    700000\n",
            "   macro avg       0.96      0.96      0.96    700000\n",
            "weighted avg       0.96      0.96      0.96    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.978 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     71246\n",
            "           1       0.99      0.98      0.98     81242\n",
            "           2       0.96      0.96      0.96     73861\n",
            "           3       0.97      0.95      0.96     72924\n",
            "           4       0.96      0.97      0.96     69050\n",
            "           5       0.95      0.96      0.96     62389\n",
            "           6       0.97      0.97      0.97     68117\n",
            "           7       0.96      0.97      0.96     71908\n",
            "           8       0.95      0.97      0.96     67645\n",
            "           9       0.95      0.95      0.95     71618\n",
            "\n",
            "    accuracy                           0.96    710000\n",
            "   macro avg       0.96      0.96      0.96    710000\n",
            "weighted avg       0.96      0.96      0.96    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.978 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     72247\n",
            "           1       0.99      0.98      0.98     82381\n",
            "           2       0.96      0.96      0.96     74899\n",
            "           3       0.97      0.95      0.96     73940\n",
            "           4       0.96      0.97      0.96     70027\n",
            "           5       0.95      0.96      0.96     63290\n",
            "           6       0.97      0.97      0.97     69070\n",
            "           7       0.96      0.97      0.96     72933\n",
            "           8       0.95      0.97      0.96     68597\n",
            "           9       0.95      0.95      0.95     72616\n",
            "\n",
            "    accuracy                           0.96    720000\n",
            "   macro avg       0.96      0.96      0.96    720000\n",
            "weighted avg       0.96      0.96      0.96    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     73240\n",
            "           1       0.99      0.98      0.98     83517\n",
            "           2       0.97      0.96      0.96     75929\n",
            "           3       0.97      0.95      0.96     74965\n",
            "           4       0.96      0.97      0.96     71006\n",
            "           5       0.95      0.96      0.96     64171\n",
            "           6       0.97      0.97      0.97     70039\n",
            "           7       0.96      0.97      0.96     73953\n",
            "           8       0.95      0.97      0.96     69562\n",
            "           9       0.95      0.95      0.95     73618\n",
            "\n",
            "    accuracy                           0.96    730000\n",
            "   macro avg       0.96      0.96      0.96    730000\n",
            "weighted avg       0.96      0.96      0.96    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.978 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     74234\n",
            "           1       0.99      0.98      0.98     84656\n",
            "           2       0.97      0.96      0.96     76981\n",
            "           3       0.97      0.95      0.96     75988\n",
            "           4       0.96      0.97      0.96     71975\n",
            "           5       0.95      0.97      0.96     65054\n",
            "           6       0.97      0.97      0.97     71006\n",
            "           7       0.96      0.97      0.96     74957\n",
            "           8       0.95      0.97      0.96     70528\n",
            "           9       0.95      0.95      0.95     74621\n",
            "\n",
            "    accuracy                           0.96    740000\n",
            "   macro avg       0.96      0.96      0.96    740000\n",
            "weighted avg       0.97      0.96      0.96    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.978 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     75235\n",
            "           1       0.99      0.98      0.98     85795\n",
            "           2       0.97      0.96      0.96     78018\n",
            "           3       0.97      0.95      0.96     77004\n",
            "           4       0.96      0.97      0.96     72952\n",
            "           5       0.95      0.97      0.96     65955\n",
            "           6       0.97      0.97      0.97     71958\n",
            "           7       0.96      0.97      0.96     75980\n",
            "           8       0.95      0.97      0.96     71482\n",
            "           9       0.95      0.95      0.95     75621\n",
            "\n",
            "    accuracy                           0.97    750000\n",
            "   macro avg       0.96      0.97      0.96    750000\n",
            "weighted avg       0.97      0.97      0.97    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.978 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     76227\n",
            "           1       0.99      0.98      0.98     86932\n",
            "           2       0.97      0.96      0.96     79051\n",
            "           3       0.97      0.95      0.96     78030\n",
            "           4       0.96      0.97      0.96     73933\n",
            "           5       0.95      0.97      0.96     66833\n",
            "           6       0.97      0.97      0.97     72927\n",
            "           7       0.96      0.97      0.96     76999\n",
            "           8       0.95      0.97      0.96     72447\n",
            "           9       0.95      0.95      0.95     76621\n",
            "\n",
            "    accuracy                           0.97    760000\n",
            "   macro avg       0.96      0.97      0.96    760000\n",
            "weighted avg       0.97      0.97      0.97    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.978 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     77221\n",
            "           1       0.99      0.98      0.98     88071\n",
            "           2       0.97      0.96      0.96     80103\n",
            "           3       0.97      0.95      0.96     79053\n",
            "           4       0.96      0.97      0.96     74903\n",
            "           5       0.95      0.97      0.96     67716\n",
            "           6       0.97      0.97      0.97     73893\n",
            "           7       0.96      0.97      0.96     78004\n",
            "           8       0.95      0.97      0.96     73415\n",
            "           9       0.95      0.95      0.95     77621\n",
            "\n",
            "    accuracy                           0.97    770000\n",
            "   macro avg       0.97      0.97      0.97    770000\n",
            "weighted avg       0.97      0.97      0.97    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.979 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     78221\n",
            "           1       0.99      0.98      0.98     89210\n",
            "           2       0.97      0.96      0.96     81140\n",
            "           3       0.97      0.95      0.96     80069\n",
            "           4       0.96      0.97      0.96     75882\n",
            "           5       0.95      0.97      0.96     68616\n",
            "           6       0.97      0.97      0.97     74846\n",
            "           7       0.96      0.97      0.96     79027\n",
            "           8       0.95      0.97      0.96     74370\n",
            "           9       0.95      0.95      0.95     78619\n",
            "\n",
            "    accuracy                           0.97    780000\n",
            "   macro avg       0.97      0.97      0.97    780000\n",
            "weighted avg       0.97      0.97      0.97    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.979 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     79213\n",
            "           1       0.99      0.98      0.98     90347\n",
            "           2       0.97      0.96      0.96     82174\n",
            "           3       0.97      0.95      0.96     81094\n",
            "           4       0.96      0.97      0.96     76863\n",
            "           5       0.95      0.97      0.96     69494\n",
            "           6       0.97      0.97      0.97     75815\n",
            "           7       0.96      0.97      0.96     80047\n",
            "           8       0.95      0.97      0.96     75334\n",
            "           9       0.95      0.95      0.95     79619\n",
            "\n",
            "    accuracy                           0.97    790000\n",
            "   macro avg       0.97      0.97      0.97    790000\n",
            "weighted avg       0.97      0.97      0.97    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.978 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     80206\n",
            "           1       0.99      0.98      0.98     91487\n",
            "           2       0.97      0.96      0.96     83225\n",
            "           3       0.97      0.95      0.96     82116\n",
            "           4       0.96      0.97      0.96     77832\n",
            "           5       0.95      0.97      0.96     70380\n",
            "           6       0.97      0.97      0.97     76781\n",
            "           7       0.96      0.97      0.96     81052\n",
            "           8       0.95      0.97      0.96     76303\n",
            "           9       0.95      0.95      0.95     80618\n",
            "\n",
            "    accuracy                           0.97    800000\n",
            "   macro avg       0.97      0.97      0.97    800000\n",
            "weighted avg       0.97      0.97      0.97    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.978 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     81204\n",
            "           1       0.99      0.98      0.98     92626\n",
            "           2       0.97      0.96      0.96     84262\n",
            "           3       0.97      0.95      0.96     83132\n",
            "           4       0.96      0.97      0.96     78810\n",
            "           5       0.95      0.97      0.96     71279\n",
            "           6       0.97      0.97      0.97     77734\n",
            "           7       0.96      0.97      0.96     82075\n",
            "           8       0.95      0.97      0.96     77258\n",
            "           9       0.95      0.96      0.95     81620\n",
            "\n",
            "    accuracy                           0.97    810000\n",
            "   macro avg       0.97      0.97      0.97    810000\n",
            "weighted avg       0.97      0.97      0.97    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.979 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     82196\n",
            "           1       0.99      0.98      0.98     93765\n",
            "           2       0.97      0.96      0.96     85295\n",
            "           3       0.97      0.95      0.96     84157\n",
            "           4       0.96      0.97      0.97     79791\n",
            "           5       0.95      0.97      0.96     72158\n",
            "           6       0.97      0.97      0.97     78702\n",
            "           7       0.96      0.97      0.96     83094\n",
            "           8       0.95      0.97      0.96     78222\n",
            "           9       0.95      0.96      0.95     82620\n",
            "\n",
            "    accuracy                           0.97    820000\n",
            "   macro avg       0.97      0.97      0.97    820000\n",
            "weighted avg       0.97      0.97      0.97    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     83189\n",
            "           1       0.99      0.98      0.98     94906\n",
            "           2       0.97      0.96      0.96     86345\n",
            "           3       0.97      0.95      0.96     85178\n",
            "           4       0.96      0.97      0.97     80760\n",
            "           5       0.95      0.97      0.96     73044\n",
            "           6       0.97      0.97      0.97     79669\n",
            "           7       0.96      0.97      0.96     84099\n",
            "           8       0.95      0.97      0.96     79190\n",
            "           9       0.95      0.96      0.95     83620\n",
            "\n",
            "    accuracy                           0.97    830000\n",
            "   macro avg       0.97      0.97      0.97    830000\n",
            "weighted avg       0.97      0.97      0.97    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.979 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     84188\n",
            "           1       0.99      0.98      0.99     96045\n",
            "           2       0.97      0.96      0.96     87380\n",
            "           3       0.97      0.95      0.96     86193\n",
            "           4       0.96      0.97      0.97     81739\n",
            "           5       0.95      0.97      0.96     73943\n",
            "           6       0.97      0.97      0.97     80621\n",
            "           7       0.96      0.97      0.96     85123\n",
            "           8       0.95      0.97      0.96     80144\n",
            "           9       0.95      0.96      0.96     84624\n",
            "\n",
            "    accuracy                           0.97    840000\n",
            "   macro avg       0.97      0.97      0.97    840000\n",
            "weighted avg       0.97      0.97      0.97    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.979 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     85181\n",
            "           1       0.99      0.98      0.99     97185\n",
            "           2       0.97      0.96      0.96     88410\n",
            "           3       0.97      0.95      0.96     87220\n",
            "           4       0.96      0.97      0.97     82718\n",
            "           5       0.95      0.97      0.96     74821\n",
            "           6       0.97      0.97      0.97     81588\n",
            "           7       0.96      0.97      0.96     86144\n",
            "           8       0.95      0.97      0.96     81107\n",
            "           9       0.95      0.96      0.96     85626\n",
            "\n",
            "    accuracy                           0.97    850000\n",
            "   macro avg       0.97      0.97      0.97    850000\n",
            "weighted avg       0.97      0.97      0.97    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     86174\n",
            "           1       0.99      0.98      0.99     98326\n",
            "           2       0.97      0.96      0.96     89459\n",
            "           3       0.97      0.95      0.96     88241\n",
            "           4       0.96      0.97      0.97     83686\n",
            "           5       0.95      0.97      0.96     75707\n",
            "           6       0.97      0.97      0.97     82555\n",
            "           7       0.96      0.97      0.96     87152\n",
            "           8       0.95      0.97      0.96     82073\n",
            "           9       0.95      0.96      0.96     86627\n",
            "\n",
            "    accuracy                           0.97    860000\n",
            "   macro avg       0.97      0.97      0.97    860000\n",
            "weighted avg       0.97      0.97      0.97    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.979 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     87172\n",
            "           1       0.99      0.98      0.99     99465\n",
            "           2       0.97      0.96      0.96     90495\n",
            "           3       0.97      0.95      0.96     89253\n",
            "           4       0.96      0.97      0.97     84667\n",
            "           5       0.95      0.97      0.96     76606\n",
            "           6       0.97      0.97      0.97     83507\n",
            "           7       0.96      0.97      0.96     88178\n",
            "           8       0.95      0.97      0.96     83030\n",
            "           9       0.95      0.96      0.96     87627\n",
            "\n",
            "    accuracy                           0.97    870000\n",
            "   macro avg       0.97      0.97      0.97    870000\n",
            "weighted avg       0.97      0.97      0.97    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.979 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     88165\n",
            "           1       0.99      0.98      0.99    100604\n",
            "           2       0.97      0.96      0.96     91525\n",
            "           3       0.97      0.95      0.96     90282\n",
            "           4       0.96      0.97      0.97     85646\n",
            "           5       0.95      0.97      0.96     77481\n",
            "           6       0.97      0.97      0.97     84473\n",
            "           7       0.96      0.97      0.97     89200\n",
            "           8       0.95      0.97      0.96     83995\n",
            "           9       0.95      0.96      0.96     88629\n",
            "\n",
            "    accuracy                           0.97    880000\n",
            "   macro avg       0.97      0.97      0.97    880000\n",
            "weighted avg       0.97      0.97      0.97    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     89157\n",
            "           1       0.99      0.98      0.99    101745\n",
            "           2       0.97      0.96      0.96     92574\n",
            "           3       0.97      0.95      0.96     91302\n",
            "           4       0.96      0.97      0.97     86614\n",
            "           5       0.96      0.97      0.96     78367\n",
            "           6       0.97      0.97      0.97     85441\n",
            "           7       0.96      0.97      0.97     90209\n",
            "           8       0.95      0.97      0.96     84962\n",
            "           9       0.96      0.96      0.96     89629\n",
            "\n",
            "    accuracy                           0.97    890000\n",
            "   macro avg       0.97      0.97      0.97    890000\n",
            "weighted avg       0.97      0.97      0.97    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.980 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     90155\n",
            "           1       0.99      0.98      0.99    102884\n",
            "           2       0.97      0.96      0.96     93611\n",
            "           3       0.97      0.95      0.96     92311\n",
            "           4       0.96      0.97      0.97     87594\n",
            "           5       0.96      0.97      0.96     79267\n",
            "           6       0.97      0.97      0.97     86394\n",
            "           7       0.96      0.97      0.97     91234\n",
            "           8       0.95      0.97      0.96     85919\n",
            "           9       0.96      0.96      0.96     90631\n",
            "\n",
            "    accuracy                           0.97    900000\n",
            "   macro avg       0.97      0.97      0.97    900000\n",
            "weighted avg       0.97      0.97      0.97    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Теперь сделаем такую же модель, но с batchorm и dropout, также попробуем другие модели![изображение_2024-03-27_030058614.png](data:image/png;base64,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![изображение_2024-03-27_030105988.png](data:image/png;base64,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))"
      ],
      "metadata": {
        "id": "CDsRPz3xN3qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.linear2 = nn.Linear(100, 10)\n",
        "    self.batch = nn.BatchNorm1d(100)\n",
        "    self.act = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear2(out)\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "Gg3D3g1lN9dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "rn9wem9mOA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLHbAb9VOHoq",
        "outputId": "a65ce14b-5d41-4e05-9e1e-d5d779bbfc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.916 Loss: 0.298\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      1006\n",
            "           1       0.98      0.92      0.95      1212\n",
            "           2       0.90      0.92      0.91      1012\n",
            "           3       0.90      0.90      0.90      1008\n",
            "           4       0.94      0.91      0.92      1015\n",
            "           5       0.87      0.90      0.88       863\n",
            "           6       0.95      0.94      0.95       964\n",
            "           7       0.90      0.93      0.92       998\n",
            "           8       0.84      0.91      0.87       905\n",
            "           9       0.89      0.89      0.89      1017\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.91      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.931 Loss: 0.240\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      2020\n",
            "           1       0.98      0.94      0.96      2372\n",
            "           2       0.91      0.92      0.92      2036\n",
            "           3       0.92      0.90      0.91      2056\n",
            "           4       0.93      0.92      0.93      1978\n",
            "           5       0.88      0.91      0.90      1730\n",
            "           6       0.96      0.94      0.95      1954\n",
            "           7       0.91      0.94      0.92      2001\n",
            "           8       0.85      0.92      0.88      1800\n",
            "           9       0.91      0.89      0.90      2053\n",
            "\n",
            "    accuracy                           0.92     20000\n",
            "   macro avg       0.92      0.92      0.92     20000\n",
            "weighted avg       0.92      0.92      0.92     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.934 Loss: 0.222\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      3029\n",
            "           1       0.98      0.94      0.96      3534\n",
            "           2       0.91      0.93      0.92      3026\n",
            "           3       0.92      0.91      0.91      3080\n",
            "           4       0.93      0.93      0.93      2943\n",
            "           5       0.90      0.91      0.90      2653\n",
            "           6       0.96      0.94      0.95      2917\n",
            "           7       0.92      0.94      0.93      3025\n",
            "           8       0.86      0.92      0.89      2752\n",
            "           9       0.91      0.91      0.91      3041\n",
            "\n",
            "    accuracy                           0.93     30000\n",
            "   macro avg       0.93      0.93      0.93     30000\n",
            "weighted avg       0.93      0.93      0.93     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.941 Loss: 0.197\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      4031\n",
            "           1       0.98      0.95      0.96      4713\n",
            "           2       0.92      0.93      0.92      4054\n",
            "           3       0.92      0.91      0.92      4091\n",
            "           4       0.93      0.93      0.93      3957\n",
            "           5       0.90      0.91      0.91      3524\n",
            "           6       0.96      0.95      0.95      3876\n",
            "           7       0.92      0.94      0.93      4045\n",
            "           8       0.87      0.92      0.90      3684\n",
            "           9       0.91      0.91      0.91      4025\n",
            "\n",
            "    accuracy                           0.93     40000\n",
            "   macro avg       0.93      0.93      0.93     40000\n",
            "weighted avg       0.93      0.93      0.93     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.948 Loss: 0.181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      5038\n",
            "           1       0.98      0.95      0.97      5856\n",
            "           2       0.92      0.93      0.93      5090\n",
            "           3       0.93      0.92      0.92      5118\n",
            "           4       0.94      0.93      0.93      4928\n",
            "           5       0.91      0.92      0.91      4390\n",
            "           6       0.96      0.95      0.95      4859\n",
            "           7       0.93      0.94      0.93      5052\n",
            "           8       0.88      0.93      0.90      4635\n",
            "           9       0.91      0.92      0.92      5034\n",
            "\n",
            "    accuracy                           0.93     50000\n",
            "   macro avg       0.93      0.93      0.93     50000\n",
            "weighted avg       0.93      0.93      0.93     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.946 Loss: 0.178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      6042\n",
            "           1       0.98      0.95      0.97      7007\n",
            "           2       0.92      0.94      0.93      6079\n",
            "           3       0.93      0.92      0.92      6128\n",
            "           4       0.93      0.94      0.93      5881\n",
            "           5       0.91      0.92      0.92      5293\n",
            "           6       0.96      0.95      0.95      5821\n",
            "           7       0.93      0.94      0.93      6058\n",
            "           8       0.89      0.92      0.91      5622\n",
            "           9       0.92      0.92      0.92      6069\n",
            "\n",
            "    accuracy                           0.94     60000\n",
            "   macro avg       0.94      0.94      0.94     60000\n",
            "weighted avg       0.94      0.94      0.94     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.950 Loss: 0.167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      7046\n",
            "           1       0.98      0.95      0.97      8180\n",
            "           2       0.92      0.94      0.93      7108\n",
            "           3       0.93      0.92      0.93      7167\n",
            "           4       0.94      0.93      0.94      6902\n",
            "           5       0.91      0.93      0.92      6148\n",
            "           6       0.96      0.95      0.96      6778\n",
            "           7       0.93      0.94      0.94      7069\n",
            "           8       0.89      0.93      0.91      6564\n",
            "           9       0.92      0.92      0.92      7038\n",
            "\n",
            "    accuracy                           0.94     70000\n",
            "   macro avg       0.94      0.94      0.94     70000\n",
            "weighted avg       0.94      0.94      0.94     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.953 Loss: 0.158\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      8049\n",
            "           1       0.98      0.96      0.97      9331\n",
            "           2       0.93      0.94      0.93      8135\n",
            "           3       0.93      0.92      0.93      8186\n",
            "           4       0.94      0.94      0.94      7894\n",
            "           5       0.92      0.93      0.93      7019\n",
            "           6       0.96      0.95      0.96      7754\n",
            "           7       0.93      0.95      0.94      8069\n",
            "           8       0.90      0.93      0.92      7525\n",
            "           9       0.92      0.92      0.92      8038\n",
            "\n",
            "    accuracy                           0.94     80000\n",
            "   macro avg       0.94      0.94      0.94     80000\n",
            "weighted avg       0.94      0.94      0.94     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.954 Loss: 0.154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      9051\n",
            "           1       0.98      0.96      0.97     10479\n",
            "           2       0.93      0.94      0.94      9135\n",
            "           3       0.94      0.93      0.93      9199\n",
            "           4       0.94      0.94      0.94      8870\n",
            "           5       0.92      0.93      0.93      7922\n",
            "           6       0.96      0.95      0.96      8709\n",
            "           7       0.93      0.95      0.94      9097\n",
            "           8       0.90      0.93      0.92      8494\n",
            "           9       0.92      0.93      0.92      9044\n",
            "\n",
            "    accuracy                           0.94     90000\n",
            "   macro avg       0.94      0.94      0.94     90000\n",
            "weighted avg       0.94      0.94      0.94     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.956 Loss: 0.148\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     10056\n",
            "           1       0.98      0.96      0.97     11641\n",
            "           2       0.93      0.94      0.94     10163\n",
            "           3       0.94      0.93      0.93     10236\n",
            "           4       0.94      0.94      0.94      9874\n",
            "           5       0.92      0.94      0.93      8770\n",
            "           6       0.96      0.95      0.96      9671\n",
            "           7       0.93      0.95      0.94     10110\n",
            "           8       0.91      0.93      0.92      9447\n",
            "           9       0.92      0.93      0.93     10032\n",
            "\n",
            "    accuracy                           0.94    100000\n",
            "   macro avg       0.94      0.94      0.94    100000\n",
            "weighted avg       0.94      0.94      0.94    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.959 Loss: 0.142\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     11058\n",
            "           1       0.98      0.96      0.97     12789\n",
            "           2       0.93      0.95      0.94     11203\n",
            "           3       0.94      0.93      0.93     11268\n",
            "           4       0.94      0.94      0.94     10850\n",
            "           5       0.92      0.94      0.93      9630\n",
            "           6       0.96      0.95      0.96     10646\n",
            "           7       0.93      0.95      0.94     11117\n",
            "           8       0.91      0.94      0.92     10402\n",
            "           9       0.92      0.93      0.93     11037\n",
            "\n",
            "    accuracy                           0.94    110000\n",
            "   macro avg       0.94      0.94      0.94    110000\n",
            "weighted avg       0.94      0.94      0.94    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.956 Loss: 0.145\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     12062\n",
            "           1       0.98      0.96      0.97     13938\n",
            "           2       0.93      0.95      0.94     12209\n",
            "           3       0.94      0.93      0.94     12310\n",
            "           4       0.94      0.94      0.94     11820\n",
            "           5       0.93      0.94      0.93     10513\n",
            "           6       0.96      0.95      0.96     11615\n",
            "           7       0.93      0.95      0.94     12131\n",
            "           8       0.91      0.94      0.92     11361\n",
            "           9       0.93      0.93      0.93     12041\n",
            "\n",
            "    accuracy                           0.95    120000\n",
            "   macro avg       0.94      0.95      0.94    120000\n",
            "weighted avg       0.95      0.95      0.95    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.959 Loss: 0.137\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     13063\n",
            "           1       0.98      0.96      0.97     15100\n",
            "           2       0.94      0.95      0.94     13235\n",
            "           3       0.94      0.93      0.94     13345\n",
            "           4       0.95      0.94      0.94     12829\n",
            "           5       0.93      0.94      0.94     11383\n",
            "           6       0.96      0.95      0.96     12576\n",
            "           7       0.94      0.95      0.94     13144\n",
            "           8       0.91      0.94      0.93     12306\n",
            "           9       0.93      0.93      0.93     13019\n",
            "\n",
            "    accuracy                           0.95    130000\n",
            "   macro avg       0.95      0.95      0.95    130000\n",
            "weighted avg       0.95      0.95      0.95    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.961 Loss: 0.132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     14061\n",
            "           1       0.98      0.96      0.97     16255\n",
            "           2       0.94      0.95      0.94     14273\n",
            "           3       0.95      0.93      0.94     14375\n",
            "           4       0.95      0.94      0.95     13819\n",
            "           5       0.93      0.95      0.94     12233\n",
            "           6       0.96      0.95      0.96     13552\n",
            "           7       0.94      0.95      0.94     14153\n",
            "           8       0.92      0.94      0.93     13268\n",
            "           9       0.93      0.93      0.93     14011\n",
            "\n",
            "    accuracy                           0.95    140000\n",
            "   macro avg       0.95      0.95      0.95    140000\n",
            "weighted avg       0.95      0.95      0.95    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.961 Loss: 0.130\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     15066\n",
            "           1       0.98      0.96      0.97     17407\n",
            "           2       0.94      0.95      0.94     15284\n",
            "           3       0.95      0.93      0.94     15399\n",
            "           4       0.95      0.94      0.95     14798\n",
            "           5       0.93      0.95      0.94     13129\n",
            "           6       0.96      0.96      0.96     14503\n",
            "           7       0.94      0.95      0.95     15174\n",
            "           8       0.92      0.94      0.93     14238\n",
            "           9       0.93      0.94      0.93     15002\n",
            "\n",
            "    accuracy                           0.95    150000\n",
            "   macro avg       0.95      0.95      0.95    150000\n",
            "weighted avg       0.95      0.95      0.95    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.963 Loss: 0.127\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     16063\n",
            "           1       0.98      0.96      0.97     18568\n",
            "           2       0.94      0.95      0.94     16314\n",
            "           3       0.95      0.93      0.94     16415\n",
            "           4       0.95      0.94      0.95     15798\n",
            "           5       0.93      0.95      0.94     14009\n",
            "           6       0.97      0.96      0.96     15473\n",
            "           7       0.94      0.95      0.95     16197\n",
            "           8       0.92      0.94      0.93     15177\n",
            "           9       0.93      0.94      0.93     15986\n",
            "\n",
            "    accuracy                           0.95    160000\n",
            "   macro avg       0.95      0.95      0.95    160000\n",
            "weighted avg       0.95      0.95      0.95    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.964 Loss: 0.124\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     17056\n",
            "           1       0.98      0.96      0.97     19719\n",
            "           2       0.94      0.95      0.95     17360\n",
            "           3       0.95      0.93      0.94     17429\n",
            "           4       0.95      0.95      0.95     16794\n",
            "           5       0.93      0.95      0.94     14873\n",
            "           6       0.97      0.96      0.96     16446\n",
            "           7       0.94      0.95      0.95     17215\n",
            "           8       0.92      0.94      0.93     16143\n",
            "           9       0.93      0.94      0.93     16965\n",
            "\n",
            "    accuracy                           0.95    170000\n",
            "   macro avg       0.95      0.95      0.95    170000\n",
            "weighted avg       0.95      0.95      0.95    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.962 Loss: 0.125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     18048\n",
            "           1       0.98      0.96      0.97     20869\n",
            "           2       0.94      0.95      0.95     18377\n",
            "           3       0.95      0.94      0.94     18444\n",
            "           4       0.95      0.95      0.95     17773\n",
            "           5       0.93      0.95      0.94     15785\n",
            "           6       0.97      0.96      0.96     17399\n",
            "           7       0.94      0.95      0.95     18240\n",
            "           8       0.92      0.95      0.93     17105\n",
            "           9       0.93      0.94      0.93     17960\n",
            "\n",
            "    accuracy                           0.95    180000\n",
            "   macro avg       0.95      0.95      0.95    180000\n",
            "weighted avg       0.95      0.95      0.95    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.965 Loss: 0.123\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     19048\n",
            "           1       0.98      0.96      0.97     22029\n",
            "           2       0.94      0.95      0.95     19409\n",
            "           3       0.95      0.94      0.94     19463\n",
            "           4       0.95      0.95      0.95     18772\n",
            "           5       0.93      0.95      0.94     16662\n",
            "           6       0.97      0.96      0.96     18369\n",
            "           7       0.94      0.95      0.95     19262\n",
            "           8       0.92      0.95      0.94     18050\n",
            "           9       0.93      0.94      0.94     18936\n",
            "\n",
            "    accuracy                           0.95    190000\n",
            "   macro avg       0.95      0.95      0.95    190000\n",
            "weighted avg       0.95      0.95      0.95    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.966 Loss: 0.118\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     20043\n",
            "           1       0.98      0.96      0.97     23179\n",
            "           2       0.94      0.95      0.95     20456\n",
            "           3       0.95      0.94      0.94     20476\n",
            "           4       0.95      0.95      0.95     19763\n",
            "           5       0.94      0.95      0.94     17539\n",
            "           6       0.97      0.96      0.96     19337\n",
            "           7       0.94      0.96      0.95     20272\n",
            "           8       0.93      0.95      0.94     19014\n",
            "           9       0.93      0.94      0.94     19921\n",
            "\n",
            "    accuracy                           0.95    200000\n",
            "   macro avg       0.95      0.95      0.95    200000\n",
            "weighted avg       0.95      0.95      0.95    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.964 Loss: 0.121\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     21048\n",
            "           1       0.99      0.97      0.97     24329\n",
            "           2       0.94      0.95      0.95     21473\n",
            "           3       0.95      0.94      0.94     21502\n",
            "           4       0.95      0.95      0.95     20744\n",
            "           5       0.94      0.95      0.94     18438\n",
            "           6       0.97      0.96      0.96     20285\n",
            "           7       0.94      0.96      0.95     21279\n",
            "           8       0.93      0.95      0.94     19977\n",
            "           9       0.93      0.94      0.94     20925\n",
            "\n",
            "    accuracy                           0.95    210000\n",
            "   macro avg       0.95      0.95      0.95    210000\n",
            "weighted avg       0.95      0.95      0.95    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.966 Loss: 0.117\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98     22046\n",
            "           1       0.99      0.97      0.98     25486\n",
            "           2       0.95      0.95      0.95     22519\n",
            "           3       0.95      0.94      0.95     22524\n",
            "           4       0.96      0.95      0.95     21743\n",
            "           5       0.94      0.95      0.94     19302\n",
            "           6       0.97      0.96      0.96     21244\n",
            "           7       0.94      0.96      0.95     22302\n",
            "           8       0.93      0.95      0.94     20927\n",
            "           9       0.93      0.94      0.94     21907\n",
            "\n",
            "    accuracy                           0.95    220000\n",
            "   macro avg       0.95      0.95      0.95    220000\n",
            "weighted avg       0.95      0.95      0.95    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.967 Loss: 0.115\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     23042\n",
            "           1       0.99      0.97      0.98     26634\n",
            "           2       0.95      0.95      0.95     23561\n",
            "           3       0.95      0.94      0.95     23553\n",
            "           4       0.96      0.95      0.95     22723\n",
            "           5       0.94      0.95      0.95     20170\n",
            "           6       0.97      0.96      0.96     22212\n",
            "           7       0.94      0.96      0.95     23320\n",
            "           8       0.93      0.95      0.94     21884\n",
            "           9       0.93      0.94      0.94     22901\n",
            "\n",
            "    accuracy                           0.95    230000\n",
            "   macro avg       0.95      0.95      0.95    230000\n",
            "weighted avg       0.95      0.95      0.95    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.966 Loss: 0.115\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     24036\n",
            "           1       0.99      0.97      0.98     27782\n",
            "           2       0.95      0.95      0.95     24582\n",
            "           3       0.95      0.94      0.95     24573\n",
            "           4       0.96      0.95      0.95     23702\n",
            "           5       0.94      0.95      0.95     21069\n",
            "           6       0.97      0.96      0.96     23168\n",
            "           7       0.94      0.96      0.95     24342\n",
            "           8       0.93      0.95      0.94     22852\n",
            "           9       0.93      0.95      0.94     23894\n",
            "\n",
            "    accuracy                           0.95    240000\n",
            "   macro avg       0.95      0.95      0.95    240000\n",
            "weighted avg       0.95      0.95      0.95    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.967 Loss: 0.113\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     25033\n",
            "           1       0.99      0.97      0.98     28938\n",
            "           2       0.95      0.95      0.95     25623\n",
            "           3       0.95      0.94      0.95     25581\n",
            "           4       0.96      0.95      0.95     24699\n",
            "           5       0.94      0.95      0.95     21953\n",
            "           6       0.97      0.96      0.96     24127\n",
            "           7       0.94      0.96      0.95     25370\n",
            "           8       0.93      0.95      0.94     23801\n",
            "           9       0.93      0.95      0.94     24875\n",
            "\n",
            "    accuracy                           0.96    250000\n",
            "   macro avg       0.95      0.95      0.95    250000\n",
            "weighted avg       0.96      0.96      0.96    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.969 Loss: 0.109\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     26031\n",
            "           1       0.99      0.97      0.98     30089\n",
            "           2       0.95      0.95      0.95     26666\n",
            "           3       0.95      0.94      0.95     26595\n",
            "           4       0.96      0.95      0.95     25687\n",
            "           5       0.94      0.96      0.95     22821\n",
            "           6       0.97      0.96      0.97     25089\n",
            "           7       0.95      0.96      0.95     26396\n",
            "           8       0.93      0.95      0.94     24762\n",
            "           9       0.93      0.95      0.94     25864\n",
            "\n",
            "    accuracy                           0.96    260000\n",
            "   macro avg       0.96      0.96      0.96    260000\n",
            "weighted avg       0.96      0.96      0.96    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.967 Loss: 0.113\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     27026\n",
            "           1       0.99      0.97      0.98     31238\n",
            "           2       0.95      0.96      0.95     27693\n",
            "           3       0.95      0.94      0.95     27613\n",
            "           4       0.96      0.95      0.96     26667\n",
            "           5       0.94      0.96      0.95     23713\n",
            "           6       0.97      0.96      0.97     26045\n",
            "           7       0.95      0.96      0.95     27426\n",
            "           8       0.93      0.95      0.94     25727\n",
            "           9       0.93      0.95      0.94     26852\n",
            "\n",
            "    accuracy                           0.96    270000\n",
            "   macro avg       0.96      0.96      0.96    270000\n",
            "weighted avg       0.96      0.96      0.96    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.968 Loss: 0.108\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     28019\n",
            "           1       0.99      0.97      0.98     32392\n",
            "           2       0.95      0.96      0.95     28728\n",
            "           3       0.96      0.94      0.95     28631\n",
            "           4       0.96      0.95      0.96     27670\n",
            "           5       0.94      0.96      0.95     24588\n",
            "           6       0.97      0.96      0.97     27007\n",
            "           7       0.95      0.96      0.95     28454\n",
            "           8       0.93      0.95      0.94     26687\n",
            "           9       0.93      0.95      0.94     27824\n",
            "\n",
            "    accuracy                           0.96    280000\n",
            "   macro avg       0.96      0.96      0.96    280000\n",
            "weighted avg       0.96      0.96      0.96    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.967 Loss: 0.107\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     29014\n",
            "           1       0.99      0.97      0.98     33542\n",
            "           2       0.95      0.96      0.95     29773\n",
            "           3       0.96      0.94      0.95     29649\n",
            "           4       0.96      0.95      0.96     28669\n",
            "           5       0.94      0.96      0.95     25464\n",
            "           6       0.97      0.96      0.97     27978\n",
            "           7       0.95      0.96      0.95     29473\n",
            "           8       0.93      0.95      0.94     27643\n",
            "           9       0.93      0.95      0.94     28795\n",
            "\n",
            "    accuracy                           0.96    290000\n",
            "   macro avg       0.96      0.96      0.96    290000\n",
            "weighted avg       0.96      0.96      0.96    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.968 Loss: 0.108\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     30012\n",
            "           1       0.99      0.97      0.98     34684\n",
            "           2       0.95      0.96      0.95     30793\n",
            "           3       0.96      0.94      0.95     30671\n",
            "           4       0.96      0.95      0.96     29651\n",
            "           5       0.94      0.96      0.95     26365\n",
            "           6       0.97      0.96      0.97     28933\n",
            "           7       0.95      0.96      0.95     30484\n",
            "           8       0.93      0.95      0.94     28621\n",
            "           9       0.94      0.95      0.94     29786\n",
            "\n",
            "    accuracy                           0.96    300000\n",
            "   macro avg       0.96      0.96      0.96    300000\n",
            "weighted avg       0.96      0.96      0.96    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.969 Loss: 0.107\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     31017\n",
            "           1       0.99      0.97      0.98     35836\n",
            "           2       0.95      0.96      0.95     31839\n",
            "           3       0.96      0.94      0.95     31696\n",
            "           4       0.96      0.95      0.96     30647\n",
            "           5       0.94      0.96      0.95     27244\n",
            "           6       0.97      0.96      0.97     29883\n",
            "           7       0.95      0.96      0.95     31503\n",
            "           8       0.93      0.95      0.94     29572\n",
            "           9       0.94      0.95      0.94     30763\n",
            "\n",
            "    accuracy                           0.96    310000\n",
            "   macro avg       0.96      0.96      0.96    310000\n",
            "weighted avg       0.96      0.96      0.96    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.970 Loss: 0.102\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     32015\n",
            "           1       0.99      0.97      0.98     36981\n",
            "           2       0.95      0.96      0.95     32877\n",
            "           3       0.96      0.95      0.95     32720\n",
            "           4       0.96      0.95      0.96     31638\n",
            "           5       0.94      0.96      0.95     28122\n",
            "           6       0.97      0.96      0.97     30844\n",
            "           7       0.95      0.96      0.95     32510\n",
            "           8       0.94      0.95      0.95     30533\n",
            "           9       0.94      0.95      0.94     31760\n",
            "\n",
            "    accuracy                           0.96    320000\n",
            "   macro avg       0.96      0.96      0.96    320000\n",
            "weighted avg       0.96      0.96      0.96    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.968 Loss: 0.105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     33012\n",
            "           1       0.99      0.97      0.98     38125\n",
            "           2       0.95      0.96      0.95     33908\n",
            "           3       0.96      0.95      0.95     33727\n",
            "           4       0.96      0.95      0.96     32614\n",
            "           5       0.94      0.96      0.95     29034\n",
            "           6       0.97      0.96      0.97     31801\n",
            "           7       0.95      0.96      0.95     33528\n",
            "           8       0.94      0.96      0.95     31501\n",
            "           9       0.94      0.95      0.94     32750\n",
            "\n",
            "    accuracy                           0.96    330000\n",
            "   macro avg       0.96      0.96      0.96    330000\n",
            "weighted avg       0.96      0.96      0.96    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.969 Loss: 0.105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     34006\n",
            "           1       0.99      0.97      0.98     39279\n",
            "           2       0.95      0.96      0.96     34943\n",
            "           3       0.96      0.95      0.95     34750\n",
            "           4       0.96      0.95      0.96     33611\n",
            "           5       0.94      0.96      0.95     29910\n",
            "           6       0.97      0.96      0.97     32758\n",
            "           7       0.95      0.96      0.95     34558\n",
            "           8       0.94      0.96      0.95     32459\n",
            "           9       0.94      0.95      0.94     33726\n",
            "\n",
            "    accuracy                           0.96    340000\n",
            "   macro avg       0.96      0.96      0.96    340000\n",
            "weighted avg       0.96      0.96      0.96    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.969 Loss: 0.103\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     35002\n",
            "           1       0.99      0.97      0.98     40422\n",
            "           2       0.95      0.96      0.96     35985\n",
            "           3       0.96      0.95      0.95     35780\n",
            "           4       0.96      0.95      0.96     34607\n",
            "           5       0.94      0.96      0.95     30781\n",
            "           6       0.97      0.96      0.97     33716\n",
            "           7       0.95      0.96      0.95     35573\n",
            "           8       0.94      0.96      0.95     33424\n",
            "           9       0.94      0.95      0.94     34710\n",
            "\n",
            "    accuracy                           0.96    350000\n",
            "   macro avg       0.96      0.96      0.96    350000\n",
            "weighted avg       0.96      0.96      0.96    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.969 Loss: 0.105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     36000\n",
            "           1       0.99      0.97      0.98     41564\n",
            "           2       0.95      0.96      0.96     37003\n",
            "           3       0.96      0.95      0.95     36809\n",
            "           4       0.96      0.96      0.96     35596\n",
            "           5       0.95      0.96      0.95     31678\n",
            "           6       0.97      0.96      0.97     34667\n",
            "           7       0.95      0.96      0.95     36601\n",
            "           8       0.94      0.96      0.95     34394\n",
            "           9       0.94      0.95      0.95     35688\n",
            "\n",
            "    accuracy                           0.96    360000\n",
            "   macro avg       0.96      0.96      0.96    360000\n",
            "weighted avg       0.96      0.96      0.96    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.969 Loss: 0.101\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     37006\n",
            "           1       0.99      0.97      0.98     42717\n",
            "           2       0.95      0.96      0.96     38038\n",
            "           3       0.96      0.95      0.95     37826\n",
            "           4       0.96      0.96      0.96     36586\n",
            "           5       0.95      0.96      0.95     32559\n",
            "           6       0.97      0.96      0.97     35619\n",
            "           7       0.95      0.96      0.95     37626\n",
            "           8       0.94      0.96      0.95     35350\n",
            "           9       0.94      0.95      0.95     36673\n",
            "\n",
            "    accuracy                           0.96    370000\n",
            "   macro avg       0.96      0.96      0.96    370000\n",
            "weighted avg       0.96      0.96      0.96    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.970 Loss: 0.099\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     38000\n",
            "           1       0.99      0.97      0.98     43861\n",
            "           2       0.95      0.96      0.96     39077\n",
            "           3       0.96      0.95      0.95     38847\n",
            "           4       0.96      0.96      0.96     37581\n",
            "           5       0.95      0.96      0.95     33438\n",
            "           6       0.97      0.97      0.97     36585\n",
            "           7       0.95      0.96      0.96     38650\n",
            "           8       0.94      0.96      0.95     36308\n",
            "           9       0.94      0.95      0.95     37653\n",
            "\n",
            "    accuracy                           0.96    380000\n",
            "   macro avg       0.96      0.96      0.96    380000\n",
            "weighted avg       0.96      0.96      0.96    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.970 Loss: 0.100\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     38998\n",
            "           1       0.99      0.97      0.98     45007\n",
            "           2       0.95      0.96      0.96     40103\n",
            "           3       0.96      0.95      0.95     39870\n",
            "           4       0.96      0.96      0.96     38559\n",
            "           5       0.95      0.96      0.95     34324\n",
            "           6       0.97      0.97      0.97     37539\n",
            "           7       0.95      0.96      0.96     39677\n",
            "           8       0.94      0.96      0.95     37275\n",
            "           9       0.94      0.96      0.95     38648\n",
            "\n",
            "    accuracy                           0.96    390000\n",
            "   macro avg       0.96      0.96      0.96    390000\n",
            "weighted avg       0.96      0.96      0.96    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.970 Loss: 0.101\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     39997\n",
            "           1       0.99      0.97      0.98     46153\n",
            "           2       0.96      0.96      0.96     41139\n",
            "           3       0.96      0.95      0.95     40876\n",
            "           4       0.96      0.96      0.96     39555\n",
            "           5       0.95      0.96      0.95     35211\n",
            "           6       0.97      0.97      0.97     38499\n",
            "           7       0.95      0.96      0.96     40715\n",
            "           8       0.94      0.96      0.95     38229\n",
            "           9       0.94      0.96      0.95     39626\n",
            "\n",
            "    accuracy                           0.96    400000\n",
            "   macro avg       0.96      0.96      0.96    400000\n",
            "weighted avg       0.96      0.96      0.96    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.972 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     40989\n",
            "           1       0.99      0.97      0.98     47295\n",
            "           2       0.96      0.96      0.96     42181\n",
            "           3       0.96      0.95      0.95     41898\n",
            "           4       0.96      0.96      0.96     40548\n",
            "           5       0.95      0.96      0.95     36088\n",
            "           6       0.97      0.97      0.97     39462\n",
            "           7       0.95      0.96      0.96     41738\n",
            "           8       0.94      0.96      0.95     39190\n",
            "           9       0.94      0.96      0.95     40611\n",
            "\n",
            "    accuracy                           0.96    410000\n",
            "   macro avg       0.96      0.96      0.96    410000\n",
            "weighted avg       0.96      0.96      0.96    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.969 Loss: 0.100\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     41981\n",
            "           1       0.99      0.97      0.98     48434\n",
            "           2       0.96      0.96      0.96     43203\n",
            "           3       0.96      0.95      0.95     42921\n",
            "           4       0.96      0.96      0.96     41525\n",
            "           5       0.95      0.96      0.95     36988\n",
            "           6       0.97      0.97      0.97     40413\n",
            "           7       0.95      0.96      0.96     42763\n",
            "           8       0.94      0.96      0.95     40159\n",
            "           9       0.94      0.96      0.95     41613\n",
            "\n",
            "    accuracy                           0.96    420000\n",
            "   macro avg       0.96      0.96      0.96    420000\n",
            "weighted avg       0.96      0.96      0.96    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.971 Loss: 0.100\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     42973\n",
            "           1       0.99      0.97      0.98     49578\n",
            "           2       0.96      0.96      0.96     44247\n",
            "           3       0.96      0.95      0.96     43942\n",
            "           4       0.96      0.96      0.96     42516\n",
            "           5       0.95      0.96      0.95     37871\n",
            "           6       0.97      0.97      0.97     41374\n",
            "           7       0.95      0.96      0.96     43798\n",
            "           8       0.94      0.96      0.95     41107\n",
            "           9       0.94      0.96      0.95     42594\n",
            "\n",
            "    accuracy                           0.96    430000\n",
            "   macro avg       0.96      0.96      0.96    430000\n",
            "weighted avg       0.96      0.96      0.96    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.972 Loss: 0.096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     43963\n",
            "           1       0.99      0.97      0.98     50719\n",
            "           2       0.96      0.96      0.96     45295\n",
            "           3       0.96      0.95      0.96     44966\n",
            "           4       0.96      0.96      0.96     43491\n",
            "           5       0.95      0.96      0.95     38756\n",
            "           6       0.97      0.97      0.97     42337\n",
            "           7       0.95      0.96      0.96     44817\n",
            "           8       0.94      0.96      0.95     42061\n",
            "           9       0.94      0.96      0.95     43595\n",
            "\n",
            "    accuracy                           0.96    440000\n",
            "   macro avg       0.96      0.96      0.96    440000\n",
            "weighted avg       0.96      0.96      0.96    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.971 Loss: 0.097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     44955\n",
            "           1       0.99      0.97      0.98     51860\n",
            "           2       0.96      0.96      0.96     46325\n",
            "           3       0.96      0.95      0.96     45983\n",
            "           4       0.96      0.96      0.96     44468\n",
            "           5       0.95      0.96      0.95     39654\n",
            "           6       0.97      0.97      0.97     43295\n",
            "           7       0.95      0.96      0.96     45843\n",
            "           8       0.94      0.96      0.95     43026\n",
            "           9       0.94      0.96      0.95     44591\n",
            "\n",
            "    accuracy                           0.96    450000\n",
            "   macro avg       0.96      0.96      0.96    450000\n",
            "weighted avg       0.96      0.96      0.96    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.971 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     45955\n",
            "           1       0.99      0.97      0.98     53006\n",
            "           2       0.96      0.96      0.96     47365\n",
            "           3       0.96      0.95      0.96     46994\n",
            "           4       0.96      0.96      0.96     45469\n",
            "           5       0.95      0.96      0.96     40536\n",
            "           6       0.97      0.97      0.97     44252\n",
            "           7       0.95      0.96      0.96     46872\n",
            "           8       0.94      0.96      0.95     43986\n",
            "           9       0.94      0.96      0.95     45565\n",
            "\n",
            "    accuracy                           0.96    460000\n",
            "   macro avg       0.96      0.96      0.96    460000\n",
            "weighted avg       0.96      0.96      0.96    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.972 Loss: 0.096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     46948\n",
            "           1       0.99      0.97      0.98     54148\n",
            "           2       0.96      0.96      0.96     48411\n",
            "           3       0.96      0.95      0.96     48011\n",
            "           4       0.96      0.96      0.96     46461\n",
            "           5       0.95      0.96      0.96     41411\n",
            "           6       0.97      0.97      0.97     45211\n",
            "           7       0.95      0.96      0.96     47899\n",
            "           8       0.94      0.96      0.95     44950\n",
            "           9       0.94      0.96      0.95     46550\n",
            "\n",
            "    accuracy                           0.96    470000\n",
            "   macro avg       0.96      0.96      0.96    470000\n",
            "weighted avg       0.96      0.96      0.96    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.972 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     47940\n",
            "           1       0.99      0.97      0.98     55296\n",
            "           2       0.96      0.96      0.96     49439\n",
            "           3       0.96      0.95      0.96     49021\n",
            "           4       0.96      0.96      0.96     47447\n",
            "           5       0.95      0.96      0.96     42314\n",
            "           6       0.97      0.97      0.97     46166\n",
            "           7       0.95      0.96      0.96     48928\n",
            "           8       0.94      0.96      0.95     45911\n",
            "           9       0.94      0.96      0.95     47538\n",
            "\n",
            "    accuracy                           0.96    480000\n",
            "   macro avg       0.96      0.96      0.96    480000\n",
            "weighted avg       0.96      0.96      0.96    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.972 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     48935\n",
            "           1       0.99      0.97      0.98     56442\n",
            "           2       0.96      0.96      0.96     50474\n",
            "           3       0.96      0.95      0.96     50031\n",
            "           4       0.97      0.96      0.96     48441\n",
            "           5       0.95      0.96      0.96     43201\n",
            "           6       0.97      0.97      0.97     47126\n",
            "           7       0.95      0.96      0.96     49962\n",
            "           8       0.94      0.96      0.95     46869\n",
            "           9       0.94      0.96      0.95     48519\n",
            "\n",
            "    accuracy                           0.96    490000\n",
            "   macro avg       0.96      0.96      0.96    490000\n",
            "weighted avg       0.96      0.96      0.96    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.972 Loss: 0.095\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     49925\n",
            "           1       0.99      0.97      0.98     57588\n",
            "           2       0.96      0.96      0.96     51521\n",
            "           3       0.96      0.95      0.96     51045\n",
            "           4       0.97      0.96      0.96     49427\n",
            "           5       0.95      0.96      0.96     44082\n",
            "           6       0.97      0.97      0.97     48087\n",
            "           7       0.95      0.96      0.96     50991\n",
            "           8       0.94      0.96      0.95     47824\n",
            "           9       0.94      0.96      0.95     49510\n",
            "\n",
            "    accuracy                           0.96    500000\n",
            "   macro avg       0.96      0.96      0.96    500000\n",
            "weighted avg       0.96      0.96      0.96    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.971 Loss: 0.095\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     50921\n",
            "           1       0.99      0.97      0.98     58737\n",
            "           2       0.96      0.96      0.96     52549\n",
            "           3       0.96      0.95      0.96     52049\n",
            "           4       0.97      0.96      0.96     50413\n",
            "           5       0.95      0.96      0.96     44983\n",
            "           6       0.97      0.97      0.97     49032\n",
            "           7       0.95      0.96      0.96     52023\n",
            "           8       0.94      0.96      0.95     48790\n",
            "           9       0.94      0.96      0.95     50503\n",
            "\n",
            "    accuracy                           0.96    510000\n",
            "   macro avg       0.96      0.96      0.96    510000\n",
            "weighted avg       0.96      0.96      0.96    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.972 Loss: 0.097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     51917\n",
            "           1       0.99      0.97      0.98     59885\n",
            "           2       0.96      0.96      0.96     53581\n",
            "           3       0.96      0.95      0.96     53073\n",
            "           4       0.97      0.96      0.96     51410\n",
            "           5       0.95      0.96      0.96     45863\n",
            "           6       0.97      0.97      0.97     49986\n",
            "           7       0.95      0.96      0.96     53055\n",
            "           8       0.94      0.96      0.95     49744\n",
            "           9       0.94      0.96      0.95     51486\n",
            "\n",
            "    accuracy                           0.96    520000\n",
            "   macro avg       0.96      0.96      0.96    520000\n",
            "weighted avg       0.96      0.96      0.96    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.972 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     52911\n",
            "           1       0.99      0.97      0.98     61026\n",
            "           2       0.96      0.96      0.96     54626\n",
            "           3       0.96      0.95      0.96     54100\n",
            "           4       0.97      0.96      0.96     52402\n",
            "           5       0.95      0.96      0.96     46736\n",
            "           6       0.97      0.97      0.97     50947\n",
            "           7       0.95      0.96      0.96     54076\n",
            "           8       0.94      0.96      0.95     50704\n",
            "           9       0.94      0.96      0.95     52472\n",
            "\n",
            "    accuracy                           0.96    530000\n",
            "   macro avg       0.96      0.96      0.96    530000\n",
            "weighted avg       0.96      0.96      0.96    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.972 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     53907\n",
            "           1       0.99      0.97      0.98     62164\n",
            "           2       0.96      0.96      0.96     55655\n",
            "           3       0.96      0.95      0.96     55119\n",
            "           4       0.97      0.96      0.96     53392\n",
            "           5       0.95      0.96      0.96     47632\n",
            "           6       0.97      0.97      0.97     51901\n",
            "           7       0.95      0.96      0.96     55106\n",
            "           8       0.94      0.96      0.95     51672\n",
            "           9       0.94      0.96      0.95     53452\n",
            "\n",
            "    accuracy                           0.96    540000\n",
            "   macro avg       0.96      0.96      0.96    540000\n",
            "weighted avg       0.96      0.96      0.96    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.971 Loss: 0.096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     54900\n",
            "           1       0.99      0.97      0.98     63312\n",
            "           2       0.96      0.96      0.96     56697\n",
            "           3       0.96      0.95      0.96     56132\n",
            "           4       0.97      0.96      0.96     54388\n",
            "           5       0.95      0.96      0.96     48513\n",
            "           6       0.97      0.97      0.97     52862\n",
            "           7       0.95      0.96      0.96     56136\n",
            "           8       0.94      0.96      0.95     52629\n",
            "           9       0.94      0.96      0.95     54431\n",
            "\n",
            "    accuracy                           0.96    550000\n",
            "   macro avg       0.96      0.96      0.96    550000\n",
            "weighted avg       0.96      0.96      0.96    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.972 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     55892\n",
            "           1       0.99      0.97      0.98     64458\n",
            "           2       0.96      0.96      0.96     57737\n",
            "           3       0.96      0.95      0.96     57156\n",
            "           4       0.97      0.96      0.96     55371\n",
            "           5       0.95      0.96      0.96     49396\n",
            "           6       0.97      0.97      0.97     53828\n",
            "           7       0.96      0.96      0.96     57159\n",
            "           8       0.94      0.96      0.95     53585\n",
            "           9       0.94      0.96      0.95     55418\n",
            "\n",
            "    accuracy                           0.96    560000\n",
            "   macro avg       0.96      0.96      0.96    560000\n",
            "weighted avg       0.96      0.96      0.96    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.972 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     56886\n",
            "           1       0.99      0.97      0.98     65597\n",
            "           2       0.96      0.96      0.96     58767\n",
            "           3       0.96      0.95      0.96     58173\n",
            "           4       0.97      0.96      0.96     56346\n",
            "           5       0.95      0.96      0.96     50293\n",
            "           6       0.97      0.97      0.97     54778\n",
            "           7       0.96      0.96      0.96     58189\n",
            "           8       0.94      0.96      0.95     54559\n",
            "           9       0.94      0.96      0.95     56412\n",
            "\n",
            "    accuracy                           0.96    570000\n",
            "   macro avg       0.96      0.96      0.96    570000\n",
            "weighted avg       0.96      0.96      0.96    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.973 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     57879\n",
            "           1       0.99      0.97      0.98     66743\n",
            "           2       0.96      0.96      0.96     59807\n",
            "           3       0.96      0.95      0.96     59193\n",
            "           4       0.97      0.96      0.96     57334\n",
            "           5       0.95      0.96      0.96     51180\n",
            "           6       0.97      0.97      0.97     55737\n",
            "           7       0.96      0.96      0.96     59220\n",
            "           8       0.94      0.96      0.95     55509\n",
            "           9       0.94      0.96      0.95     57398\n",
            "\n",
            "    accuracy                           0.96    580000\n",
            "   macro avg       0.96      0.96      0.96    580000\n",
            "weighted avg       0.96      0.96      0.96    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.973 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     58870\n",
            "           1       0.99      0.97      0.98     67885\n",
            "           2       0.96      0.96      0.96     60845\n",
            "           3       0.96      0.95      0.96     60220\n",
            "           4       0.97      0.96      0.96     58318\n",
            "           5       0.95      0.96      0.96     52060\n",
            "           6       0.97      0.97      0.97     56700\n",
            "           7       0.96      0.96      0.96     60241\n",
            "           8       0.95      0.96      0.95     56470\n",
            "           9       0.94      0.96      0.95     58391\n",
            "\n",
            "    accuracy                           0.96    590000\n",
            "   macro avg       0.96      0.96      0.96    590000\n",
            "weighted avg       0.96      0.96      0.96    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.972 Loss: 0.093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     59861\n",
            "           1       0.99      0.97      0.98     69027\n",
            "           2       0.96      0.96      0.96     61873\n",
            "           3       0.96      0.95      0.96     61235\n",
            "           4       0.97      0.96      0.96     59302\n",
            "           5       0.95      0.96      0.96     52960\n",
            "           6       0.97      0.97      0.97     57648\n",
            "           7       0.96      0.96      0.96     61276\n",
            "           8       0.95      0.96      0.95     57438\n",
            "           9       0.94      0.96      0.95     59380\n",
            "\n",
            "    accuracy                           0.96    600000\n",
            "   macro avg       0.96      0.96      0.96    600000\n",
            "weighted avg       0.96      0.96      0.96    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.973 Loss: 0.093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     60855\n",
            "           1       0.99      0.97      0.98     70177\n",
            "           2       0.96      0.96      0.96     62913\n",
            "           3       0.96      0.95      0.96     62252\n",
            "           4       0.97      0.96      0.96     60291\n",
            "           5       0.95      0.96      0.96     53845\n",
            "           6       0.97      0.97      0.97     58605\n",
            "           7       0.96      0.96      0.96     62300\n",
            "           8       0.95      0.96      0.95     58395\n",
            "           9       0.94      0.96      0.95     60367\n",
            "\n",
            "    accuracy                           0.96    610000\n",
            "   macro avg       0.96      0.96      0.96    610000\n",
            "weighted avg       0.96      0.96      0.96    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.972 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     61844\n",
            "           1       0.99      0.97      0.98     71320\n",
            "           2       0.96      0.96      0.96     63954\n",
            "           3       0.96      0.95      0.96     63277\n",
            "           4       0.97      0.96      0.96     61272\n",
            "           5       0.95      0.96      0.96     54731\n",
            "           6       0.97      0.97      0.97     59568\n",
            "           7       0.96      0.96      0.96     63323\n",
            "           8       0.95      0.96      0.95     59351\n",
            "           9       0.94      0.96      0.95     61360\n",
            "\n",
            "    accuracy                           0.96    620000\n",
            "   macro avg       0.96      0.96      0.96    620000\n",
            "weighted avg       0.96      0.96      0.96    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.973 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     62837\n",
            "           1       0.99      0.97      0.98     72461\n",
            "           2       0.96      0.96      0.96     64978\n",
            "           3       0.96      0.95      0.96     64290\n",
            "           4       0.97      0.96      0.96     62254\n",
            "           5       0.95      0.96      0.96     55632\n",
            "           6       0.97      0.97      0.97     60522\n",
            "           7       0.96      0.96      0.96     64359\n",
            "           8       0.95      0.96      0.95     60317\n",
            "           9       0.94      0.96      0.95     62350\n",
            "\n",
            "    accuracy                           0.96    630000\n",
            "   macro avg       0.96      0.96      0.96    630000\n",
            "weighted avg       0.96      0.96      0.96    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.974 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     63832\n",
            "           1       0.99      0.97      0.98     73603\n",
            "           2       0.96      0.96      0.96     66019\n",
            "           3       0.96      0.95      0.96     65308\n",
            "           4       0.97      0.96      0.96     63243\n",
            "           5       0.95      0.96      0.96     56517\n",
            "           6       0.97      0.97      0.97     61478\n",
            "           7       0.96      0.96      0.96     65393\n",
            "           8       0.95      0.96      0.95     61271\n",
            "           9       0.94      0.96      0.95     63336\n",
            "\n",
            "    accuracy                           0.96    640000\n",
            "   macro avg       0.96      0.96      0.96    640000\n",
            "weighted avg       0.96      0.96      0.96    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.973 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     64826\n",
            "           1       0.99      0.97      0.98     74748\n",
            "           2       0.96      0.96      0.96     67066\n",
            "           3       0.96      0.96      0.96     66329\n",
            "           4       0.97      0.96      0.96     64230\n",
            "           5       0.95      0.96      0.96     57389\n",
            "           6       0.97      0.97      0.97     62439\n",
            "           7       0.96      0.96      0.96     66415\n",
            "           8       0.95      0.96      0.95     62242\n",
            "           9       0.94      0.96      0.95     64316\n",
            "\n",
            "    accuracy                           0.96    650000\n",
            "   macro avg       0.96      0.96      0.96    650000\n",
            "weighted avg       0.96      0.96      0.96    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.973 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     65814\n",
            "           1       0.99      0.97      0.98     75886\n",
            "           2       0.96      0.96      0.96     68102\n",
            "           3       0.97      0.96      0.96     67340\n",
            "           4       0.97      0.96      0.96     65207\n",
            "           5       0.95      0.96      0.96     58295\n",
            "           6       0.97      0.97      0.97     63388\n",
            "           7       0.96      0.96      0.96     67444\n",
            "           8       0.95      0.96      0.95     63211\n",
            "           9       0.94      0.96      0.95     65313\n",
            "\n",
            "    accuracy                           0.96    660000\n",
            "   macro avg       0.96      0.96      0.96    660000\n",
            "weighted avg       0.97      0.96      0.96    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.973 Loss: 0.094\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     66811\n",
            "           1       0.99      0.97      0.98     77027\n",
            "           2       0.96      0.96      0.96     69142\n",
            "           3       0.97      0.96      0.96     68351\n",
            "           4       0.97      0.96      0.97     66196\n",
            "           5       0.95      0.96      0.96     59180\n",
            "           6       0.97      0.97      0.97     64351\n",
            "           7       0.96      0.96      0.96     68480\n",
            "           8       0.95      0.96      0.95     64171\n",
            "           9       0.94      0.96      0.95     66291\n",
            "\n",
            "    accuracy                           0.96    670000\n",
            "   macro avg       0.96      0.96      0.96    670000\n",
            "weighted avg       0.97      0.96      0.97    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     67804\n",
            "           1       0.99      0.97      0.98     78167\n",
            "           2       0.96      0.96      0.96     70194\n",
            "           3       0.97      0.96      0.96     69370\n",
            "           4       0.97      0.96      0.97     67183\n",
            "           5       0.95      0.96      0.96     60063\n",
            "           6       0.97      0.97      0.97     65313\n",
            "           7       0.96      0.96      0.96     69505\n",
            "           8       0.95      0.96      0.95     65132\n",
            "           9       0.95      0.96      0.95     67269\n",
            "\n",
            "    accuracy                           0.97    680000\n",
            "   macro avg       0.96      0.97      0.96    680000\n",
            "weighted avg       0.97      0.97      0.97    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.975 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     68791\n",
            "           1       0.99      0.98      0.98     79310\n",
            "           2       0.96      0.96      0.96     71221\n",
            "           3       0.97      0.96      0.96     70383\n",
            "           4       0.97      0.96      0.97     68164\n",
            "           5       0.95      0.96      0.96     60953\n",
            "           6       0.97      0.97      0.97     66270\n",
            "           7       0.96      0.96      0.96     70535\n",
            "           8       0.95      0.96      0.96     66108\n",
            "           9       0.95      0.96      0.95     68265\n",
            "\n",
            "    accuracy                           0.97    690000\n",
            "   macro avg       0.96      0.97      0.97    690000\n",
            "weighted avg       0.97      0.97      0.97    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.974 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     69781\n",
            "           1       0.99      0.98      0.98     80455\n",
            "           2       0.96      0.96      0.96     72262\n",
            "           3       0.97      0.96      0.96     71393\n",
            "           4       0.97      0.96      0.97     69150\n",
            "           5       0.95      0.96      0.96     61843\n",
            "           6       0.97      0.97      0.97     67225\n",
            "           7       0.96      0.96      0.96     71570\n",
            "           8       0.95      0.96      0.96     67061\n",
            "           9       0.95      0.96      0.95     69260\n",
            "\n",
            "    accuracy                           0.97    700000\n",
            "   macro avg       0.97      0.97      0.97    700000\n",
            "weighted avg       0.97      0.97      0.97    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.974 Loss: 0.089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     70770\n",
            "           1       0.99      0.98      0.98     81597\n",
            "           2       0.96      0.96      0.96     73305\n",
            "           3       0.97      0.96      0.96     72425\n",
            "           4       0.97      0.96      0.97     70128\n",
            "           5       0.95      0.96      0.96     62717\n",
            "           6       0.97      0.97      0.97     68191\n",
            "           7       0.96      0.96      0.96     72595\n",
            "           8       0.95      0.96      0.96     68025\n",
            "           9       0.95      0.96      0.95     70247\n",
            "\n",
            "    accuracy                           0.97    710000\n",
            "   macro avg       0.97      0.97      0.97    710000\n",
            "weighted avg       0.97      0.97      0.97    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.974 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     71762\n",
            "           1       0.99      0.98      0.98     82739\n",
            "           2       0.96      0.96      0.96     74333\n",
            "           3       0.97      0.96      0.96     73439\n",
            "           4       0.97      0.96      0.97     71108\n",
            "           5       0.95      0.96      0.96     63618\n",
            "           6       0.97      0.97      0.97     69140\n",
            "           7       0.96      0.96      0.96     73632\n",
            "           8       0.95      0.96      0.96     68995\n",
            "           9       0.95      0.96      0.96     71234\n",
            "\n",
            "    accuracy                           0.97    720000\n",
            "   macro avg       0.97      0.97      0.97    720000\n",
            "weighted avg       0.97      0.97      0.97    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.974 Loss: 0.093\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     72760\n",
            "           1       0.99      0.98      0.98     83883\n",
            "           2       0.96      0.96      0.96     75375\n",
            "           3       0.97      0.96      0.96     74449\n",
            "           4       0.97      0.96      0.97     72093\n",
            "           5       0.96      0.96      0.96     64511\n",
            "           6       0.97      0.97      0.97     70095\n",
            "           7       0.96      0.96      0.96     74660\n",
            "           8       0.95      0.96      0.96     69953\n",
            "           9       0.95      0.96      0.96     72221\n",
            "\n",
            "    accuracy                           0.97    730000\n",
            "   macro avg       0.97      0.97      0.97    730000\n",
            "weighted avg       0.97      0.97      0.97    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     73748\n",
            "           1       0.99      0.98      0.98     85027\n",
            "           2       0.96      0.96      0.96     76423\n",
            "           3       0.97      0.96      0.96     75472\n",
            "           4       0.97      0.96      0.97     73079\n",
            "           5       0.96      0.96      0.96     65378\n",
            "           6       0.97      0.97      0.97     71058\n",
            "           7       0.96      0.96      0.96     75674\n",
            "           8       0.95      0.96      0.96     70922\n",
            "           9       0.95      0.96      0.96     73219\n",
            "\n",
            "    accuracy                           0.97    740000\n",
            "   macro avg       0.97      0.97      0.97    740000\n",
            "weighted avg       0.97      0.97      0.97    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.974 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     74741\n",
            "           1       0.99      0.98      0.98     86173\n",
            "           2       0.96      0.96      0.96     77449\n",
            "           3       0.97      0.96      0.96     76492\n",
            "           4       0.97      0.96      0.97     74061\n",
            "           5       0.96      0.96      0.96     66266\n",
            "           6       0.97      0.97      0.97     72008\n",
            "           7       0.96      0.96      0.96     76710\n",
            "           8       0.95      0.96      0.96     71891\n",
            "           9       0.95      0.97      0.96     74209\n",
            "\n",
            "    accuracy                           0.97    750000\n",
            "   macro avg       0.97      0.97      0.97    750000\n",
            "weighted avg       0.97      0.97      0.97    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.974 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     75739\n",
            "           1       0.99      0.98      0.98     87319\n",
            "           2       0.96      0.96      0.96     78487\n",
            "           3       0.97      0.96      0.96     77504\n",
            "           4       0.97      0.96      0.97     75050\n",
            "           5       0.96      0.96      0.96     67145\n",
            "           6       0.97      0.97      0.97     72964\n",
            "           7       0.96      0.96      0.96     77745\n",
            "           8       0.95      0.96      0.96     72858\n",
            "           9       0.95      0.97      0.96     75189\n",
            "\n",
            "    accuracy                           0.97    760000\n",
            "   macro avg       0.97      0.97      0.97    760000\n",
            "weighted avg       0.97      0.97      0.97    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.974 Loss: 0.087\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     76730\n",
            "           1       0.99      0.98      0.98     88460\n",
            "           2       0.96      0.96      0.96     79531\n",
            "           3       0.97      0.96      0.96     78528\n",
            "           4       0.97      0.96      0.97     76026\n",
            "           5       0.96      0.96      0.96     68026\n",
            "           6       0.97      0.97      0.97     73925\n",
            "           7       0.96      0.96      0.96     78772\n",
            "           8       0.95      0.96      0.96     73821\n",
            "           9       0.95      0.97      0.96     76181\n",
            "\n",
            "    accuracy                           0.97    770000\n",
            "   macro avg       0.97      0.97      0.97    770000\n",
            "weighted avg       0.97      0.97      0.97    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.974 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     77724\n",
            "           1       0.99      0.98      0.98     89602\n",
            "           2       0.96      0.96      0.96     80558\n",
            "           3       0.97      0.96      0.96     79543\n",
            "           4       0.97      0.96      0.97     77010\n",
            "           5       0.96      0.96      0.96     68920\n",
            "           6       0.97      0.97      0.97     74876\n",
            "           7       0.96      0.96      0.96     79804\n",
            "           8       0.95      0.96      0.96     74793\n",
            "           9       0.95      0.97      0.96     77170\n",
            "\n",
            "    accuracy                           0.97    780000\n",
            "   macro avg       0.97      0.97      0.97    780000\n",
            "weighted avg       0.97      0.97      0.97    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.974 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     78718\n",
            "           1       0.99      0.98      0.98     90752\n",
            "           2       0.96      0.96      0.96     81595\n",
            "           3       0.97      0.96      0.96     80558\n",
            "           4       0.97      0.96      0.97     78001\n",
            "           5       0.96      0.96      0.96     69800\n",
            "           6       0.97      0.97      0.97     75834\n",
            "           7       0.96      0.96      0.96     80839\n",
            "           8       0.95      0.96      0.96     75745\n",
            "           9       0.95      0.97      0.96     78158\n",
            "\n",
            "    accuracy                           0.97    790000\n",
            "   macro avg       0.97      0.97      0.97    790000\n",
            "weighted avg       0.97      0.97      0.97    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     79710\n",
            "           1       0.99      0.98      0.98     91893\n",
            "           2       0.96      0.96      0.96     82637\n",
            "           3       0.97      0.96      0.96     81579\n",
            "           4       0.97      0.96      0.97     78993\n",
            "           5       0.96      0.96      0.96     70684\n",
            "           6       0.97      0.97      0.97     76789\n",
            "           7       0.96      0.96      0.96     81860\n",
            "           8       0.95      0.96      0.96     76713\n",
            "           9       0.95      0.97      0.96     79142\n",
            "\n",
            "    accuracy                           0.97    800000\n",
            "   macro avg       0.97      0.97      0.97    800000\n",
            "weighted avg       0.97      0.97      0.97    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.974 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     80698\n",
            "           1       0.99      0.98      0.98     93028\n",
            "           2       0.96      0.96      0.96     83666\n",
            "           3       0.97      0.96      0.96     82600\n",
            "           4       0.97      0.96      0.97     79982\n",
            "           5       0.96      0.97      0.96     71570\n",
            "           6       0.97      0.97      0.97     77739\n",
            "           7       0.96      0.96      0.96     82899\n",
            "           8       0.95      0.96      0.96     77687\n",
            "           9       0.95      0.97      0.96     80131\n",
            "\n",
            "    accuracy                           0.97    810000\n",
            "   macro avg       0.97      0.97      0.97    810000\n",
            "weighted avg       0.97      0.97      0.97    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.974 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     81696\n",
            "           1       0.99      0.98      0.98     94173\n",
            "           2       0.96      0.96      0.96     84706\n",
            "           3       0.97      0.96      0.96     83612\n",
            "           4       0.97      0.96      0.97     80966\n",
            "           5       0.96      0.97      0.96     72453\n",
            "           6       0.97      0.97      0.97     78691\n",
            "           7       0.96      0.96      0.96     83931\n",
            "           8       0.95      0.96      0.96     78648\n",
            "           9       0.95      0.97      0.96     81124\n",
            "\n",
            "    accuracy                           0.97    820000\n",
            "   macro avg       0.97      0.97      0.97    820000\n",
            "weighted avg       0.97      0.97      0.97    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.975 Loss: 0.086\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     82688\n",
            "           1       0.99      0.98      0.98     95314\n",
            "           2       0.96      0.96      0.96     85742\n",
            "           3       0.97      0.96      0.96     84633\n",
            "           4       0.97      0.96      0.97     81955\n",
            "           5       0.96      0.97      0.96     73327\n",
            "           6       0.97      0.97      0.97     79651\n",
            "           7       0.96      0.96      0.96     84964\n",
            "           8       0.95      0.96      0.96     79613\n",
            "           9       0.95      0.97      0.96     82113\n",
            "\n",
            "    accuracy                           0.97    830000\n",
            "   macro avg       0.97      0.97      0.97    830000\n",
            "weighted avg       0.97      0.97      0.97    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.975 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     83684\n",
            "           1       0.99      0.98      0.98     96453\n",
            "           2       0.96      0.96      0.96     86768\n",
            "           3       0.97      0.96      0.96     85652\n",
            "           4       0.97      0.96      0.97     82936\n",
            "           5       0.96      0.97      0.96     74218\n",
            "           6       0.97      0.97      0.97     80607\n",
            "           7       0.96      0.96      0.96     85993\n",
            "           8       0.95      0.96      0.96     80583\n",
            "           9       0.95      0.97      0.96     83106\n",
            "\n",
            "    accuracy                           0.97    840000\n",
            "   macro avg       0.97      0.97      0.97    840000\n",
            "weighted avg       0.97      0.97      0.97    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.974 Loss: 0.089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     84677\n",
            "           1       0.99      0.98      0.98     97601\n",
            "           2       0.96      0.96      0.96     87805\n",
            "           3       0.97      0.96      0.96     86666\n",
            "           4       0.97      0.96      0.97     83926\n",
            "           5       0.96      0.97      0.96     75104\n",
            "           6       0.97      0.97      0.97     81571\n",
            "           7       0.96      0.96      0.96     87028\n",
            "           8       0.95      0.96      0.96     81538\n",
            "           9       0.95      0.97      0.96     84084\n",
            "\n",
            "    accuracy                           0.97    850000\n",
            "   macro avg       0.97      0.97      0.97    850000\n",
            "weighted avg       0.97      0.97      0.97    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.974 Loss: 0.087\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     85669\n",
            "           1       0.99      0.98      0.98     98744\n",
            "           2       0.96      0.96      0.96     88833\n",
            "           3       0.97      0.96      0.96     87701\n",
            "           4       0.97      0.96      0.97     84917\n",
            "           5       0.96      0.97      0.96     75973\n",
            "           6       0.97      0.97      0.97     82528\n",
            "           7       0.96      0.96      0.96     88055\n",
            "           8       0.95      0.96      0.96     82507\n",
            "           9       0.95      0.97      0.96     85073\n",
            "\n",
            "    accuracy                           0.97    860000\n",
            "   macro avg       0.97      0.97      0.97    860000\n",
            "weighted avg       0.97      0.97      0.97    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.975 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     86664\n",
            "           1       0.99      0.98      0.98     99884\n",
            "           2       0.96      0.96      0.96     89863\n",
            "           3       0.97      0.96      0.96     88715\n",
            "           4       0.97      0.97      0.97     85892\n",
            "           5       0.96      0.97      0.96     76866\n",
            "           6       0.97      0.97      0.97     83480\n",
            "           7       0.96      0.96      0.96     89093\n",
            "           8       0.95      0.97      0.96     83473\n",
            "           9       0.95      0.97      0.96     86070\n",
            "\n",
            "    accuracy                           0.97    870000\n",
            "   macro avg       0.97      0.97      0.97    870000\n",
            "weighted avg       0.97      0.97      0.97    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.973 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     87663\n",
            "           1       0.99      0.98      0.98    101030\n",
            "           2       0.96      0.96      0.96     90897\n",
            "           3       0.97      0.96      0.96     89735\n",
            "           4       0.97      0.97      0.97     86878\n",
            "           5       0.96      0.97      0.96     77745\n",
            "           6       0.97      0.97      0.97     84437\n",
            "           7       0.96      0.96      0.96     90125\n",
            "           8       0.95      0.97      0.96     84431\n",
            "           9       0.95      0.97      0.96     87059\n",
            "\n",
            "    accuracy                           0.97    880000\n",
            "   macro avg       0.97      0.97      0.97    880000\n",
            "weighted avg       0.97      0.97      0.97    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     88654\n",
            "           1       0.99      0.98      0.98    102175\n",
            "           2       0.96      0.96      0.96     91930\n",
            "           3       0.97      0.96      0.96     90762\n",
            "           4       0.97      0.97      0.97     87859\n",
            "           5       0.96      0.97      0.96     78629\n",
            "           6       0.97      0.97      0.97     85402\n",
            "           7       0.96      0.96      0.96     91153\n",
            "           8       0.95      0.97      0.96     85384\n",
            "           9       0.95      0.97      0.96     88052\n",
            "\n",
            "    accuracy                           0.97    890000\n",
            "   macro avg       0.97      0.97      0.97    890000\n",
            "weighted avg       0.97      0.97      0.97    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     89647\n",
            "           1       0.99      0.98      0.98    103315\n",
            "           2       0.96      0.96      0.96     92953\n",
            "           3       0.97      0.96      0.96     91782\n",
            "           4       0.97      0.97      0.97     88839\n",
            "           5       0.96      0.97      0.96     79519\n",
            "           6       0.97      0.97      0.97     86358\n",
            "           7       0.96      0.96      0.96     92191\n",
            "           8       0.95      0.97      0.96     86352\n",
            "           9       0.95      0.97      0.96     89044\n",
            "\n",
            "    accuracy                           0.97    900000\n",
            "   macro avg       0.97      0.97      0.97    900000\n",
            "weighted avg       0.97      0.97      0.97    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.drop1 = nn.Dropout(p=0.7)\n",
        "    self.drop2 = nn.Dropout(p=0.5)\n",
        "    self.linear2 = nn.Linear(100, 100)\n",
        "    self.batch = nn.BatchNorm1d(100)\n",
        "    self.act = nn.Sigmoid()\n",
        "    self.linear3 = nn.Linear(100,10)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop1(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop2(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear3(out)\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "RQgaOZzmT7uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "xCpDZHP8UCUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxFRS_omUE9B",
        "outputId": "1a3827ea-afa7-4e66-de27-137f215dffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.835 Loss: 0.488\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.88      0.93      1099\n",
            "           1       0.87      0.99      0.93      1004\n",
            "           2       0.86      0.86      0.86      1037\n",
            "           3       0.92      0.77      0.84      1207\n",
            "           4       0.52      0.97      0.67       521\n",
            "           5       0.65      0.95      0.77       609\n",
            "           6       0.84      0.97      0.90       825\n",
            "           7       0.85      0.93      0.89       940\n",
            "           8       0.89      0.71      0.79      1220\n",
            "           9       0.94      0.61      0.74      1538\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.83      0.86      0.83     10000\n",
            "weighted avg       0.86      0.84      0.84     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.870 Loss: 0.403\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.94      2154\n",
            "           1       0.88      0.99      0.93      2017\n",
            "           2       0.87      0.87      0.87      2061\n",
            "           3       0.92      0.80      0.85      2341\n",
            "           4       0.60      0.97      0.75      1223\n",
            "           5       0.68      0.95      0.79      1269\n",
            "           6       0.87      0.97      0.92      1731\n",
            "           7       0.85      0.95      0.89      1844\n",
            "           8       0.91      0.71      0.80      2512\n",
            "           9       0.93      0.66      0.77      2848\n",
            "\n",
            "    accuracy                           0.85     20000\n",
            "   macro avg       0.85      0.88      0.85     20000\n",
            "weighted avg       0.87      0.85      0.85     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.845 Loss: 0.469\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.94      3235\n",
            "           1       0.86      0.99      0.92      2951\n",
            "           2       0.86      0.89      0.87      2972\n",
            "           3       0.92      0.80      0.86      3479\n",
            "           4       0.57      0.98      0.72      1732\n",
            "           5       0.69      0.95      0.80      1953\n",
            "           6       0.88      0.96      0.92      2617\n",
            "           7       0.86      0.95      0.90      2821\n",
            "           8       0.93      0.68      0.79      3965\n",
            "           9       0.93      0.66      0.77      4275\n",
            "\n",
            "    accuracy                           0.85     30000\n",
            "   macro avg       0.85      0.88      0.85     30000\n",
            "weighted avg       0.87      0.85      0.85     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.875 Loss: 0.381\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      4271\n",
            "           1       0.87      0.99      0.92      3971\n",
            "           2       0.86      0.90      0.88      3933\n",
            "           3       0.93      0.80      0.86      4684\n",
            "           4       0.60      0.97      0.75      2439\n",
            "           5       0.70      0.95      0.81      2646\n",
            "           6       0.89      0.96      0.92      3522\n",
            "           7       0.86      0.95      0.90      3712\n",
            "           8       0.92      0.70      0.80      5156\n",
            "           9       0.93      0.66      0.78      5666\n",
            "\n",
            "    accuracy                           0.86     40000\n",
            "   macro avg       0.85      0.88      0.86     40000\n",
            "weighted avg       0.88      0.86      0.86     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.883 Loss: 0.364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95      5292\n",
            "           1       0.87      0.99      0.93      4970\n",
            "           2       0.87      0.91      0.89      4941\n",
            "           3       0.93      0.80      0.86      5826\n",
            "           4       0.63      0.97      0.76      3164\n",
            "           5       0.71      0.95      0.81      3305\n",
            "           6       0.90      0.96      0.93      4451\n",
            "           7       0.86      0.95      0.91      4667\n",
            "           8       0.93      0.70      0.80      6437\n",
            "           9       0.93      0.68      0.79      6947\n",
            "\n",
            "    accuracy                           0.86     50000\n",
            "   macro avg       0.86      0.88      0.86     50000\n",
            "weighted avg       0.88      0.86      0.86     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.847 Loss: 0.475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95      6337\n",
            "           1       0.85      0.99      0.91      5824\n",
            "           2       0.86      0.91      0.89      5871\n",
            "           3       0.93      0.80      0.86      7046\n",
            "           4       0.62      0.97      0.76      3781\n",
            "           5       0.71      0.95      0.81      3973\n",
            "           6       0.90      0.96      0.93      5338\n",
            "           7       0.86      0.96      0.91      5544\n",
            "           8       0.93      0.68      0.79      7999\n",
            "           9       0.93      0.68      0.79      8287\n",
            "\n",
            "    accuracy                           0.86     60000\n",
            "   macro avg       0.86      0.88      0.86     60000\n",
            "weighted avg       0.88      0.86      0.86     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.876 Loss: 0.376\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95      7376\n",
            "           1       0.86      0.99      0.92      6879\n",
            "           2       0.87      0.92      0.89      6830\n",
            "           3       0.93      0.80      0.86      8241\n",
            "           4       0.63      0.97      0.77      4467\n",
            "           5       0.71      0.95      0.81      4648\n",
            "           6       0.90      0.97      0.93      6231\n",
            "           7       0.85      0.96      0.90      6394\n",
            "           8       0.93      0.69      0.79      9229\n",
            "           9       0.94      0.68      0.79      9705\n",
            "\n",
            "    accuracy                           0.86     70000\n",
            "   macro avg       0.86      0.88      0.86     70000\n",
            "weighted avg       0.88      0.86      0.86     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.875 Loss: 0.377\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95      8417\n",
            "           1       0.86      0.99      0.92      7886\n",
            "           2       0.87      0.92      0.89      7822\n",
            "           3       0.93      0.80      0.86      9411\n",
            "           4       0.63      0.97      0.77      5092\n",
            "           5       0.71      0.95      0.82      5346\n",
            "           6       0.90      0.97      0.93      7124\n",
            "           7       0.86      0.96      0.90      7342\n",
            "           8       0.93      0.70      0.80     10441\n",
            "           9       0.94      0.68      0.79     11119\n",
            "\n",
            "    accuracy                           0.86     80000\n",
            "   macro avg       0.86      0.89      0.86     80000\n",
            "weighted avg       0.88      0.86      0.86     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.866 Loss: 0.410\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95      9477\n",
            "           1       0.86      0.99      0.92      8856\n",
            "           2       0.87      0.92      0.89      8787\n",
            "           3       0.94      0.80      0.86     10620\n",
            "           4       0.63      0.97      0.76      5722\n",
            "           5       0.71      0.95      0.82      6022\n",
            "           6       0.90      0.97      0.93      8022\n",
            "           7       0.86      0.96      0.91      8233\n",
            "           8       0.93      0.70      0.80     11739\n",
            "           9       0.94      0.68      0.79     12522\n",
            "\n",
            "    accuracy                           0.86     90000\n",
            "   macro avg       0.86      0.89      0.86     90000\n",
            "weighted avg       0.88      0.86      0.86     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.874 Loss: 0.386\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     10519\n",
            "           1       0.86      0.99      0.92      9883\n",
            "           2       0.87      0.92      0.90      9783\n",
            "           3       0.94      0.79      0.86     11933\n",
            "           4       0.64      0.97      0.77      6472\n",
            "           5       0.71      0.95      0.81      6646\n",
            "           6       0.90      0.97      0.93      8904\n",
            "           7       0.85      0.96      0.90      9072\n",
            "           8       0.93      0.70      0.80     12916\n",
            "           9       0.94      0.68      0.79     13872\n",
            "\n",
            "    accuracy                           0.86    100000\n",
            "   macro avg       0.86      0.89      0.86    100000\n",
            "weighted avg       0.88      0.86      0.86    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.869 Loss: 0.400\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     11553\n",
            "           1       0.86      0.99      0.92     10834\n",
            "           2       0.87      0.92      0.90     10798\n",
            "           3       0.94      0.79      0.86     13162\n",
            "           4       0.65      0.97      0.78      7193\n",
            "           5       0.71      0.95      0.81      7284\n",
            "           6       0.90      0.97      0.93      9776\n",
            "           7       0.85      0.96      0.90      9938\n",
            "           8       0.93      0.70      0.80     14272\n",
            "           9       0.94      0.69      0.79     15190\n",
            "\n",
            "    accuracy                           0.87    110000\n",
            "   macro avg       0.86      0.89      0.86    110000\n",
            "weighted avg       0.88      0.87      0.86    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.823 Loss: 0.520\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     12600\n",
            "           1       0.84      0.99      0.91     11484\n",
            "           2       0.87      0.92      0.90     11768\n",
            "           3       0.94      0.79      0.86     14429\n",
            "           4       0.64      0.97      0.77      7755\n",
            "           5       0.71      0.95      0.81      7979\n",
            "           6       0.89      0.97      0.93     10615\n",
            "           7       0.85      0.97      0.90     10816\n",
            "           8       0.94      0.69      0.79     15922\n",
            "           9       0.94      0.68      0.79     16632\n",
            "\n",
            "    accuracy                           0.86    120000\n",
            "   macro avg       0.86      0.89      0.86    120000\n",
            "weighted avg       0.88      0.86      0.86    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.841 Loss: 0.458\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     13651\n",
            "           1       0.83      0.99      0.90     12286\n",
            "           2       0.87      0.92      0.90     12738\n",
            "           3       0.94      0.79      0.86     15735\n",
            "           4       0.64      0.97      0.77      8397\n",
            "           5       0.71      0.95      0.81      8661\n",
            "           6       0.89      0.97      0.93     11480\n",
            "           7       0.84      0.97      0.90     11624\n",
            "           8       0.94      0.68      0.79     17293\n",
            "           9       0.94      0.68      0.79     18135\n",
            "\n",
            "    accuracy                           0.86    130000\n",
            "   macro avg       0.86      0.88      0.86    130000\n",
            "weighted avg       0.88      0.86      0.86    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.876 Loss: 0.380\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     14677\n",
            "           1       0.83      0.99      0.90     13209\n",
            "           2       0.88      0.92      0.90     13728\n",
            "           3       0.94      0.79      0.86     16942\n",
            "           4       0.65      0.97      0.78      9145\n",
            "           5       0.71      0.95      0.81      9333\n",
            "           6       0.89      0.97      0.93     12352\n",
            "           7       0.84      0.97      0.90     12542\n",
            "           8       0.94      0.69      0.79     18615\n",
            "           9       0.94      0.68      0.79     19457\n",
            "\n",
            "    accuracy                           0.86    140000\n",
            "   macro avg       0.86      0.89      0.86    140000\n",
            "weighted avg       0.88      0.86      0.86    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.845 Loss: 0.462\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95     15719\n",
            "           1       0.81      0.99      0.89     13931\n",
            "           2       0.88      0.92      0.90     14710\n",
            "           3       0.94      0.78      0.86     18194\n",
            "           4       0.65      0.97      0.78      9801\n",
            "           5       0.71      0.95      0.82     10001\n",
            "           6       0.89      0.97      0.93     13222\n",
            "           7       0.84      0.97      0.90     13448\n",
            "           8       0.94      0.68      0.79     20145\n",
            "           9       0.94      0.69      0.79     20829\n",
            "\n",
            "    accuracy                           0.86    150000\n",
            "   macro avg       0.86      0.88      0.86    150000\n",
            "weighted avg       0.88      0.86      0.86    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.884 Loss: 0.365\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     16777\n",
            "           1       0.81      0.99      0.90     14885\n",
            "           2       0.88      0.92      0.90     15702\n",
            "           3       0.94      0.78      0.86     19449\n",
            "           4       0.65      0.97      0.78     10595\n",
            "           5       0.71      0.95      0.82     10690\n",
            "           6       0.89      0.97      0.93     14110\n",
            "           7       0.85      0.97      0.90     14346\n",
            "           8       0.94      0.68      0.79     21371\n",
            "           9       0.94      0.69      0.80     22075\n",
            "\n",
            "    accuracy                           0.86    160000\n",
            "   macro avg       0.86      0.89      0.86    160000\n",
            "weighted avg       0.88      0.86      0.86    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.882 Loss: 0.374\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     17819\n",
            "           1       0.82      0.99      0.90     15835\n",
            "           2       0.88      0.92      0.90     16741\n",
            "           3       0.94      0.78      0.86     20696\n",
            "           4       0.66      0.97      0.78     11340\n",
            "           5       0.71      0.95      0.82     11334\n",
            "           6       0.89      0.97      0.93     14998\n",
            "           7       0.85      0.97      0.90     15302\n",
            "           8       0.94      0.69      0.79     22634\n",
            "           9       0.94      0.69      0.80     23301\n",
            "\n",
            "    accuracy                           0.86    170000\n",
            "   macro avg       0.86      0.89      0.86    170000\n",
            "weighted avg       0.88      0.86      0.86    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.857 Loss: 0.434\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     18880\n",
            "           1       0.81      0.99      0.89     16728\n",
            "           2       0.88      0.92      0.90     17743\n",
            "           3       0.94      0.78      0.85     22028\n",
            "           4       0.66      0.97      0.78     11953\n",
            "           5       0.71      0.95      0.82     11980\n",
            "           6       0.89      0.97      0.93     15871\n",
            "           7       0.85      0.97      0.91     16241\n",
            "           8       0.94      0.69      0.79     23915\n",
            "           9       0.94      0.69      0.80     24661\n",
            "\n",
            "    accuracy                           0.86    180000\n",
            "   macro avg       0.86      0.89      0.86    180000\n",
            "weighted avg       0.88      0.86      0.86    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.873 Loss: 0.392\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     19950\n",
            "           1       0.81      0.99      0.89     17660\n",
            "           2       0.88      0.92      0.90     18761\n",
            "           3       0.95      0.78      0.85     23306\n",
            "           4       0.66      0.97      0.78     12659\n",
            "           5       0.71      0.95      0.82     12643\n",
            "           6       0.89      0.97      0.93     16725\n",
            "           7       0.85      0.97      0.91     17163\n",
            "           8       0.94      0.69      0.79     25177\n",
            "           9       0.94      0.70      0.80     25956\n",
            "\n",
            "    accuracy                           0.86    190000\n",
            "   macro avg       0.86      0.89      0.86    190000\n",
            "weighted avg       0.88      0.86      0.86    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.877 Loss: 0.381\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     20982\n",
            "           1       0.82      0.99      0.90     18639\n",
            "           2       0.89      0.92      0.90     19841\n",
            "           3       0.95      0.78      0.85     24552\n",
            "           4       0.66      0.97      0.78     13320\n",
            "           5       0.71      0.96      0.82     13276\n",
            "           6       0.89      0.97      0.93     17615\n",
            "           7       0.85      0.97      0.91     18096\n",
            "           8       0.94      0.69      0.80     26344\n",
            "           9       0.94      0.70      0.80     27335\n",
            "\n",
            "    accuracy                           0.86    200000\n",
            "   macro avg       0.86      0.89      0.86    200000\n",
            "weighted avg       0.88      0.86      0.86    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.864 Loss: 0.416\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     22034\n",
            "           1       0.82      0.99      0.90     19555\n",
            "           2       0.89      0.92      0.90     20820\n",
            "           3       0.95      0.78      0.85     25795\n",
            "           4       0.66      0.97      0.78     13959\n",
            "           5       0.71      0.96      0.82     13939\n",
            "           6       0.89      0.97      0.93     18476\n",
            "           7       0.85      0.97      0.91     19026\n",
            "           8       0.94      0.69      0.80     27693\n",
            "           9       0.94      0.70      0.80     28703\n",
            "\n",
            "    accuracy                           0.86    210000\n",
            "   macro avg       0.86      0.89      0.86    210000\n",
            "weighted avg       0.88      0.86      0.86    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.880 Loss: 0.371\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     23085\n",
            "           1       0.82      0.99      0.90     20575\n",
            "           2       0.89      0.92      0.90     21810\n",
            "           3       0.95      0.78      0.85     27093\n",
            "           4       0.66      0.97      0.79     14689\n",
            "           5       0.71      0.96      0.82     14586\n",
            "           6       0.89      0.97      0.93     19354\n",
            "           7       0.85      0.97      0.91     19912\n",
            "           8       0.94      0.69      0.80     28882\n",
            "           9       0.94      0.70      0.80     30014\n",
            "\n",
            "    accuracy                           0.86    220000\n",
            "   macro avg       0.86      0.89      0.86    220000\n",
            "weighted avg       0.88      0.86      0.86    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.885 Loss: 0.358\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     24128\n",
            "           1       0.82      0.99      0.90     21574\n",
            "           2       0.89      0.92      0.90     22840\n",
            "           3       0.95      0.78      0.85     28349\n",
            "           4       0.66      0.97      0.79     15472\n",
            "           5       0.71      0.96      0.81     15211\n",
            "           6       0.89      0.97      0.93     20226\n",
            "           7       0.85      0.97      0.91     20819\n",
            "           8       0.94      0.70      0.80     30107\n",
            "           9       0.94      0.70      0.80     31274\n",
            "\n",
            "    accuracy                           0.87    230000\n",
            "   macro avg       0.86      0.89      0.87    230000\n",
            "weighted avg       0.88      0.87      0.86    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.840 Loss: 0.478\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     25179\n",
            "           1       0.81      0.99      0.89     22311\n",
            "           2       0.89      0.92      0.90     23808\n",
            "           3       0.95      0.77      0.85     29657\n",
            "           4       0.66      0.97      0.79     16099\n",
            "           5       0.71      0.96      0.81     15851\n",
            "           6       0.89      0.97      0.93     21075\n",
            "           7       0.85      0.97      0.91     21731\n",
            "           8       0.94      0.69      0.80     31613\n",
            "           9       0.94      0.70      0.80     32676\n",
            "\n",
            "    accuracy                           0.86    240000\n",
            "   macro avg       0.86      0.89      0.86    240000\n",
            "weighted avg       0.88      0.86      0.86    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.881 Loss: 0.360\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     26231\n",
            "           1       0.82      0.99      0.90     23326\n",
            "           2       0.89      0.92      0.91     24817\n",
            "           3       0.95      0.77      0.85     30912\n",
            "           4       0.66      0.97      0.79     16799\n",
            "           5       0.71      0.96      0.82     16522\n",
            "           6       0.89      0.97      0.93     21973\n",
            "           7       0.85      0.97      0.91     22593\n",
            "           8       0.94      0.70      0.80     32771\n",
            "           9       0.94      0.70      0.80     34056\n",
            "\n",
            "    accuracy                           0.86    250000\n",
            "   macro avg       0.86      0.89      0.87    250000\n",
            "weighted avg       0.88      0.86      0.86    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.871 Loss: 0.384\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     27271\n",
            "           1       0.82      0.99      0.90     24247\n",
            "           2       0.89      0.92      0.91     25844\n",
            "           3       0.95      0.77      0.85     32205\n",
            "           4       0.67      0.97      0.79     17512\n",
            "           5       0.71      0.96      0.82     17183\n",
            "           6       0.89      0.97      0.93     22867\n",
            "           7       0.85      0.97      0.91     23457\n",
            "           8       0.94      0.70      0.80     34022\n",
            "           9       0.94      0.70      0.80     35392\n",
            "\n",
            "    accuracy                           0.87    260000\n",
            "   macro avg       0.86      0.89      0.87    260000\n",
            "weighted avg       0.88      0.87      0.86    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.869 Loss: 0.404\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     28335\n",
            "           1       0.82      0.99      0.90     25212\n",
            "           2       0.89      0.92      0.91     26814\n",
            "           3       0.95      0.77      0.85     33478\n",
            "           4       0.66      0.97      0.79     18113\n",
            "           5       0.71      0.96      0.82     17877\n",
            "           6       0.89      0.97      0.93     23744\n",
            "           7       0.85      0.97      0.91     24378\n",
            "           8       0.94      0.70      0.80     35235\n",
            "           9       0.95      0.70      0.80     36814\n",
            "\n",
            "    accuracy                           0.87    270000\n",
            "   macro avg       0.86      0.89      0.87    270000\n",
            "weighted avg       0.88      0.87      0.86    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.882 Loss: 0.361\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     29388\n",
            "           1       0.82      0.99      0.90     26218\n",
            "           2       0.89      0.92      0.91     27831\n",
            "           3       0.95      0.77      0.85     34746\n",
            "           4       0.66      0.97      0.79     18785\n",
            "           5       0.71      0.96      0.82     18546\n",
            "           6       0.89      0.97      0.93     24636\n",
            "           7       0.85      0.97      0.91     25290\n",
            "           8       0.94      0.70      0.80     36366\n",
            "           9       0.95      0.70      0.80     38194\n",
            "\n",
            "    accuracy                           0.87    280000\n",
            "   macro avg       0.86      0.89      0.87    280000\n",
            "weighted avg       0.88      0.87      0.86    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.886 Loss: 0.345\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     30411\n",
            "           1       0.82      0.99      0.90     27181\n",
            "           2       0.89      0.92      0.91     28832\n",
            "           3       0.95      0.77      0.85     35978\n",
            "           4       0.67      0.97      0.79     19530\n",
            "           5       0.71      0.96      0.82     19224\n",
            "           6       0.89      0.97      0.93     25536\n",
            "           7       0.85      0.97      0.91     26211\n",
            "           8       0.94      0.70      0.80     37618\n",
            "           9       0.95      0.70      0.81     39479\n",
            "\n",
            "    accuracy                           0.87    290000\n",
            "   macro avg       0.87      0.89      0.87    290000\n",
            "weighted avg       0.88      0.87      0.87    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.853 Loss: 0.435\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     31451\n",
            "           1       0.82      0.99      0.90     27932\n",
            "           2       0.89      0.93      0.91     29808\n",
            "           3       0.95      0.77      0.85     37313\n",
            "           4       0.66      0.97      0.79     20198\n",
            "           5       0.71      0.96      0.82     19934\n",
            "           6       0.89      0.97      0.93     26385\n",
            "           7       0.86      0.97      0.91     27158\n",
            "           8       0.94      0.70      0.80     38986\n",
            "           9       0.95      0.70      0.81     40835\n",
            "\n",
            "    accuracy                           0.87    300000\n",
            "   macro avg       0.87      0.89      0.87    300000\n",
            "weighted avg       0.88      0.87      0.87    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.886 Loss: 0.351\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     32498\n",
            "           1       0.82      0.99      0.90     28936\n",
            "           2       0.89      0.93      0.91     30827\n",
            "           3       0.95      0.77      0.85     38591\n",
            "           4       0.67      0.97      0.79     20934\n",
            "           5       0.71      0.96      0.82     20580\n",
            "           6       0.89      0.97      0.93     27280\n",
            "           7       0.86      0.97      0.91     28073\n",
            "           8       0.94      0.70      0.80     40126\n",
            "           9       0.95      0.70      0.81     42155\n",
            "\n",
            "    accuracy                           0.87    310000\n",
            "   macro avg       0.87      0.89      0.87    310000\n",
            "weighted avg       0.89      0.87      0.87    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.882 Loss: 0.360\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     33547\n",
            "           1       0.82      0.99      0.90     29883\n",
            "           2       0.89      0.93      0.91     31849\n",
            "           3       0.95      0.77      0.85     39866\n",
            "           4       0.67      0.97      0.79     21670\n",
            "           5       0.71      0.96      0.82     21236\n",
            "           6       0.89      0.97      0.93     28170\n",
            "           7       0.86      0.97      0.91     29005\n",
            "           8       0.94      0.71      0.80     41339\n",
            "           9       0.95      0.70      0.81     43435\n",
            "\n",
            "    accuracy                           0.87    320000\n",
            "   macro avg       0.87      0.89      0.87    320000\n",
            "weighted avg       0.89      0.87      0.87    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.852 Loss: 0.445\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     34593\n",
            "           1       0.81      0.99      0.90     30670\n",
            "           2       0.89      0.93      0.91     32841\n",
            "           3       0.95      0.77      0.85     41194\n",
            "           4       0.67      0.97      0.79     22284\n",
            "           5       0.71      0.96      0.82     21908\n",
            "           6       0.89      0.97      0.93     29050\n",
            "           7       0.86      0.97      0.91     29935\n",
            "           8       0.94      0.70      0.80     42705\n",
            "           9       0.95      0.70      0.81     44820\n",
            "\n",
            "    accuracy                           0.87    330000\n",
            "   macro avg       0.87      0.89      0.87    330000\n",
            "weighted avg       0.89      0.87      0.87    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.897 Loss: 0.313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     35634\n",
            "           1       0.82      0.99      0.90     31679\n",
            "           2       0.89      0.93      0.91     33837\n",
            "           3       0.95      0.77      0.85     42370\n",
            "           4       0.67      0.97      0.79     23041\n",
            "           5       0.71      0.96      0.82     22613\n",
            "           6       0.89      0.97      0.93     29948\n",
            "           7       0.86      0.97      0.91     30883\n",
            "           8       0.94      0.71      0.81     43908\n",
            "           9       0.95      0.70      0.81     46087\n",
            "\n",
            "    accuracy                           0.87    340000\n",
            "   macro avg       0.87      0.89      0.87    340000\n",
            "weighted avg       0.89      0.87      0.87    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.872 Loss: 0.383\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     36674\n",
            "           1       0.81      0.99      0.90     32544\n",
            "           2       0.89      0.93      0.91     34858\n",
            "           3       0.95      0.77      0.85     43641\n",
            "           4       0.67      0.97      0.79     23761\n",
            "           5       0.72      0.96      0.82     23289\n",
            "           6       0.89      0.97      0.93     30814\n",
            "           7       0.86      0.97      0.91     31829\n",
            "           8       0.94      0.71      0.81     45215\n",
            "           9       0.95      0.71      0.81     47375\n",
            "\n",
            "    accuracy                           0.87    350000\n",
            "   macro avg       0.87      0.89      0.87    350000\n",
            "weighted avg       0.89      0.87      0.87    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.858 Loss: 0.427\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     37720\n",
            "           1       0.81      0.99      0.89     33384\n",
            "           2       0.89      0.93      0.91     35838\n",
            "           3       0.95      0.77      0.85     44924\n",
            "           4       0.67      0.97      0.79     24378\n",
            "           5       0.72      0.96      0.82     23988\n",
            "           6       0.89      0.97      0.93     31662\n",
            "           7       0.86      0.97      0.91     32755\n",
            "           8       0.94      0.71      0.81     46563\n",
            "           9       0.95      0.71      0.81     48788\n",
            "\n",
            "    accuracy                           0.87    360000\n",
            "   macro avg       0.87      0.89      0.87    360000\n",
            "weighted avg       0.89      0.87      0.87    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.891 Loss: 0.329\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     38764\n",
            "           1       0.82      0.99      0.90     34414\n",
            "           2       0.89      0.93      0.91     36812\n",
            "           3       0.95      0.77      0.85     46146\n",
            "           4       0.67      0.97      0.79     25132\n",
            "           5       0.72      0.96      0.82     24657\n",
            "           6       0.89      0.97      0.93     32552\n",
            "           7       0.86      0.97      0.91     33667\n",
            "           8       0.94      0.71      0.81     47735\n",
            "           9       0.95      0.71      0.81     50121\n",
            "\n",
            "    accuracy                           0.87    370000\n",
            "   macro avg       0.87      0.89      0.87    370000\n",
            "weighted avg       0.89      0.87      0.87    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.884 Loss: 0.352\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     39801\n",
            "           1       0.82      0.99      0.90     35384\n",
            "           2       0.90      0.93      0.91     37851\n",
            "           3       0.95      0.77      0.85     47362\n",
            "           4       0.67      0.97      0.79     25806\n",
            "           5       0.72      0.96      0.82     25340\n",
            "           6       0.89      0.97      0.93     33458\n",
            "           7       0.86      0.97      0.91     34600\n",
            "           8       0.94      0.71      0.81     48918\n",
            "           9       0.95      0.71      0.81     51480\n",
            "\n",
            "    accuracy                           0.87    380000\n",
            "   macro avg       0.87      0.89      0.87    380000\n",
            "weighted avg       0.89      0.87      0.87    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.869 Loss: 0.395\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     40844\n",
            "           1       0.81      0.99      0.90     36257\n",
            "           2       0.90      0.93      0.91     38841\n",
            "           3       0.95      0.77      0.85     48579\n",
            "           4       0.67      0.97      0.79     26459\n",
            "           5       0.72      0.96      0.82     26046\n",
            "           6       0.89      0.97      0.93     34328\n",
            "           7       0.86      0.97      0.91     35557\n",
            "           8       0.94      0.71      0.81     50276\n",
            "           9       0.95      0.71      0.81     52813\n",
            "\n",
            "    accuracy                           0.87    390000\n",
            "   macro avg       0.87      0.89      0.87    390000\n",
            "weighted avg       0.89      0.87      0.87    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.899 Loss: 0.316\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     41886\n",
            "           1       0.82      0.99      0.90     37241\n",
            "           2       0.90      0.93      0.91     39843\n",
            "           3       0.95      0.77      0.85     49790\n",
            "           4       0.67      0.97      0.79     27254\n",
            "           5       0.72      0.96      0.82     26758\n",
            "           6       0.90      0.97      0.93     35245\n",
            "           7       0.86      0.97      0.91     36476\n",
            "           8       0.94      0.71      0.81     51459\n",
            "           9       0.95      0.71      0.81     54048\n",
            "\n",
            "    accuracy                           0.87    400000\n",
            "   macro avg       0.87      0.89      0.87    400000\n",
            "weighted avg       0.89      0.87      0.87    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.905 Loss: 0.299\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     42918\n",
            "           1       0.82      0.99      0.90     38230\n",
            "           2       0.90      0.93      0.91     40858\n",
            "           3       0.95      0.77      0.85     50948\n",
            "           4       0.68      0.97      0.80     28073\n",
            "           5       0.72      0.96      0.82     27471\n",
            "           6       0.90      0.97      0.93     36155\n",
            "           7       0.86      0.97      0.91     37433\n",
            "           8       0.94      0.71      0.81     52673\n",
            "           9       0.95      0.71      0.81     55241\n",
            "\n",
            "    accuracy                           0.87    410000\n",
            "   macro avg       0.87      0.89      0.87    410000\n",
            "weighted avg       0.89      0.87      0.87    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.870 Loss: 0.398\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     43966\n",
            "           1       0.82      0.99      0.90     39138\n",
            "           2       0.90      0.93      0.91     41860\n",
            "           3       0.95      0.77      0.85     52189\n",
            "           4       0.68      0.97      0.80     28747\n",
            "           5       0.72      0.96      0.82     28125\n",
            "           6       0.90      0.97      0.93     37040\n",
            "           7       0.86      0.97      0.91     38327\n",
            "           8       0.94      0.71      0.81     53981\n",
            "           9       0.95      0.71      0.81     56627\n",
            "\n",
            "    accuracy                           0.87    420000\n",
            "   macro avg       0.87      0.89      0.87    420000\n",
            "weighted avg       0.89      0.87      0.87    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.896 Loss: 0.323\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     45021\n",
            "           1       0.82      0.99      0.90     40158\n",
            "           2       0.90      0.93      0.91     42841\n",
            "           3       0.95      0.77      0.85     53365\n",
            "           4       0.68      0.97      0.80     29554\n",
            "           5       0.72      0.96      0.82     28829\n",
            "           6       0.90      0.97      0.93     37932\n",
            "           7       0.86      0.97      0.91     39205\n",
            "           8       0.94      0.71      0.81     55167\n",
            "           9       0.95      0.71      0.81     57928\n",
            "\n",
            "    accuracy                           0.87    430000\n",
            "   macro avg       0.87      0.89      0.87    430000\n",
            "weighted avg       0.89      0.87      0.87    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.883 Loss: 0.348\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     46067\n",
            "           1       0.82      0.99      0.90     40992\n",
            "           2       0.90      0.93      0.91     43833\n",
            "           3       0.95      0.78      0.85     54510\n",
            "           4       0.68      0.97      0.80     30317\n",
            "           5       0.72      0.96      0.83     29577\n",
            "           6       0.90      0.97      0.93     38838\n",
            "           7       0.86      0.97      0.91     40130\n",
            "           8       0.94      0.71      0.81     56555\n",
            "           9       0.95      0.71      0.81     59181\n",
            "\n",
            "    accuracy                           0.87    440000\n",
            "   macro avg       0.87      0.89      0.87    440000\n",
            "weighted avg       0.89      0.87      0.87    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.865 Loss: 0.396\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     47111\n",
            "           1       0.81      0.99      0.89     41721\n",
            "           2       0.90      0.93      0.91     44789\n",
            "           3       0.95      0.78      0.85     55705\n",
            "           4       0.68      0.97      0.80     31057\n",
            "           5       0.73      0.96      0.83     30335\n",
            "           6       0.90      0.97      0.93     39699\n",
            "           7       0.86      0.97      0.91     41075\n",
            "           8       0.94      0.71      0.81     58043\n",
            "           9       0.95      0.71      0.81     60465\n",
            "\n",
            "    accuracy                           0.87    450000\n",
            "   macro avg       0.87      0.89      0.87    450000\n",
            "weighted avg       0.89      0.87      0.87    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.892 Loss: 0.327\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     48152\n",
            "           1       0.81      1.00      0.90     42717\n",
            "           2       0.90      0.93      0.91     45779\n",
            "           3       0.95      0.78      0.86     56894\n",
            "           4       0.68      0.97      0.80     31789\n",
            "           5       0.73      0.96      0.83     31084\n",
            "           6       0.90      0.97      0.93     40591\n",
            "           7       0.86      0.97      0.91     41979\n",
            "           8       0.94      0.71      0.81     59266\n",
            "           9       0.95      0.71      0.81     61749\n",
            "\n",
            "    accuracy                           0.87    460000\n",
            "   macro avg       0.87      0.89      0.87    460000\n",
            "weighted avg       0.89      0.87      0.87    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.886 Loss: 0.347\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     49199\n",
            "           1       0.81      1.00      0.89     43556\n",
            "           2       0.90      0.93      0.91     46774\n",
            "           3       0.95      0.78      0.86     58059\n",
            "           4       0.68      0.97      0.80     32597\n",
            "           5       0.73      0.96      0.83     31848\n",
            "           6       0.90      0.97      0.93     41494\n",
            "           7       0.86      0.97      0.91     42862\n",
            "           8       0.94      0.71      0.81     60619\n",
            "           9       0.95      0.71      0.81     62992\n",
            "\n",
            "    accuracy                           0.87    470000\n",
            "   macro avg       0.87      0.89      0.87    470000\n",
            "weighted avg       0.89      0.87      0.87    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.863 Loss: 0.413\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     50249\n",
            "           1       0.81      1.00      0.89     44275\n",
            "           2       0.90      0.93      0.91     47736\n",
            "           3       0.95      0.78      0.86     59276\n",
            "           4       0.68      0.97      0.80     33285\n",
            "           5       0.73      0.96      0.83     32610\n",
            "           6       0.90      0.97      0.93     42397\n",
            "           7       0.86      0.97      0.91     43773\n",
            "           8       0.94      0.71      0.81     62075\n",
            "           9       0.95      0.71      0.81     64324\n",
            "\n",
            "    accuracy                           0.87    480000\n",
            "   macro avg       0.87      0.89      0.87    480000\n",
            "weighted avg       0.89      0.87      0.87    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.881 Loss: 0.359\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     51304\n",
            "           1       0.81      1.00      0.89     45150\n",
            "           2       0.90      0.93      0.92     48727\n",
            "           3       0.95      0.78      0.86     60517\n",
            "           4       0.69      0.97      0.80     34065\n",
            "           5       0.73      0.96      0.83     33310\n",
            "           6       0.90      0.97      0.93     43295\n",
            "           7       0.86      0.97      0.91     44657\n",
            "           8       0.94      0.71      0.81     63372\n",
            "           9       0.95      0.71      0.82     65603\n",
            "\n",
            "    accuracy                           0.87    490000\n",
            "   macro avg       0.87      0.89      0.87    490000\n",
            "weighted avg       0.89      0.87      0.87    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.895 Loss: 0.324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     52344\n",
            "           1       0.81      1.00      0.89     46062\n",
            "           2       0.90      0.93      0.92     49750\n",
            "           3       0.95      0.78      0.86     61689\n",
            "           4       0.69      0.97      0.80     34846\n",
            "           5       0.73      0.96      0.83     34048\n",
            "           6       0.90      0.97      0.93     44201\n",
            "           7       0.86      0.97      0.91     45593\n",
            "           8       0.94      0.71      0.81     64613\n",
            "           9       0.95      0.72      0.82     66854\n",
            "\n",
            "    accuracy                           0.87    500000\n",
            "   macro avg       0.87      0.89      0.87    500000\n",
            "weighted avg       0.89      0.87      0.87    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.880 Loss: 0.364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     53387\n",
            "           1       0.81      1.00      0.89     46917\n",
            "           2       0.90      0.93      0.92     50735\n",
            "           3       0.95      0.78      0.86     62845\n",
            "           4       0.69      0.97      0.80     35560\n",
            "           5       0.73      0.96      0.83     34832\n",
            "           6       0.90      0.97      0.93     45072\n",
            "           7       0.86      0.97      0.91     46528\n",
            "           8       0.94      0.71      0.81     65953\n",
            "           9       0.95      0.72      0.82     68171\n",
            "\n",
            "    accuracy                           0.87    510000\n",
            "   macro avg       0.87      0.89      0.87    510000\n",
            "weighted avg       0.89      0.87      0.87    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.894 Loss: 0.325\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     54455\n",
            "           1       0.81      1.00      0.89     47882\n",
            "           2       0.90      0.93      0.92     51744\n",
            "           3       0.95      0.78      0.86     63990\n",
            "           4       0.69      0.97      0.81     36298\n",
            "           5       0.74      0.96      0.83     35571\n",
            "           6       0.90      0.97      0.93     45963\n",
            "           7       0.86      0.97      0.91     47440\n",
            "           8       0.94      0.71      0.81     67171\n",
            "           9       0.95      0.72      0.82     69486\n",
            "\n",
            "    accuracy                           0.87    520000\n",
            "   macro avg       0.87      0.89      0.87    520000\n",
            "weighted avg       0.89      0.87      0.87    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.897 Loss: 0.317\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     55504\n",
            "           1       0.81      1.00      0.89     48808\n",
            "           2       0.90      0.93      0.92     52745\n",
            "           3       0.95      0.78      0.86     65084\n",
            "           4       0.69      0.97      0.81     37097\n",
            "           5       0.74      0.96      0.83     36349\n",
            "           6       0.90      0.97      0.93     46864\n",
            "           7       0.86      0.97      0.91     48352\n",
            "           8       0.94      0.71      0.81     68454\n",
            "           9       0.95      0.72      0.82     70743\n",
            "\n",
            "    accuracy                           0.87    530000\n",
            "   macro avg       0.87      0.89      0.87    530000\n",
            "weighted avg       0.89      0.87      0.87    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.878 Loss: 0.372\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     56559\n",
            "           1       0.81      1.00      0.89     49635\n",
            "           2       0.90      0.93      0.92     53712\n",
            "           3       0.95      0.78      0.86     66297\n",
            "           4       0.69      0.97      0.81     37810\n",
            "           5       0.74      0.96      0.83     37131\n",
            "           6       0.90      0.97      0.93     47748\n",
            "           7       0.86      0.97      0.91     49291\n",
            "           8       0.94      0.71      0.81     69792\n",
            "           9       0.95      0.72      0.82     72025\n",
            "\n",
            "    accuracy                           0.87    540000\n",
            "   macro avg       0.87      0.89      0.87    540000\n",
            "weighted avg       0.89      0.87      0.87    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.901 Loss: 0.308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     57617\n",
            "           1       0.81      1.00      0.89     50621\n",
            "           2       0.90      0.93      0.92     54710\n",
            "           3       0.95      0.78      0.86     67440\n",
            "           4       0.69      0.97      0.81     38576\n",
            "           5       0.74      0.96      0.84     37910\n",
            "           6       0.90      0.97      0.93     48650\n",
            "           7       0.86      0.97      0.91     50210\n",
            "           8       0.94      0.71      0.81     70969\n",
            "           9       0.95      0.72      0.82     73297\n",
            "\n",
            "    accuracy                           0.87    550000\n",
            "   macro avg       0.87      0.89      0.87    550000\n",
            "weighted avg       0.89      0.87      0.87    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.908 Loss: 0.288\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     58657\n",
            "           1       0.81      1.00      0.89     51567\n",
            "           2       0.90      0.93      0.92     55719\n",
            "           3       0.95      0.78      0.86     68543\n",
            "           4       0.69      0.97      0.81     39446\n",
            "           5       0.74      0.96      0.84     38692\n",
            "           6       0.90      0.97      0.93     49559\n",
            "           7       0.86      0.97      0.91     51125\n",
            "           8       0.94      0.71      0.81     72211\n",
            "           9       0.95      0.72      0.82     74481\n",
            "\n",
            "    accuracy                           0.87    560000\n",
            "   macro avg       0.87      0.89      0.87    560000\n",
            "weighted avg       0.89      0.87      0.87    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.869 Loss: 0.388\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     59706\n",
            "           1       0.81      1.00      0.89     52370\n",
            "           2       0.90      0.93      0.92     56673\n",
            "           3       0.95      0.79      0.86     69757\n",
            "           4       0.69      0.97      0.81     40120\n",
            "           5       0.74      0.96      0.84     39488\n",
            "           6       0.90      0.97      0.93     50451\n",
            "           7       0.86      0.97      0.91     52044\n",
            "           8       0.94      0.71      0.81     73598\n",
            "           9       0.95      0.72      0.82     75793\n",
            "\n",
            "    accuracy                           0.87    570000\n",
            "   macro avg       0.87      0.89      0.87    570000\n",
            "weighted avg       0.89      0.87      0.87    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.890 Loss: 0.337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     60777\n",
            "           1       0.81      1.00      0.89     53284\n",
            "           2       0.90      0.93      0.92     57667\n",
            "           3       0.95      0.79      0.86     70901\n",
            "           4       0.69      0.97      0.81     40848\n",
            "           5       0.75      0.96      0.84     40263\n",
            "           6       0.90      0.97      0.93     51345\n",
            "           7       0.86      0.97      0.91     52971\n",
            "           8       0.95      0.71      0.81     74848\n",
            "           9       0.95      0.72      0.82     77096\n",
            "\n",
            "    accuracy                           0.87    580000\n",
            "   macro avg       0.87      0.89      0.88    580000\n",
            "weighted avg       0.89      0.87      0.87    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.895 Loss: 0.319\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     61817\n",
            "           1       0.81      1.00      0.89     54204\n",
            "           2       0.90      0.94      0.92     58670\n",
            "           3       0.95      0.79      0.86     72067\n",
            "           4       0.70      0.97      0.81     41627\n",
            "           5       0.75      0.96      0.84     40991\n",
            "           6       0.90      0.97      0.93     52254\n",
            "           7       0.86      0.97      0.91     53908\n",
            "           8       0.95      0.71      0.81     76154\n",
            "           9       0.95      0.72      0.82     78308\n",
            "\n",
            "    accuracy                           0.88    590000\n",
            "   macro avg       0.87      0.89      0.88    590000\n",
            "weighted avg       0.89      0.88      0.87    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.881 Loss: 0.361\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     62860\n",
            "           1       0.80      1.00      0.89     55012\n",
            "           2       0.90      0.94      0.92     59661\n",
            "           3       0.95      0.79      0.86     73276\n",
            "           4       0.70      0.97      0.81     42348\n",
            "           5       0.75      0.96      0.84     41793\n",
            "           6       0.90      0.97      0.93     53149\n",
            "           7       0.86      0.97      0.91     54852\n",
            "           8       0.95      0.71      0.81     77481\n",
            "           9       0.95      0.72      0.82     79568\n",
            "\n",
            "    accuracy                           0.88    600000\n",
            "   macro avg       0.87      0.89      0.88    600000\n",
            "weighted avg       0.89      0.88      0.87    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.898 Loss: 0.313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     63928\n",
            "           1       0.81      1.00      0.89     55989\n",
            "           2       0.90      0.94      0.92     60670\n",
            "           3       0.95      0.79      0.86     74428\n",
            "           4       0.70      0.97      0.81     43086\n",
            "           5       0.75      0.96      0.84     42531\n",
            "           6       0.90      0.97      0.93     54052\n",
            "           7       0.86      0.97      0.92     55780\n",
            "           8       0.95      0.71      0.81     78709\n",
            "           9       0.95      0.72      0.82     80827\n",
            "\n",
            "    accuracy                           0.88    610000\n",
            "   macro avg       0.88      0.90      0.88    610000\n",
            "weighted avg       0.89      0.88      0.87    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.913 Loss: 0.278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     64973\n",
            "           1       0.81      1.00      0.89     56954\n",
            "           2       0.90      0.94      0.92     61701\n",
            "           3       0.95      0.79      0.86     75527\n",
            "           4       0.70      0.97      0.81     43950\n",
            "           5       0.75      0.96      0.84     43323\n",
            "           6       0.90      0.97      0.94     54965\n",
            "           7       0.86      0.97      0.92     56718\n",
            "           8       0.95      0.72      0.81     79913\n",
            "           9       0.95      0.72      0.82     81976\n",
            "\n",
            "    accuracy                           0.88    620000\n",
            "   macro avg       0.88      0.90      0.88    620000\n",
            "weighted avg       0.89      0.88      0.87    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.890 Loss: 0.339\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     66034\n",
            "           1       0.81      1.00      0.89     57827\n",
            "           2       0.90      0.94      0.92     62694\n",
            "           3       0.95      0.79      0.86     76682\n",
            "           4       0.70      0.97      0.81     44706\n",
            "           5       0.75      0.96      0.84     44109\n",
            "           6       0.90      0.97      0.94     55846\n",
            "           7       0.87      0.97      0.92     57692\n",
            "           8       0.95      0.72      0.81     81227\n",
            "           9       0.95      0.73      0.82     83183\n",
            "\n",
            "    accuracy                           0.88    630000\n",
            "   macro avg       0.88      0.90      0.88    630000\n",
            "weighted avg       0.89      0.88      0.88    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.905 Loss: 0.288\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     67089\n",
            "           1       0.81      1.00      0.89     58828\n",
            "           2       0.90      0.94      0.92     63695\n",
            "           3       0.95      0.79      0.86     77804\n",
            "           4       0.70      0.97      0.81     45488\n",
            "           5       0.75      0.96      0.84     44886\n",
            "           6       0.90      0.97      0.94     56744\n",
            "           7       0.87      0.97      0.92     58630\n",
            "           8       0.95      0.72      0.82     82398\n",
            "           9       0.95      0.73      0.82     84438\n",
            "\n",
            "    accuracy                           0.88    640000\n",
            "   macro avg       0.88      0.90      0.88    640000\n",
            "weighted avg       0.89      0.88      0.88    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.901 Loss: 0.308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     68143\n",
            "           1       0.81      1.00      0.89     59745\n",
            "           2       0.90      0.94      0.92     64700\n",
            "           3       0.95      0.79      0.86     78954\n",
            "           4       0.70      0.97      0.82     46321\n",
            "           5       0.75      0.96      0.84     45660\n",
            "           6       0.90      0.97      0.94     57647\n",
            "           7       0.87      0.97      0.92     59550\n",
            "           8       0.95      0.72      0.82     83656\n",
            "           9       0.95      0.73      0.82     85624\n",
            "\n",
            "    accuracy                           0.88    650000\n",
            "   macro avg       0.88      0.90      0.88    650000\n",
            "weighted avg       0.89      0.88      0.88    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.855 Loss: 0.434\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     69177\n",
            "           1       0.80      1.00      0.89     60401\n",
            "           2       0.90      0.94      0.92     65665\n",
            "           3       0.95      0.79      0.86     80228\n",
            "           4       0.70      0.97      0.81     46994\n",
            "           5       0.75      0.96      0.84     46462\n",
            "           6       0.90      0.97      0.94     58527\n",
            "           7       0.87      0.97      0.92     60478\n",
            "           8       0.95      0.72      0.81     85127\n",
            "           9       0.95      0.73      0.82     86941\n",
            "\n",
            "    accuracy                           0.88    660000\n",
            "   macro avg       0.88      0.90      0.88    660000\n",
            "weighted avg       0.89      0.88      0.88    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.897 Loss: 0.313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     70232\n",
            "           1       0.80      1.00      0.89     61366\n",
            "           2       0.90      0.94      0.92     66658\n",
            "           3       0.95      0.79      0.86     81363\n",
            "           4       0.70      0.97      0.82     47773\n",
            "           5       0.76      0.96      0.84     47254\n",
            "           6       0.90      0.97      0.94     59422\n",
            "           7       0.87      0.97      0.92     61362\n",
            "           8       0.95      0.72      0.82     86349\n",
            "           9       0.95      0.73      0.82     88221\n",
            "\n",
            "    accuracy                           0.88    670000\n",
            "   macro avg       0.88      0.90      0.88    670000\n",
            "weighted avg       0.89      0.88      0.88    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.898 Loss: 0.322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     71266\n",
            "           1       0.80      1.00      0.89     62244\n",
            "           2       0.90      0.94      0.92     67653\n",
            "           3       0.95      0.79      0.86     82523\n",
            "           4       0.71      0.97      0.82     48614\n",
            "           5       0.76      0.96      0.85     48052\n",
            "           6       0.90      0.97      0.94     60317\n",
            "           7       0.87      0.97      0.92     62280\n",
            "           8       0.95      0.72      0.82     87669\n",
            "           9       0.95      0.73      0.82     89382\n",
            "\n",
            "    accuracy                           0.88    680000\n",
            "   macro avg       0.88      0.90      0.88    680000\n",
            "weighted avg       0.89      0.88      0.88    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.862 Loss: 0.423\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     72315\n",
            "           1       0.80      1.00      0.89     62966\n",
            "           2       0.90      0.94      0.92     68648\n",
            "           3       0.95      0.79      0.86     83740\n",
            "           4       0.71      0.97      0.82     49293\n",
            "           5       0.76      0.96      0.85     48886\n",
            "           6       0.90      0.97      0.93     61165\n",
            "           7       0.87      0.97      0.92     63197\n",
            "           8       0.95      0.71      0.81     89120\n",
            "           9       0.95      0.73      0.82     90670\n",
            "\n",
            "    accuracy                           0.88    690000\n",
            "   macro avg       0.88      0.90      0.88    690000\n",
            "weighted avg       0.89      0.88      0.88    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.899 Loss: 0.309\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     73378\n",
            "           1       0.80      1.00      0.89     63955\n",
            "           2       0.90      0.94      0.92     69638\n",
            "           3       0.95      0.79      0.86     84852\n",
            "           4       0.71      0.97      0.82     50052\n",
            "           5       0.76      0.96      0.85     49683\n",
            "           6       0.90      0.97      0.93     62046\n",
            "           7       0.87      0.97      0.92     64110\n",
            "           8       0.95      0.72      0.82     90343\n",
            "           9       0.95      0.73      0.82     91943\n",
            "\n",
            "    accuracy                           0.88    700000\n",
            "   macro avg       0.88      0.90      0.88    700000\n",
            "weighted avg       0.89      0.88      0.88    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.894 Loss: 0.328\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     74428\n",
            "           1       0.80      1.00      0.89     64839\n",
            "           2       0.90      0.94      0.92     70639\n",
            "           3       0.95      0.79      0.86     86025\n",
            "           4       0.71      0.97      0.82     50841\n",
            "           5       0.76      0.96      0.85     50448\n",
            "           6       0.90      0.97      0.94     62957\n",
            "           7       0.87      0.97      0.92     65031\n",
            "           8       0.95      0.72      0.82     91646\n",
            "           9       0.95      0.73      0.83     93146\n",
            "\n",
            "    accuracy                           0.88    710000\n",
            "   macro avg       0.88      0.90      0.88    710000\n",
            "weighted avg       0.89      0.88      0.88    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.879 Loss: 0.360\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     75479\n",
            "           1       0.80      1.00      0.89     65673\n",
            "           2       0.90      0.94      0.92     71618\n",
            "           3       0.95      0.79      0.86     87217\n",
            "           4       0.71      0.97      0.82     51555\n",
            "           5       0.76      0.95      0.85     51268\n",
            "           6       0.90      0.97      0.94     63828\n",
            "           7       0.87      0.97      0.92     65958\n",
            "           8       0.95      0.72      0.82     93008\n",
            "           9       0.95      0.73      0.83     94396\n",
            "\n",
            "    accuracy                           0.88    720000\n",
            "   macro avg       0.88      0.90      0.88    720000\n",
            "weighted avg       0.89      0.88      0.88    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.905 Loss: 0.290\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     76537\n",
            "           1       0.80      1.00      0.89     66680\n",
            "           2       0.90      0.94      0.92     72609\n",
            "           3       0.95      0.79      0.87     88358\n",
            "           4       0.71      0.97      0.82     52359\n",
            "           5       0.76      0.95      0.85     52049\n",
            "           6       0.90      0.97      0.94     64717\n",
            "           7       0.87      0.97      0.92     66873\n",
            "           8       0.95      0.72      0.82     94214\n",
            "           9       0.95      0.73      0.83     95604\n",
            "\n",
            "    accuracy                           0.88    730000\n",
            "   macro avg       0.88      0.90      0.88    730000\n",
            "weighted avg       0.89      0.88      0.88    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.896 Loss: 0.322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     77577\n",
            "           1       0.80      1.00      0.89     67581\n",
            "           2       0.90      0.94      0.92     73601\n",
            "           3       0.95      0.79      0.87     89511\n",
            "           4       0.71      0.97      0.82     53154\n",
            "           5       0.76      0.95      0.85     52854\n",
            "           6       0.90      0.97      0.94     65608\n",
            "           7       0.87      0.97      0.92     67801\n",
            "           8       0.95      0.72      0.82     95508\n",
            "           9       0.95      0.73      0.83     96805\n",
            "\n",
            "    accuracy                           0.88    740000\n",
            "   macro avg       0.88      0.90      0.88    740000\n",
            "weighted avg       0.89      0.88      0.88    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.874 Loss: 0.379\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     78645\n",
            "           1       0.80      1.00      0.89     68323\n",
            "           2       0.90      0.94      0.92     74575\n",
            "           3       0.95      0.79      0.87     90712\n",
            "           4       0.71      0.97      0.82     53910\n",
            "           5       0.77      0.95      0.85     53643\n",
            "           6       0.90      0.97      0.94     66468\n",
            "           7       0.87      0.97      0.92     68757\n",
            "           8       0.95      0.72      0.82     96926\n",
            "           9       0.95      0.73      0.83     98041\n",
            "\n",
            "    accuracy                           0.88    750000\n",
            "   macro avg       0.88      0.90      0.88    750000\n",
            "weighted avg       0.89      0.88      0.88    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.911 Loss: 0.277\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     79703\n",
            "           1       0.80      1.00      0.89     69313\n",
            "           2       0.90      0.94      0.92     75573\n",
            "           3       0.95      0.80      0.87     91815\n",
            "           4       0.71      0.97      0.82     54749\n",
            "           5       0.77      0.95      0.85     54442\n",
            "           6       0.90      0.97      0.94     67364\n",
            "           7       0.87      0.97      0.92     69685\n",
            "           8       0.95      0.72      0.82     98137\n",
            "           9       0.95      0.73      0.83     99219\n",
            "\n",
            "    accuracy                           0.88    760000\n",
            "   macro avg       0.88      0.90      0.88    760000\n",
            "weighted avg       0.89      0.88      0.88    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.910 Loss: 0.288\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     80761\n",
            "           1       0.80      1.00      0.89     70258\n",
            "           2       0.90      0.94      0.92     76596\n",
            "           3       0.95      0.80      0.87     92938\n",
            "           4       0.71      0.97      0.82     55589\n",
            "           5       0.77      0.95      0.85     55214\n",
            "           6       0.90      0.97      0.94     68273\n",
            "           7       0.87      0.97      0.92     70639\n",
            "           8       0.95      0.72      0.82     99367\n",
            "           9       0.95      0.73      0.83    100365\n",
            "\n",
            "    accuracy                           0.88    770000\n",
            "   macro avg       0.88      0.90      0.88    770000\n",
            "weighted avg       0.89      0.88      0.88    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.891 Loss: 0.337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     81827\n",
            "           1       0.80      1.00      0.89     71137\n",
            "           2       0.90      0.94      0.92     77605\n",
            "           3       0.95      0.80      0.87     94031\n",
            "           4       0.71      0.97      0.82     56301\n",
            "           5       0.77      0.95      0.85     56012\n",
            "           6       0.90      0.97      0.94     69167\n",
            "           7       0.87      0.97      0.92     71600\n",
            "           8       0.95      0.72      0.82    100711\n",
            "           9       0.95      0.74      0.83    101609\n",
            "\n",
            "    accuracy                           0.88    780000\n",
            "   macro avg       0.88      0.90      0.88    780000\n",
            "weighted avg       0.89      0.88      0.88    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.916 Loss: 0.260\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     82872\n",
            "           1       0.80      1.00      0.89     72157\n",
            "           2       0.91      0.94      0.92     78613\n",
            "           3       0.95      0.80      0.87     95150\n",
            "           4       0.71      0.97      0.82     57143\n",
            "           5       0.77      0.95      0.85     56793\n",
            "           6       0.90      0.97      0.94     70081\n",
            "           7       0.87      0.97      0.92     72531\n",
            "           8       0.95      0.72      0.82    101881\n",
            "           9       0.95      0.74      0.83    102779\n",
            "\n",
            "    accuracy                           0.88    790000\n",
            "   macro avg       0.88      0.90      0.88    790000\n",
            "weighted avg       0.89      0.88      0.88    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.906 Loss: 0.289\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     83918\n",
            "           1       0.80      1.00      0.89     73118\n",
            "           2       0.91      0.94      0.92     79601\n",
            "           3       0.95      0.80      0.87     96237\n",
            "           4       0.72      0.97      0.82     57962\n",
            "           5       0.77      0.95      0.85     57573\n",
            "           6       0.90      0.97      0.94     70992\n",
            "           7       0.87      0.97      0.92     73458\n",
            "           8       0.95      0.72      0.82    103163\n",
            "           9       0.95      0.74      0.83    103978\n",
            "\n",
            "    accuracy                           0.88    800000\n",
            "   macro avg       0.88      0.90      0.88    800000\n",
            "weighted avg       0.89      0.88      0.88    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.901 Loss: 0.308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     84980\n",
            "           1       0.80      1.00      0.89     74042\n",
            "           2       0.91      0.94      0.92     80607\n",
            "           3       0.95      0.80      0.87     97357\n",
            "           4       0.72      0.97      0.82     58730\n",
            "           5       0.77      0.95      0.85     58368\n",
            "           6       0.90      0.97      0.94     71904\n",
            "           7       0.87      0.97      0.92     74404\n",
            "           8       0.95      0.72      0.82    104422\n",
            "           9       0.95      0.74      0.83    105186\n",
            "\n",
            "    accuracy                           0.88    810000\n",
            "   macro avg       0.88      0.90      0.88    810000\n",
            "weighted avg       0.89      0.88      0.88    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.910 Loss: 0.287\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     86032\n",
            "           1       0.80      1.00      0.89     75006\n",
            "           2       0.91      0.94      0.92     81618\n",
            "           3       0.95      0.80      0.87     98475\n",
            "           4       0.72      0.97      0.82     59552\n",
            "           5       0.77      0.95      0.85     59123\n",
            "           6       0.90      0.97      0.94     72830\n",
            "           7       0.87      0.97      0.92     75353\n",
            "           8       0.95      0.72      0.82    105673\n",
            "           9       0.95      0.74      0.83    106338\n",
            "\n",
            "    accuracy                           0.88    820000\n",
            "   macro avg       0.88      0.90      0.88    820000\n",
            "weighted avg       0.89      0.88      0.88    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.907 Loss: 0.303\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     87068\n",
            "           1       0.80      1.00      0.89     75909\n",
            "           2       0.91      0.94      0.92     82628\n",
            "           3       0.95      0.80      0.87     99617\n",
            "           4       0.72      0.97      0.83     60419\n",
            "           5       0.77      0.95      0.85     59898\n",
            "           6       0.90      0.97      0.94     73762\n",
            "           7       0.87      0.97      0.92     76270\n",
            "           8       0.95      0.72      0.82    106957\n",
            "           9       0.95      0.74      0.83    107472\n",
            "\n",
            "    accuracy                           0.88    830000\n",
            "   macro avg       0.88      0.90      0.88    830000\n",
            "weighted avg       0.89      0.88      0.88    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.879 Loss: 0.376\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     88125\n",
            "           1       0.80      1.00      0.89     76655\n",
            "           2       0.91      0.94      0.92     83620\n",
            "           3       0.95      0.80      0.87    100785\n",
            "           4       0.72      0.97      0.83     61183\n",
            "           5       0.77      0.95      0.85     60704\n",
            "           6       0.90      0.97      0.94     74668\n",
            "           7       0.87      0.97      0.92     77190\n",
            "           8       0.95      0.72      0.82    108418\n",
            "           9       0.95      0.74      0.83    108652\n",
            "\n",
            "    accuracy                           0.88    840000\n",
            "   macro avg       0.88      0.90      0.88    840000\n",
            "weighted avg       0.89      0.88      0.88    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.909 Loss: 0.282\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     89186\n",
            "           1       0.80      1.00      0.89     77653\n",
            "           2       0.91      0.94      0.92     84615\n",
            "           3       0.95      0.80      0.87    101886\n",
            "           4       0.72      0.97      0.83     61988\n",
            "           5       0.77      0.95      0.85     61497\n",
            "           6       0.90      0.97      0.94     75571\n",
            "           7       0.87      0.97      0.92     78127\n",
            "           8       0.95      0.72      0.82    109639\n",
            "           9       0.95      0.74      0.83    109838\n",
            "\n",
            "    accuracy                           0.88    850000\n",
            "   macro avg       0.88      0.90      0.88    850000\n",
            "weighted avg       0.90      0.88      0.88    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.920 Loss: 0.254\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     90228\n",
            "           1       0.80      1.00      0.89     78665\n",
            "           2       0.91      0.94      0.92     85617\n",
            "           3       0.95      0.80      0.87    102988\n",
            "           4       0.72      0.97      0.83     62847\n",
            "           5       0.77      0.95      0.86     62285\n",
            "           6       0.90      0.97      0.94     76504\n",
            "           7       0.87      0.97      0.92     79071\n",
            "           8       0.95      0.72      0.82    110835\n",
            "           9       0.95      0.74      0.83    110960\n",
            "\n",
            "    accuracy                           0.88    860000\n",
            "   macro avg       0.88      0.90      0.88    860000\n",
            "weighted avg       0.90      0.88      0.88    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.890 Loss: 0.336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     91280\n",
            "           1       0.80      1.00      0.89     79537\n",
            "           2       0.91      0.94      0.92     86610\n",
            "           3       0.95      0.80      0.87    104121\n",
            "           4       0.72      0.97      0.83     63577\n",
            "           5       0.78      0.95      0.86     63061\n",
            "           6       0.90      0.97      0.94     77413\n",
            "           7       0.87      0.97      0.92     80016\n",
            "           8       0.95      0.72      0.82    112179\n",
            "           9       0.95      0.74      0.83    112206\n",
            "\n",
            "    accuracy                           0.88    870000\n",
            "   macro avg       0.88      0.90      0.88    870000\n",
            "weighted avg       0.90      0.88      0.88    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.915 Loss: 0.262\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     92342\n",
            "           1       0.80      1.00      0.89     80555\n",
            "           2       0.91      0.94      0.92     87612\n",
            "           3       0.95      0.80      0.87    105247\n",
            "           4       0.72      0.97      0.83     64409\n",
            "           5       0.78      0.95      0.86     63833\n",
            "           6       0.90      0.97      0.94     78328\n",
            "           7       0.87      0.97      0.92     80952\n",
            "           8       0.95      0.72      0.82    113334\n",
            "           9       0.95      0.74      0.83    113388\n",
            "\n",
            "    accuracy                           0.88    880000\n",
            "   macro avg       0.88      0.90      0.88    880000\n",
            "weighted avg       0.90      0.88      0.88    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.911 Loss: 0.285\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     93390\n",
            "           1       0.80      1.00      0.89     81508\n",
            "           2       0.91      0.94      0.92     88632\n",
            "           3       0.95      0.80      0.87    106393\n",
            "           4       0.72      0.97      0.83     65244\n",
            "           5       0.78      0.95      0.86     64628\n",
            "           6       0.90      0.97      0.94     79248\n",
            "           7       0.87      0.97      0.92     81884\n",
            "           8       0.95      0.72      0.82    114539\n",
            "           9       0.95      0.74      0.83    114534\n",
            "\n",
            "    accuracy                           0.88    890000\n",
            "   macro avg       0.88      0.90      0.88    890000\n",
            "weighted avg       0.90      0.88      0.88    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.887 Loss: 0.343\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     94436\n",
            "           1       0.80      1.00      0.89     82326\n",
            "           2       0.91      0.94      0.92     89634\n",
            "           3       0.95      0.80      0.87    107588\n",
            "           4       0.72      0.97      0.83     65997\n",
            "           5       0.78      0.95      0.86     65421\n",
            "           6       0.90      0.97      0.94     80130\n",
            "           7       0.87      0.97      0.92     82835\n",
            "           8       0.95      0.72      0.82    115855\n",
            "           9       0.95      0.74      0.83    115778\n",
            "\n",
            "    accuracy                           0.88    900000\n",
            "   macro avg       0.88      0.90      0.88    900000\n",
            "weighted avg       0.90      0.88      0.88    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.drop1 = nn.Dropout(p=0.7)\n",
        "    self.drop2 = nn.Dropout(p=0.5)\n",
        "    self.linear2 = nn.Linear(100, 100)\n",
        "    self.batch = nn.BatchNorm1d(100)\n",
        "    self.act = nn.Sigmoid()\n",
        "    self.linear3 = nn.Linear(100,10)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop1(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop2(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear3(out)\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "zRdd4tSZenMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "Pqr3vj_xenb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgoaTRWtenmc",
        "outputId": "1ae3f3ea-1638-48fc-cb6b-60b984582bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.835 Loss: 1.597\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.93      1059\n",
            "           1       0.97      0.93      0.95      1183\n",
            "           2       0.87      0.78      0.82      1145\n",
            "           3       0.86      0.75      0.80      1164\n",
            "           4       0.66      0.97      0.78       669\n",
            "           5       0.44      0.96      0.60       412\n",
            "           6       0.94      0.89      0.92      1012\n",
            "           7       0.81      0.96      0.88       873\n",
            "           8       0.83      0.79      0.81      1031\n",
            "           9       0.93      0.65      0.77      1452\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.83      0.86      0.83     10000\n",
            "weighted avg       0.87      0.83      0.84     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.832 Loss: 1.145\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.95      2023\n",
            "           1       0.93      0.95      0.94      2218\n",
            "           2       0.89      0.77      0.83      2376\n",
            "           3       0.85      0.80      0.83      2139\n",
            "           4       0.71      0.97      0.82      1447\n",
            "           5       0.38      0.96      0.55       710\n",
            "           6       0.91      0.93      0.92      1883\n",
            "           7       0.86      0.94      0.89      1880\n",
            "           8       0.90      0.61      0.73      2879\n",
            "           9       0.88      0.73      0.80      2445\n",
            "\n",
            "    accuracy                           0.83     20000\n",
            "   macro avg       0.83      0.86      0.82     20000\n",
            "weighted avg       0.87      0.83      0.84     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.839 Loss: 0.987\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.94      0.95      2979\n",
            "           1       0.90      0.96      0.93      3163\n",
            "           2       0.89      0.80      0.84      3454\n",
            "           3       0.85      0.84      0.84      3080\n",
            "           4       0.69      0.97      0.81      2114\n",
            "           5       0.47      0.94      0.62      1326\n",
            "           6       0.91      0.94      0.92      2781\n",
            "           7       0.85      0.94      0.89      2769\n",
            "           8       0.92      0.58      0.71      4691\n",
            "           9       0.88      0.73      0.79      3643\n",
            "\n",
            "    accuracy                           0.84     30000\n",
            "   macro avg       0.83      0.86      0.83     30000\n",
            "weighted avg       0.86      0.84      0.84     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.849 Loss: 0.815\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.95      3978\n",
            "           1       0.89      0.97      0.93      4159\n",
            "           2       0.89      0.81      0.85      4561\n",
            "           3       0.85      0.86      0.85      3991\n",
            "           4       0.68      0.97      0.80      2773\n",
            "           5       0.50      0.94      0.66      1908\n",
            "           6       0.90      0.95      0.92      3621\n",
            "           7       0.85      0.95      0.90      3676\n",
            "           8       0.94      0.57      0.71      6398\n",
            "           9       0.89      0.72      0.80      4935\n",
            "\n",
            "    accuracy                           0.84     40000\n",
            "   macro avg       0.83      0.87      0.84     40000\n",
            "weighted avg       0.86      0.84      0.84     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.855 Loss: 0.693\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96      4933\n",
            "           1       0.88      0.97      0.92      5119\n",
            "           2       0.90      0.82      0.86      5655\n",
            "           3       0.85      0.87      0.86      4958\n",
            "           4       0.70      0.97      0.81      3550\n",
            "           5       0.51      0.95      0.67      2419\n",
            "           6       0.90      0.95      0.93      4533\n",
            "           7       0.86      0.95      0.90      4614\n",
            "           8       0.95      0.56      0.70      8222\n",
            "           9       0.88      0.74      0.81      5997\n",
            "\n",
            "    accuracy                           0.84     50000\n",
            "   macro avg       0.84      0.87      0.84     50000\n",
            "weighted avg       0.87      0.84      0.84     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.829 Loss: 0.676\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96      5933\n",
            "           1       0.87      0.97      0.92      6060\n",
            "           2       0.89      0.83      0.86      6662\n",
            "           3       0.85      0.87      0.86      5951\n",
            "           4       0.68      0.97      0.80      4151\n",
            "           5       0.51      0.95      0.67      2891\n",
            "           6       0.90      0.95      0.93      5410\n",
            "           7       0.85      0.96      0.90      5507\n",
            "           8       0.95      0.55      0.69     10172\n",
            "           9       0.88      0.74      0.80      7263\n",
            "\n",
            "    accuracy                           0.84     60000\n",
            "   macro avg       0.84      0.87      0.84     60000\n",
            "weighted avg       0.87      0.84      0.84     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.850 Loss: 0.621\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96      6952\n",
            "           1       0.86      0.98      0.91      6975\n",
            "           2       0.90      0.84      0.87      7699\n",
            "           3       0.86      0.87      0.86      6911\n",
            "           4       0.69      0.97      0.81      4880\n",
            "           5       0.52      0.95      0.67      3410\n",
            "           6       0.90      0.96      0.93      6279\n",
            "           7       0.86      0.96      0.90      6433\n",
            "           8       0.96      0.54      0.69     12023\n",
            "           9       0.89      0.74      0.81      8438\n",
            "\n",
            "    accuracy                           0.84     70000\n",
            "   macro avg       0.84      0.88      0.84     70000\n",
            "weighted avg       0.87      0.84      0.84     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.867 Loss: 0.550\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      7951\n",
            "           1       0.86      0.98      0.91      7983\n",
            "           2       0.90      0.85      0.87      8741\n",
            "           3       0.85      0.88      0.87      7847\n",
            "           4       0.70      0.97      0.81      5643\n",
            "           5       0.53      0.96      0.68      3961\n",
            "           6       0.90      0.96      0.93      7189\n",
            "           7       0.86      0.96      0.91      7380\n",
            "           8       0.96      0.54      0.69     13750\n",
            "           9       0.89      0.75      0.81      9555\n",
            "\n",
            "    accuracy                           0.84     80000\n",
            "   macro avg       0.84      0.88      0.84     80000\n",
            "weighted avg       0.87      0.84      0.84     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.870 Loss: 0.531\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      8980\n",
            "           1       0.86      0.98      0.91      8951\n",
            "           2       0.90      0.85      0.88      9770\n",
            "           3       0.86      0.88      0.87      8877\n",
            "           4       0.70      0.97      0.81      6336\n",
            "           5       0.54      0.96      0.69      4538\n",
            "           6       0.90      0.96      0.93      8107\n",
            "           7       0.86      0.96      0.91      8315\n",
            "           8       0.96      0.55      0.70     15308\n",
            "           9       0.89      0.75      0.81     10818\n",
            "\n",
            "    accuracy                           0.85     90000\n",
            "   macro avg       0.84      0.88      0.85     90000\n",
            "weighted avg       0.87      0.85      0.84     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.875 Loss: 0.482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      9999\n",
            "           1       0.86      0.98      0.91      9929\n",
            "           2       0.90      0.86      0.88     10794\n",
            "           3       0.86      0.88      0.87      9888\n",
            "           4       0.70      0.97      0.81      7093\n",
            "           5       0.55      0.96      0.70      5112\n",
            "           6       0.90      0.96      0.93      9012\n",
            "           7       0.86      0.96      0.91      9251\n",
            "           8       0.96      0.55      0.70     16886\n",
            "           9       0.90      0.75      0.82     12036\n",
            "\n",
            "    accuracy                           0.85    100000\n",
            "   macro avg       0.85      0.88      0.85    100000\n",
            "weighted avg       0.87      0.85      0.85    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.899 Loss: 0.411\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     11006\n",
            "           1       0.86      0.98      0.92     10919\n",
            "           2       0.90      0.86      0.88     11875\n",
            "           3       0.86      0.88      0.87     10859\n",
            "           4       0.71      0.97      0.82      7944\n",
            "           5       0.57      0.96      0.71      5796\n",
            "           6       0.91      0.96      0.93      9953\n",
            "           7       0.87      0.96      0.91     10213\n",
            "           8       0.96      0.56      0.71     18296\n",
            "           9       0.90      0.76      0.82     13139\n",
            "\n",
            "    accuracy                           0.85    110000\n",
            "   macro avg       0.85      0.89      0.86    110000\n",
            "weighted avg       0.88      0.85      0.85    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.882 Loss: 0.445\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     12037\n",
            "           1       0.85      0.98      0.91     11811\n",
            "           2       0.90      0.87      0.89     12842\n",
            "           3       0.87      0.89      0.88     11875\n",
            "           4       0.72      0.97      0.82      8692\n",
            "           5       0.58      0.96      0.73      6506\n",
            "           6       0.91      0.96      0.93     10893\n",
            "           7       0.87      0.96      0.91     11184\n",
            "           8       0.96      0.57      0.72     19804\n",
            "           9       0.90      0.76      0.83     14356\n",
            "\n",
            "    accuracy                           0.86    120000\n",
            "   macro avg       0.85      0.89      0.86    120000\n",
            "weighted avg       0.88      0.86      0.85    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.896 Loss: 0.395\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     13064\n",
            "           1       0.85      0.98      0.91     12806\n",
            "           2       0.90      0.87      0.89     13865\n",
            "           3       0.87      0.89      0.88     12859\n",
            "           4       0.72      0.97      0.83      9525\n",
            "           5       0.59      0.96      0.73      7145\n",
            "           6       0.91      0.96      0.94     11834\n",
            "           7       0.87      0.96      0.91     12136\n",
            "           8       0.96      0.58      0.72     21240\n",
            "           9       0.91      0.76      0.83     15526\n",
            "\n",
            "    accuracy                           0.86    130000\n",
            "   macro avg       0.86      0.89      0.86    130000\n",
            "weighted avg       0.88      0.86      0.86    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.899 Loss: 0.375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     14074\n",
            "           1       0.85      0.98      0.91     13773\n",
            "           2       0.91      0.88      0.89     14938\n",
            "           3       0.87      0.89      0.88     13834\n",
            "           4       0.73      0.97      0.84     10392\n",
            "           5       0.60      0.96      0.74      7823\n",
            "           6       0.91      0.96      0.94     12772\n",
            "           7       0.87      0.96      0.92     13099\n",
            "           8       0.97      0.58      0.73     22679\n",
            "           9       0.91      0.77      0.83     16616\n",
            "\n",
            "    accuracy                           0.86    140000\n",
            "   macro avg       0.86      0.89      0.86    140000\n",
            "weighted avg       0.88      0.86      0.86    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.886 Loss: 0.395\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     15098\n",
            "           1       0.85      0.98      0.91     14640\n",
            "           2       0.91      0.88      0.89     15951\n",
            "           3       0.87      0.89      0.88     14853\n",
            "           4       0.74      0.97      0.84     11212\n",
            "           5       0.61      0.96      0.75      8488\n",
            "           6       0.91      0.96      0.94     13686\n",
            "           7       0.88      0.96      0.92     14079\n",
            "           8       0.97      0.58      0.73     24231\n",
            "           9       0.91      0.77      0.84     17762\n",
            "\n",
            "    accuracy                           0.86    150000\n",
            "   macro avg       0.86      0.89      0.87    150000\n",
            "weighted avg       0.88      0.86      0.86    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.899 Loss: 0.349\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     16123\n",
            "           1       0.85      0.98      0.91     15665\n",
            "           2       0.91      0.88      0.90     16956\n",
            "           3       0.88      0.89      0.88     15925\n",
            "           4       0.74      0.97      0.84     12055\n",
            "           5       0.62      0.97      0.75      9116\n",
            "           6       0.91      0.96      0.94     14604\n",
            "           7       0.88      0.96      0.92     15026\n",
            "           8       0.97      0.59      0.73     25632\n",
            "           9       0.91      0.78      0.84     18898\n",
            "\n",
            "    accuracy                           0.87    160000\n",
            "   macro avg       0.86      0.89      0.87    160000\n",
            "weighted avg       0.88      0.87      0.86    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.910 Loss: 0.320\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     17128\n",
            "           1       0.85      0.98      0.91     16692\n",
            "           2       0.91      0.89      0.90     18009\n",
            "           3       0.88      0.89      0.89     16920\n",
            "           4       0.75      0.97      0.85     12927\n",
            "           5       0.62      0.97      0.76      9810\n",
            "           6       0.92      0.96      0.94     15546\n",
            "           7       0.88      0.96      0.92     15991\n",
            "           8       0.97      0.59      0.74     26992\n",
            "           9       0.91      0.78      0.84     19985\n",
            "\n",
            "    accuracy                           0.87    170000\n",
            "   macro avg       0.87      0.89      0.87    170000\n",
            "weighted avg       0.89      0.87      0.87    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.898 Loss: 0.349\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     18162\n",
            "           1       0.85      0.98      0.91     17650\n",
            "           2       0.91      0.89      0.90     19032\n",
            "           3       0.88      0.89      0.89     17985\n",
            "           4       0.75      0.97      0.85     13737\n",
            "           5       0.63      0.97      0.76     10501\n",
            "           6       0.92      0.96      0.94     16463\n",
            "           7       0.88      0.96      0.92     16943\n",
            "           8       0.97      0.60      0.74     28361\n",
            "           9       0.91      0.78      0.84     21166\n",
            "\n",
            "    accuracy                           0.87    180000\n",
            "   macro avg       0.87      0.90      0.87    180000\n",
            "weighted avg       0.89      0.87      0.87    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.914 Loss: 0.302\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     19184\n",
            "           1       0.85      0.98      0.91     18690\n",
            "           2       0.91      0.89      0.90     20064\n",
            "           3       0.89      0.89      0.89     19017\n",
            "           4       0.76      0.97      0.85     14605\n",
            "           5       0.64      0.97      0.77     11191\n",
            "           6       0.92      0.96      0.94     17396\n",
            "           7       0.88      0.96      0.92     17906\n",
            "           8       0.97      0.60      0.74     29658\n",
            "           9       0.92      0.79      0.85     22289\n",
            "\n",
            "    accuracy                           0.87    190000\n",
            "   macro avg       0.87      0.90      0.87    190000\n",
            "weighted avg       0.89      0.87      0.87    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.924 Loss: 0.269\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     20193\n",
            "           1       0.86      0.99      0.92     19727\n",
            "           2       0.91      0.89      0.90     21099\n",
            "           3       0.89      0.89      0.89     20061\n",
            "           4       0.77      0.97      0.86     15518\n",
            "           5       0.65      0.97      0.77     11932\n",
            "           6       0.92      0.96      0.94     18355\n",
            "           7       0.88      0.96      0.92     18878\n",
            "           8       0.97      0.61      0.75     30895\n",
            "           9       0.92      0.79      0.85     23342\n",
            "\n",
            "    accuracy                           0.88    200000\n",
            "   macro avg       0.87      0.90      0.88    200000\n",
            "weighted avg       0.89      0.88      0.87    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.913 Loss: 0.301\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     21214\n",
            "           1       0.86      0.99      0.92     20720\n",
            "           2       0.91      0.89      0.90     22123\n",
            "           3       0.89      0.89      0.89     21142\n",
            "           4       0.77      0.97      0.86     16373\n",
            "           5       0.65      0.97      0.78     12671\n",
            "           6       0.92      0.96      0.94     19298\n",
            "           7       0.88      0.96      0.92     19838\n",
            "           8       0.97      0.62      0.75     32166\n",
            "           9       0.92      0.79      0.85     24455\n",
            "\n",
            "    accuracy                           0.88    210000\n",
            "   macro avg       0.88      0.90      0.88    210000\n",
            "weighted avg       0.89      0.88      0.87    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.924 Loss: 0.263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     22234\n",
            "           1       0.86      0.99      0.92     21783\n",
            "           2       0.91      0.90      0.90     23159\n",
            "           3       0.89      0.89      0.89     22238\n",
            "           4       0.78      0.97      0.86     17320\n",
            "           5       0.66      0.97      0.78     13355\n",
            "           6       0.92      0.96      0.94     20260\n",
            "           7       0.88      0.96      0.92     20785\n",
            "           8       0.97      0.62      0.76     33346\n",
            "           9       0.92      0.80      0.85     25520\n",
            "\n",
            "    accuracy                           0.88    220000\n",
            "   macro avg       0.88      0.90      0.88    220000\n",
            "weighted avg       0.89      0.88      0.88    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.927 Loss: 0.251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     23237\n",
            "           1       0.86      0.99      0.92     22830\n",
            "           2       0.91      0.90      0.91     24190\n",
            "           3       0.90      0.89      0.89     23337\n",
            "           4       0.78      0.97      0.87     18258\n",
            "           5       0.67      0.97      0.79     14093\n",
            "           6       0.92      0.96      0.94     21218\n",
            "           7       0.89      0.96      0.92     21735\n",
            "           8       0.97      0.63      0.76     34529\n",
            "           9       0.92      0.80      0.86     26573\n",
            "\n",
            "    accuracy                           0.88    230000\n",
            "   macro avg       0.88      0.90      0.88    230000\n",
            "weighted avg       0.89      0.88      0.88    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.918 Loss: 0.278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     24260\n",
            "           1       0.86      0.99      0.92     23834\n",
            "           2       0.92      0.90      0.91     25198\n",
            "           3       0.90      0.89      0.89     24423\n",
            "           4       0.79      0.97      0.87     19141\n",
            "           5       0.67      0.97      0.79     14854\n",
            "           6       0.92      0.96      0.94     22144\n",
            "           7       0.89      0.96      0.92     22706\n",
            "           8       0.97      0.63      0.77     35782\n",
            "           9       0.92      0.80      0.86     27658\n",
            "\n",
            "    accuracy                           0.88    240000\n",
            "   macro avg       0.88      0.90      0.88    240000\n",
            "weighted avg       0.90      0.88      0.88    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.925 Loss: 0.256\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     25280\n",
            "           1       0.86      0.99      0.92     24895\n",
            "           2       0.92      0.90      0.91     26222\n",
            "           3       0.90      0.89      0.90     25496\n",
            "           4       0.79      0.97      0.87     20031\n",
            "           5       0.68      0.97      0.80     15590\n",
            "           6       0.92      0.96      0.94     23067\n",
            "           7       0.89      0.96      0.92     23679\n",
            "           8       0.97      0.64      0.77     36958\n",
            "           9       0.92      0.81      0.86     28782\n",
            "\n",
            "    accuracy                           0.88    250000\n",
            "   macro avg       0.88      0.90      0.89    250000\n",
            "weighted avg       0.90      0.88      0.88    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.934 Loss: 0.227\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     26293\n",
            "           1       0.87      0.99      0.92     25969\n",
            "           2       0.92      0.90      0.91     27268\n",
            "           3       0.90      0.89      0.90     26577\n",
            "           4       0.79      0.97      0.87     20957\n",
            "           5       0.68      0.97      0.80     16358\n",
            "           6       0.93      0.96      0.94     24022\n",
            "           7       0.89      0.96      0.92     24644\n",
            "           8       0.97      0.64      0.77     38077\n",
            "           9       0.92      0.81      0.86     29835\n",
            "\n",
            "    accuracy                           0.89    260000\n",
            "   macro avg       0.88      0.90      0.89    260000\n",
            "weighted avg       0.90      0.89      0.88    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.922 Loss: 0.261\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     27323\n",
            "           1       0.87      0.99      0.92     26984\n",
            "           2       0.92      0.90      0.91     28298\n",
            "           3       0.90      0.89      0.90     27672\n",
            "           4       0.80      0.97      0.87     21843\n",
            "           5       0.69      0.97      0.80     17129\n",
            "           6       0.93      0.96      0.94     24952\n",
            "           7       0.89      0.96      0.93     25599\n",
            "           8       0.97      0.65      0.78     39267\n",
            "           9       0.92      0.81      0.86     30933\n",
            "\n",
            "    accuracy                           0.89    270000\n",
            "   macro avg       0.89      0.90      0.89    270000\n",
            "weighted avg       0.90      0.89      0.89    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.926 Loss: 0.241\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     28341\n",
            "           1       0.87      0.99      0.92     28069\n",
            "           2       0.92      0.90      0.91     29312\n",
            "           3       0.91      0.89      0.90     28813\n",
            "           4       0.80      0.97      0.88     22740\n",
            "           5       0.69      0.97      0.81     17830\n",
            "           6       0.93      0.96      0.94     25899\n",
            "           7       0.89      0.96      0.93     26539\n",
            "           8       0.97      0.65      0.78     40408\n",
            "           9       0.92      0.81      0.87     32049\n",
            "\n",
            "    accuracy                           0.89    280000\n",
            "   macro avg       0.89      0.91      0.89    280000\n",
            "weighted avg       0.90      0.89      0.89    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.934 Loss: 0.220\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     29351\n",
            "           1       0.87      0.99      0.93     29138\n",
            "           2       0.92      0.91      0.91     30344\n",
            "           3       0.91      0.89      0.90     29895\n",
            "           4       0.80      0.97      0.88     23675\n",
            "           5       0.70      0.97      0.81     18579\n",
            "           6       0.93      0.96      0.94     26848\n",
            "           7       0.89      0.96      0.93     27503\n",
            "           8       0.97      0.66      0.78     41564\n",
            "           9       0.92      0.82      0.87     33103\n",
            "\n",
            "    accuracy                           0.89    290000\n",
            "   macro avg       0.89      0.91      0.89    290000\n",
            "weighted avg       0.90      0.89      0.89    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.924 Loss: 0.249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     30365\n",
            "           1       0.87      0.99      0.93     30171\n",
            "           2       0.92      0.91      0.91     31371\n",
            "           3       0.91      0.89      0.90     31038\n",
            "           4       0.81      0.97      0.88     24580\n",
            "           5       0.70      0.97      0.81     19292\n",
            "           6       0.93      0.96      0.94     27787\n",
            "           7       0.89      0.96      0.93     28480\n",
            "           8       0.97      0.66      0.79     42743\n",
            "           9       0.92      0.82      0.87     34173\n",
            "\n",
            "    accuracy                           0.89    300000\n",
            "   macro avg       0.89      0.91      0.89    300000\n",
            "weighted avg       0.90      0.89      0.89    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.932 Loss: 0.229\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     31380\n",
            "           1       0.88      0.99      0.93     31233\n",
            "           2       0.92      0.91      0.91     32412\n",
            "           3       0.91      0.89      0.90     32155\n",
            "           4       0.81      0.97      0.88     25495\n",
            "           5       0.70      0.97      0.81     20049\n",
            "           6       0.93      0.96      0.94     28721\n",
            "           7       0.89      0.96      0.93     29451\n",
            "           8       0.97      0.67      0.79     43859\n",
            "           9       0.93      0.82      0.87     35245\n",
            "\n",
            "    accuracy                           0.89    310000\n",
            "   macro avg       0.89      0.91      0.89    310000\n",
            "weighted avg       0.90      0.89      0.89    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.936 Loss: 0.215\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     32388\n",
            "           1       0.88      0.99      0.93     32325\n",
            "           2       0.92      0.91      0.91     33470\n",
            "           3       0.91      0.89      0.90     33284\n",
            "           4       0.81      0.97      0.88     26425\n",
            "           5       0.71      0.97      0.82     20790\n",
            "           6       0.93      0.96      0.94     29672\n",
            "           7       0.89      0.96      0.93     30429\n",
            "           8       0.97      0.67      0.79     44930\n",
            "           9       0.93      0.82      0.87     36287\n",
            "\n",
            "    accuracy                           0.89    320000\n",
            "   macro avg       0.89      0.91      0.89    320000\n",
            "weighted avg       0.90      0.89      0.89    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.927 Loss: 0.238\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     33398\n",
            "           1       0.88      0.99      0.93     33345\n",
            "           2       0.92      0.91      0.92     34497\n",
            "           3       0.91      0.89      0.90     34384\n",
            "           4       0.82      0.97      0.88     27331\n",
            "           5       0.71      0.97      0.82     21570\n",
            "           6       0.93      0.96      0.94     30595\n",
            "           7       0.89      0.97      0.93     31396\n",
            "           8       0.97      0.67      0.79     46120\n",
            "           9       0.93      0.83      0.87     37364\n",
            "\n",
            "    accuracy                           0.90    330000\n",
            "   macro avg       0.89      0.91      0.90    330000\n",
            "weighted avg       0.90      0.90      0.89    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.931 Loss: 0.221\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     34419\n",
            "           1       0.88      0.99      0.93     34420\n",
            "           2       0.92      0.91      0.92     35543\n",
            "           3       0.92      0.89      0.90     35522\n",
            "           4       0.82      0.97      0.89     28258\n",
            "           5       0.71      0.97      0.82     22283\n",
            "           6       0.93      0.96      0.95     31532\n",
            "           7       0.89      0.97      0.93     32362\n",
            "           8       0.97      0.68      0.80     47215\n",
            "           9       0.93      0.83      0.87     38446\n",
            "\n",
            "    accuracy                           0.90    340000\n",
            "   macro avg       0.89      0.91      0.90    340000\n",
            "weighted avg       0.91      0.90      0.89    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.937 Loss: 0.203\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     35422\n",
            "           1       0.88      0.99      0.93     35521\n",
            "           2       0.92      0.91      0.92     36612\n",
            "           3       0.92      0.89      0.90     36637\n",
            "           4       0.82      0.97      0.89     29181\n",
            "           5       0.72      0.97      0.82     23032\n",
            "           6       0.93      0.96      0.95     32483\n",
            "           7       0.89      0.97      0.93     33325\n",
            "           8       0.97      0.68      0.80     48289\n",
            "           9       0.93      0.83      0.88     39498\n",
            "\n",
            "    accuracy                           0.90    350000\n",
            "   macro avg       0.90      0.91      0.90    350000\n",
            "weighted avg       0.91      0.90      0.90    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.928 Loss: 0.227\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     36434\n",
            "           1       0.88      0.99      0.93     36565\n",
            "           2       0.92      0.91      0.92     37626\n",
            "           3       0.92      0.88      0.90     37735\n",
            "           4       0.82      0.97      0.89     30054\n",
            "           5       0.72      0.97      0.83     23819\n",
            "           6       0.93      0.96      0.95     33408\n",
            "           7       0.90      0.97      0.93     34311\n",
            "           8       0.97      0.69      0.80     49448\n",
            "           9       0.93      0.83      0.88     40600\n",
            "\n",
            "    accuracy                           0.90    360000\n",
            "   macro avg       0.90      0.91      0.90    360000\n",
            "weighted avg       0.91      0.90      0.90    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.938 Loss: 0.206\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     37453\n",
            "           1       0.88      0.99      0.93     37651\n",
            "           2       0.92      0.91      0.92     38644\n",
            "           3       0.92      0.88      0.90     38839\n",
            "           4       0.82      0.97      0.89     30979\n",
            "           5       0.72      0.97      0.83     24591\n",
            "           6       0.93      0.96      0.95     34349\n",
            "           7       0.90      0.97      0.93     35290\n",
            "           8       0.97      0.69      0.81     50541\n",
            "           9       0.93      0.83      0.88     41663\n",
            "\n",
            "    accuracy                           0.90    370000\n",
            "   macro avg       0.90      0.91      0.90    370000\n",
            "weighted avg       0.91      0.90      0.90    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.939 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     38460\n",
            "           1       0.89      0.99      0.93     38736\n",
            "           2       0.92      0.91      0.92     39690\n",
            "           3       0.92      0.89      0.90     39935\n",
            "           4       0.83      0.97      0.89     31911\n",
            "           5       0.73      0.97      0.83     25365\n",
            "           6       0.93      0.96      0.95     35302\n",
            "           7       0.90      0.97      0.93     36256\n",
            "           8       0.97      0.69      0.81     51619\n",
            "           9       0.93      0.83      0.88     42726\n",
            "\n",
            "    accuracy                           0.90    380000\n",
            "   macro avg       0.90      0.91      0.90    380000\n",
            "weighted avg       0.91      0.90      0.90    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.934 Loss: 0.216\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     39481\n",
            "           1       0.89      0.99      0.93     39783\n",
            "           2       0.92      0.91      0.92     40707\n",
            "           3       0.92      0.88      0.90     41047\n",
            "           4       0.83      0.97      0.89     32810\n",
            "           5       0.73      0.97      0.83     26151\n",
            "           6       0.93      0.96      0.95     36254\n",
            "           7       0.90      0.97      0.93     37226\n",
            "           8       0.97      0.70      0.81     52718\n",
            "           9       0.93      0.83      0.88     43823\n",
            "\n",
            "    accuracy                           0.90    390000\n",
            "   macro avg       0.90      0.91      0.90    390000\n",
            "weighted avg       0.91      0.90      0.90    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.944 Loss: 0.188\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     40498\n",
            "           1       0.89      0.99      0.93     40892\n",
            "           2       0.93      0.92      0.92     41734\n",
            "           3       0.92      0.89      0.90     42136\n",
            "           4       0.83      0.97      0.89     33745\n",
            "           5       0.73      0.97      0.83     26940\n",
            "           6       0.93      0.96      0.95     37197\n",
            "           7       0.90      0.97      0.93     38222\n",
            "           8       0.97      0.70      0.81     53758\n",
            "           9       0.93      0.84      0.88     44878\n",
            "\n",
            "    accuracy                           0.90    400000\n",
            "   macro avg       0.90      0.91      0.90    400000\n",
            "weighted avg       0.91      0.90      0.90    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.940 Loss: 0.194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     41511\n",
            "           1       0.89      0.99      0.94     41990\n",
            "           2       0.93      0.92      0.92     42788\n",
            "           3       0.92      0.88      0.90     43254\n",
            "           4       0.83      0.97      0.89     34678\n",
            "           5       0.74      0.97      0.84     27720\n",
            "           6       0.93      0.96      0.95     38141\n",
            "           7       0.90      0.97      0.93     39189\n",
            "           8       0.97      0.70      0.81     54798\n",
            "           9       0.93      0.84      0.88     45931\n",
            "\n",
            "    accuracy                           0.90    410000\n",
            "   macro avg       0.90      0.91      0.90    410000\n",
            "weighted avg       0.91      0.90      0.90    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.932 Loss: 0.208\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     42520\n",
            "           1       0.89      0.99      0.94     43057\n",
            "           2       0.93      0.92      0.92     43800\n",
            "           3       0.93      0.88      0.90     44362\n",
            "           4       0.83      0.97      0.89     35568\n",
            "           5       0.74      0.97      0.84     28506\n",
            "           6       0.93      0.96      0.95     39084\n",
            "           7       0.90      0.97      0.93     40164\n",
            "           8       0.97      0.71      0.82     55910\n",
            "           9       0.93      0.84      0.88     47029\n",
            "\n",
            "    accuracy                           0.90    420000\n",
            "   macro avg       0.90      0.91      0.90    420000\n",
            "weighted avg       0.91      0.90      0.90    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.944 Loss: 0.189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     43536\n",
            "           1       0.89      0.99      0.94     44152\n",
            "           2       0.93      0.92      0.92     44859\n",
            "           3       0.93      0.89      0.91     45428\n",
            "           4       0.84      0.97      0.90     36511\n",
            "           5       0.74      0.97      0.84     29294\n",
            "           6       0.93      0.96      0.95     40049\n",
            "           7       0.90      0.97      0.93     41142\n",
            "           8       0.97      0.71      0.82     56971\n",
            "           9       0.93      0.84      0.88     48058\n",
            "\n",
            "    accuracy                           0.90    430000\n",
            "   macro avg       0.90      0.92      0.90    430000\n",
            "weighted avg       0.91      0.90      0.90    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.939 Loss: 0.191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     44532\n",
            "           1       0.89      0.99      0.94     45246\n",
            "           2       0.93      0.92      0.92     45921\n",
            "           3       0.93      0.89      0.91     46526\n",
            "           4       0.84      0.97      0.90     37426\n",
            "           5       0.74      0.97      0.84     30080\n",
            "           6       0.93      0.96      0.95     41016\n",
            "           7       0.90      0.97      0.93     42112\n",
            "           8       0.97      0.71      0.82     58013\n",
            "           9       0.93      0.84      0.88     49128\n",
            "\n",
            "    accuracy                           0.91    440000\n",
            "   macro avg       0.90      0.92      0.91    440000\n",
            "weighted avg       0.91      0.91      0.90    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.934 Loss: 0.204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     45542\n",
            "           1       0.89      0.99      0.94     46304\n",
            "           2       0.93      0.92      0.92     46955\n",
            "           3       0.93      0.89      0.91     47622\n",
            "           4       0.84      0.97      0.90     38329\n",
            "           5       0.75      0.97      0.84     30870\n",
            "           6       0.94      0.96      0.95     41973\n",
            "           7       0.90      0.97      0.93     43071\n",
            "           8       0.97      0.72      0.82     59127\n",
            "           9       0.93      0.84      0.88     50207\n",
            "\n",
            "    accuracy                           0.91    450000\n",
            "   macro avg       0.90      0.92      0.91    450000\n",
            "weighted avg       0.91      0.91      0.91    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.938 Loss: 0.196\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     46554\n",
            "           1       0.90      0.99      0.94     47394\n",
            "           2       0.93      0.92      0.92     47972\n",
            "           3       0.93      0.88      0.91     48753\n",
            "           4       0.84      0.97      0.90     39242\n",
            "           5       0.75      0.97      0.84     31635\n",
            "           6       0.94      0.96      0.95     42920\n",
            "           7       0.90      0.97      0.93     44027\n",
            "           8       0.97      0.72      0.82     60198\n",
            "           9       0.93      0.84      0.89     51305\n",
            "\n",
            "    accuracy                           0.91    460000\n",
            "   macro avg       0.91      0.92      0.91    460000\n",
            "weighted avg       0.91      0.91      0.91    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.943 Loss: 0.184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     47551\n",
            "           1       0.90      0.99      0.94     48490\n",
            "           2       0.93      0.92      0.92     49014\n",
            "           3       0.93      0.88      0.91     49842\n",
            "           4       0.84      0.97      0.90     40169\n",
            "           5       0.75      0.97      0.85     32435\n",
            "           6       0.94      0.96      0.95     43877\n",
            "           7       0.90      0.97      0.93     44994\n",
            "           8       0.97      0.72      0.83     61251\n",
            "           9       0.93      0.84      0.89     52377\n",
            "\n",
            "    accuracy                           0.91    470000\n",
            "   macro avg       0.91      0.92      0.91    470000\n",
            "weighted avg       0.91      0.91      0.91    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.935 Loss: 0.199\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     48551\n",
            "           1       0.90      0.99      0.94     49560\n",
            "           2       0.93      0.92      0.92     50029\n",
            "           3       0.93      0.88      0.91     50959\n",
            "           4       0.84      0.97      0.90     41073\n",
            "           5       0.75      0.97      0.85     33238\n",
            "           6       0.94      0.96      0.95     44817\n",
            "           7       0.90      0.97      0.93     45966\n",
            "           8       0.97      0.72      0.83     62344\n",
            "           9       0.93      0.85      0.89     53463\n",
            "\n",
            "    accuracy                           0.91    480000\n",
            "   macro avg       0.91      0.92      0.91    480000\n",
            "weighted avg       0.91      0.91      0.91    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.942 Loss: 0.183\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     49558\n",
            "           1       0.90      0.99      0.94     50663\n",
            "           2       0.93      0.92      0.92     51072\n",
            "           3       0.93      0.88      0.91     52056\n",
            "           4       0.84      0.97      0.90     41993\n",
            "           5       0.76      0.97      0.85     34021\n",
            "           6       0.94      0.96      0.95     45781\n",
            "           7       0.90      0.97      0.93     46931\n",
            "           8       0.97      0.73      0.83     63379\n",
            "           9       0.93      0.85      0.89     54546\n",
            "\n",
            "    accuracy                           0.91    490000\n",
            "   macro avg       0.91      0.92      0.91    490000\n",
            "weighted avg       0.92      0.91      0.91    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.940 Loss: 0.189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     50553\n",
            "           1       0.90      0.99      0.94     51756\n",
            "           2       0.93      0.92      0.92     52119\n",
            "           3       0.93      0.88      0.91     53190\n",
            "           4       0.84      0.97      0.90     42923\n",
            "           5       0.76      0.97      0.85     34800\n",
            "           6       0.94      0.96      0.95     46736\n",
            "           7       0.90      0.97      0.93     47896\n",
            "           8       0.97      0.73      0.83     64428\n",
            "           9       0.93      0.85      0.89     55599\n",
            "\n",
            "    accuracy                           0.91    500000\n",
            "   macro avg       0.91      0.92      0.91    500000\n",
            "weighted avg       0.92      0.91      0.91    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.938 Loss: 0.196\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     51563\n",
            "           1       0.90      0.99      0.94     52827\n",
            "           2       0.93      0.92      0.93     53148\n",
            "           3       0.93      0.88      0.91     54301\n",
            "           4       0.85      0.97      0.90     43846\n",
            "           5       0.76      0.97      0.85     35596\n",
            "           6       0.94      0.96      0.95     47694\n",
            "           7       0.90      0.97      0.93     48860\n",
            "           8       0.97      0.73      0.83     65508\n",
            "           9       0.93      0.85      0.89     56657\n",
            "\n",
            "    accuracy                           0.91    510000\n",
            "   macro avg       0.91      0.92      0.91    510000\n",
            "weighted avg       0.92      0.91      0.91    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.941 Loss: 0.184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     52568\n",
            "           1       0.90      0.99      0.94     53929\n",
            "           2       0.93      0.92      0.93     54194\n",
            "           3       0.93      0.88      0.91     55397\n",
            "           4       0.85      0.97      0.90     44776\n",
            "           5       0.76      0.97      0.85     36376\n",
            "           6       0.94      0.96      0.95     48655\n",
            "           7       0.90      0.97      0.93     49818\n",
            "           8       0.97      0.73      0.83     66576\n",
            "           9       0.93      0.85      0.89     57711\n",
            "\n",
            "    accuracy                           0.91    520000\n",
            "   macro avg       0.91      0.92      0.91    520000\n",
            "weighted avg       0.92      0.91      0.91    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.941 Loss: 0.189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     53568\n",
            "           1       0.90      0.99      0.94     55009\n",
            "           2       0.93      0.92      0.93     55257\n",
            "           3       0.93      0.88      0.91     56510\n",
            "           4       0.85      0.97      0.90     45702\n",
            "           5       0.76      0.97      0.85     37170\n",
            "           6       0.94      0.96      0.95     49623\n",
            "           7       0.90      0.97      0.93     50764\n",
            "           8       0.97      0.74      0.84     67619\n",
            "           9       0.93      0.85      0.89     58778\n",
            "\n",
            "    accuracy                           0.91    530000\n",
            "   macro avg       0.91      0.92      0.91    530000\n",
            "weighted avg       0.92      0.91      0.91    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.937 Loss: 0.193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     54586\n",
            "           1       0.90      0.99      0.94     56070\n",
            "           2       0.93      0.92      0.93     56286\n",
            "           3       0.93      0.88      0.91     57606\n",
            "           4       0.85      0.97      0.90     46600\n",
            "           5       0.76      0.97      0.86     37983\n",
            "           6       0.94      0.96      0.95     50568\n",
            "           7       0.90      0.97      0.93     51735\n",
            "           8       0.97      0.74      0.84     68702\n",
            "           9       0.93      0.85      0.89     59864\n",
            "\n",
            "    accuracy                           0.91    540000\n",
            "   macro avg       0.91      0.92      0.91    540000\n",
            "weighted avg       0.92      0.91      0.91    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.945 Loss: 0.176\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     55600\n",
            "           1       0.90      0.99      0.94     57186\n",
            "           2       0.93      0.92      0.93     57320\n",
            "           3       0.93      0.88      0.91     58709\n",
            "           4       0.85      0.97      0.90     47523\n",
            "           5       0.77      0.97      0.86     38771\n",
            "           6       0.94      0.96      0.95     51520\n",
            "           7       0.90      0.97      0.93     52706\n",
            "           8       0.97      0.74      0.84     69726\n",
            "           9       0.94      0.85      0.89     60939\n",
            "\n",
            "    accuracy                           0.91    550000\n",
            "   macro avg       0.91      0.92      0.91    550000\n",
            "weighted avg       0.92      0.91      0.91    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.946 Loss: 0.176\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     56596\n",
            "           1       0.90      0.99      0.94     58287\n",
            "           2       0.93      0.92      0.93     58375\n",
            "           3       0.94      0.88      0.91     59826\n",
            "           4       0.85      0.97      0.91     48466\n",
            "           5       0.77      0.97      0.86     39582\n",
            "           6       0.94      0.96      0.95     52473\n",
            "           7       0.90      0.97      0.93     53696\n",
            "           8       0.96      0.74      0.84     70721\n",
            "           9       0.94      0.85      0.89     61978\n",
            "\n",
            "    accuracy                           0.91    560000\n",
            "   macro avg       0.91      0.92      0.91    560000\n",
            "weighted avg       0.92      0.91      0.91    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.942 Loss: 0.181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     57599\n",
            "           1       0.91      0.99      0.94     59352\n",
            "           2       0.93      0.92      0.93     59392\n",
            "           3       0.94      0.88      0.91     60929\n",
            "           4       0.85      0.97      0.91     49389\n",
            "           5       0.77      0.97      0.86     40403\n",
            "           6       0.94      0.96      0.95     53433\n",
            "           7       0.90      0.97      0.93     54687\n",
            "           8       0.96      0.75      0.84     71785\n",
            "           9       0.94      0.85      0.89     63031\n",
            "\n",
            "    accuracy                           0.91    570000\n",
            "   macro avg       0.91      0.92      0.91    570000\n",
            "weighted avg       0.92      0.91      0.91    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.945 Loss: 0.178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     58614\n",
            "           1       0.91      0.99      0.94     60455\n",
            "           2       0.93      0.92      0.93     60435\n",
            "           3       0.94      0.88      0.91     62044\n",
            "           4       0.85      0.97      0.91     50327\n",
            "           5       0.77      0.97      0.86     41181\n",
            "           6       0.94      0.96      0.95     54371\n",
            "           7       0.90      0.97      0.93     55659\n",
            "           8       0.96      0.75      0.84     72819\n",
            "           9       0.94      0.85      0.89     64095\n",
            "\n",
            "    accuracy                           0.91    580000\n",
            "   macro avg       0.91      0.92      0.91    580000\n",
            "weighted avg       0.92      0.91      0.91    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.947 Loss: 0.171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     59614\n",
            "           1       0.91      0.99      0.95     61557\n",
            "           2       0.93      0.92      0.93     61484\n",
            "           3       0.94      0.88      0.91     63136\n",
            "           4       0.85      0.97      0.91     51266\n",
            "           5       0.77      0.97      0.86     41985\n",
            "           6       0.94      0.96      0.95     55327\n",
            "           7       0.90      0.97      0.93     56637\n",
            "           8       0.96      0.75      0.84     73853\n",
            "           9       0.94      0.86      0.89     65141\n",
            "\n",
            "    accuracy                           0.91    590000\n",
            "   macro avg       0.91      0.92      0.91    590000\n",
            "weighted avg       0.92      0.91      0.91    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.942 Loss: 0.180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     60612\n",
            "           1       0.91      0.99      0.95     62629\n",
            "           2       0.93      0.92      0.93     62519\n",
            "           3       0.94      0.88      0.91     64232\n",
            "           4       0.86      0.97      0.91     52167\n",
            "           5       0.78      0.97      0.86     42826\n",
            "           6       0.94      0.96      0.95     56270\n",
            "           7       0.90      0.97      0.93     57631\n",
            "           8       0.96      0.75      0.85     74906\n",
            "           9       0.94      0.86      0.89     66208\n",
            "\n",
            "    accuracy                           0.92    600000\n",
            "   macro avg       0.91      0.92      0.91    600000\n",
            "weighted avg       0.92      0.92      0.91    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.949 Loss: 0.166\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     61621\n",
            "           1       0.91      0.99      0.95     63734\n",
            "           2       0.93      0.92      0.93     63550\n",
            "           3       0.94      0.88      0.91     65341\n",
            "           4       0.86      0.97      0.91     53107\n",
            "           5       0.78      0.97      0.86     43621\n",
            "           6       0.94      0.96      0.95     57230\n",
            "           7       0.90      0.97      0.93     58623\n",
            "           8       0.96      0.75      0.85     75907\n",
            "           9       0.94      0.86      0.90     67266\n",
            "\n",
            "    accuracy                           0.92    610000\n",
            "   macro avg       0.91      0.92      0.92    610000\n",
            "weighted avg       0.92      0.92      0.91    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.945 Loss: 0.172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     62622\n",
            "           1       0.91      0.99      0.95     64833\n",
            "           2       0.93      0.92      0.93     64595\n",
            "           3       0.94      0.88      0.91     66488\n",
            "           4       0.86      0.97      0.91     54044\n",
            "           5       0.78      0.97      0.86     44405\n",
            "           6       0.94      0.96      0.95     58191\n",
            "           7       0.90      0.97      0.94     59610\n",
            "           8       0.96      0.76      0.85     76909\n",
            "           9       0.94      0.86      0.90     68303\n",
            "\n",
            "    accuracy                           0.92    620000\n",
            "   macro avg       0.91      0.92      0.92    620000\n",
            "weighted avg       0.92      0.92      0.92    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.940 Loss: 0.180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     63634\n",
            "           1       0.91      0.99      0.95     65910\n",
            "           2       0.93      0.93      0.93     65614\n",
            "           3       0.94      0.88      0.91     67569\n",
            "           4       0.86      0.97      0.91     54929\n",
            "           5       0.78      0.97      0.86     45253\n",
            "           6       0.94      0.96      0.95     59135\n",
            "           7       0.91      0.97      0.94     60597\n",
            "           8       0.96      0.76      0.85     77953\n",
            "           9       0.94      0.86      0.90     69406\n",
            "\n",
            "    accuracy                           0.92    630000\n",
            "   macro avg       0.92      0.92      0.92    630000\n",
            "weighted avg       0.92      0.92      0.92    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.948 Loss: 0.167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     64637\n",
            "           1       0.91      0.99      0.95     67008\n",
            "           2       0.93      0.93      0.93     66648\n",
            "           3       0.94      0.88      0.91     68647\n",
            "           4       0.86      0.97      0.91     55862\n",
            "           5       0.78      0.97      0.87     46063\n",
            "           6       0.94      0.96      0.95     60094\n",
            "           7       0.91      0.97      0.94     61590\n",
            "           8       0.96      0.76      0.85     78987\n",
            "           9       0.94      0.86      0.90     70464\n",
            "\n",
            "    accuracy                           0.92    640000\n",
            "   macro avg       0.92      0.92      0.92    640000\n",
            "weighted avg       0.92      0.92      0.92    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.948 Loss: 0.164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     65633\n",
            "           1       0.91      0.99      0.95     68107\n",
            "           2       0.93      0.93      0.93     67693\n",
            "           3       0.94      0.88      0.91     69740\n",
            "           4       0.86      0.97      0.91     56795\n",
            "           5       0.78      0.97      0.87     46866\n",
            "           6       0.94      0.96      0.95     61048\n",
            "           7       0.91      0.97      0.94     62598\n",
            "           8       0.96      0.76      0.85     80013\n",
            "           9       0.94      0.86      0.90     71507\n",
            "\n",
            "    accuracy                           0.92    650000\n",
            "   macro avg       0.92      0.92      0.92    650000\n",
            "weighted avg       0.92      0.92      0.92    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.943 Loss: 0.172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     66643\n",
            "           1       0.91      0.99      0.95     69189\n",
            "           2       0.93      0.93      0.93     68712\n",
            "           3       0.94      0.88      0.91     70841\n",
            "           4       0.86      0.97      0.91     57722\n",
            "           5       0.78      0.97      0.87     47678\n",
            "           6       0.94      0.96      0.95     61996\n",
            "           7       0.91      0.97      0.94     63596\n",
            "           8       0.96      0.76      0.85     81064\n",
            "           9       0.94      0.86      0.90     72559\n",
            "\n",
            "    accuracy                           0.92    660000\n",
            "   macro avg       0.92      0.92      0.92    660000\n",
            "weighted avg       0.92      0.92      0.92    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.948 Loss: 0.165\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     67649\n",
            "           1       0.91      0.99      0.95     70297\n",
            "           2       0.93      0.93      0.93     69737\n",
            "           3       0.94      0.89      0.91     71922\n",
            "           4       0.86      0.97      0.91     58649\n",
            "           5       0.79      0.97      0.87     48481\n",
            "           6       0.94      0.96      0.95     62954\n",
            "           7       0.91      0.97      0.94     64594\n",
            "           8       0.96      0.77      0.85     82087\n",
            "           9       0.94      0.86      0.90     73630\n",
            "\n",
            "    accuracy                           0.92    670000\n",
            "   macro avg       0.92      0.92      0.92    670000\n",
            "weighted avg       0.92      0.92      0.92    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.946 Loss: 0.171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     68647\n",
            "           1       0.91      0.99      0.95     71381\n",
            "           2       0.94      0.93      0.93     70789\n",
            "           3       0.94      0.89      0.91     73028\n",
            "           4       0.86      0.97      0.91     59574\n",
            "           5       0.79      0.97      0.87     49286\n",
            "           6       0.94      0.96      0.95     63911\n",
            "           7       0.91      0.97      0.94     65591\n",
            "           8       0.96      0.77      0.85     83121\n",
            "           9       0.94      0.86      0.90     74672\n",
            "\n",
            "    accuracy                           0.92    680000\n",
            "   macro avg       0.92      0.92      0.92    680000\n",
            "weighted avg       0.92      0.92      0.92    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.944 Loss: 0.173\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     69653\n",
            "           1       0.91      0.99      0.95     72456\n",
            "           2       0.94      0.93      0.93     71822\n",
            "           3       0.94      0.89      0.91     74127\n",
            "           4       0.86      0.97      0.91     60495\n",
            "           5       0.79      0.97      0.87     50105\n",
            "           6       0.94      0.96      0.95     64866\n",
            "           7       0.91      0.97      0.94     66589\n",
            "           8       0.96      0.77      0.86     84176\n",
            "           9       0.94      0.86      0.90     75711\n",
            "\n",
            "    accuracy                           0.92    690000\n",
            "   macro avg       0.92      0.93      0.92    690000\n",
            "weighted avg       0.92      0.92      0.92    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.946 Loss: 0.172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     70658\n",
            "           1       0.91      0.99      0.95     73554\n",
            "           2       0.94      0.93      0.93     72869\n",
            "           3       0.94      0.89      0.91     75235\n",
            "           4       0.86      0.97      0.91     61431\n",
            "           5       0.79      0.97      0.87     50884\n",
            "           6       0.94      0.96      0.95     65816\n",
            "           7       0.91      0.97      0.94     67578\n",
            "           8       0.96      0.77      0.86     85211\n",
            "           9       0.94      0.86      0.90     76764\n",
            "\n",
            "    accuracy                           0.92    700000\n",
            "   macro avg       0.92      0.93      0.92    700000\n",
            "weighted avg       0.92      0.92      0.92    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.948 Loss: 0.168\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     71649\n",
            "           1       0.91      0.99      0.95     74655\n",
            "           2       0.94      0.93      0.93     73907\n",
            "           3       0.94      0.89      0.91     76350\n",
            "           4       0.86      0.97      0.91     62373\n",
            "           5       0.79      0.97      0.87     51678\n",
            "           6       0.94      0.96      0.95     66779\n",
            "           7       0.91      0.97      0.94     68577\n",
            "           8       0.96      0.77      0.86     86234\n",
            "           9       0.94      0.86      0.90     77798\n",
            "\n",
            "    accuracy                           0.92    710000\n",
            "   macro avg       0.92      0.93      0.92    710000\n",
            "weighted avg       0.92      0.92      0.92    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.945 Loss: 0.170\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     72662\n",
            "           1       0.91      0.99      0.95     75749\n",
            "           2       0.94      0.93      0.93     74930\n",
            "           3       0.94      0.89      0.91     77458\n",
            "           4       0.87      0.97      0.91     63299\n",
            "           5       0.79      0.97      0.87     52485\n",
            "           6       0.94      0.96      0.95     67724\n",
            "           7       0.91      0.97      0.94     69573\n",
            "           8       0.96      0.77      0.86     87267\n",
            "           9       0.94      0.87      0.90     78853\n",
            "\n",
            "    accuracy                           0.92    720000\n",
            "   macro avg       0.92      0.93      0.92    720000\n",
            "weighted avg       0.92      0.92      0.92    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.948 Loss: 0.162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     73667\n",
            "           1       0.92      0.99      0.95     76855\n",
            "           2       0.94      0.93      0.93     75962\n",
            "           3       0.94      0.89      0.91     78541\n",
            "           4       0.87      0.97      0.91     64220\n",
            "           5       0.79      0.97      0.87     53301\n",
            "           6       0.94      0.96      0.95     68686\n",
            "           7       0.91      0.97      0.94     70565\n",
            "           8       0.96      0.78      0.86     88284\n",
            "           9       0.94      0.87      0.90     79919\n",
            "\n",
            "    accuracy                           0.92    730000\n",
            "   macro avg       0.92      0.93      0.92    730000\n",
            "weighted avg       0.92      0.92      0.92    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.948 Loss: 0.163\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     74662\n",
            "           1       0.92      0.99      0.95     77961\n",
            "           2       0.94      0.93      0.93     77008\n",
            "           3       0.94      0.89      0.91     79633\n",
            "           4       0.87      0.97      0.91     65129\n",
            "           5       0.79      0.97      0.87     54140\n",
            "           6       0.94      0.96      0.95     69654\n",
            "           7       0.91      0.97      0.94     71558\n",
            "           8       0.96      0.78      0.86     89266\n",
            "           9       0.94      0.87      0.90     80989\n",
            "\n",
            "    accuracy                           0.92    740000\n",
            "   macro avg       0.92      0.93      0.92    740000\n",
            "weighted avg       0.93      0.92      0.92    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.945 Loss: 0.170\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     75669\n",
            "           1       0.92      0.99      0.95     79052\n",
            "           2       0.94      0.93      0.93     78028\n",
            "           3       0.94      0.89      0.91     80737\n",
            "           4       0.87      0.97      0.91     66040\n",
            "           5       0.80      0.97      0.87     54982\n",
            "           6       0.94      0.96      0.95     70594\n",
            "           7       0.91      0.97      0.94     72550\n",
            "           8       0.96      0.78      0.86     90302\n",
            "           9       0.94      0.87      0.90     82046\n",
            "\n",
            "    accuracy                           0.92    750000\n",
            "   macro avg       0.92      0.93      0.92    750000\n",
            "weighted avg       0.93      0.92      0.92    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.947 Loss: 0.166\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     76679\n",
            "           1       0.92      0.99      0.95     80150\n",
            "           2       0.94      0.93      0.93     79074\n",
            "           3       0.94      0.89      0.91     81816\n",
            "           4       0.87      0.97      0.91     66948\n",
            "           5       0.80      0.97      0.87     55792\n",
            "           6       0.94      0.96      0.95     71552\n",
            "           7       0.91      0.97      0.94     73544\n",
            "           8       0.96      0.78      0.86     91325\n",
            "           9       0.94      0.87      0.90     83120\n",
            "\n",
            "    accuracy                           0.92    760000\n",
            "   macro avg       0.92      0.93      0.92    760000\n",
            "weighted avg       0.93      0.92      0.92    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.946 Loss: 0.171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     77673\n",
            "           1       0.92      0.99      0.95     81248\n",
            "           2       0.94      0.93      0.93     80140\n",
            "           3       0.94      0.89      0.91     82914\n",
            "           4       0.87      0.97      0.91     67857\n",
            "           5       0.80      0.97      0.88     56601\n",
            "           6       0.94      0.96      0.95     72520\n",
            "           7       0.91      0.97      0.94     74521\n",
            "           8       0.96      0.78      0.86     92339\n",
            "           9       0.94      0.87      0.90     84187\n",
            "\n",
            "    accuracy                           0.92    770000\n",
            "   macro avg       0.92      0.93      0.92    770000\n",
            "weighted avg       0.93      0.92      0.92    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.944 Loss: 0.171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     78668\n",
            "           1       0.92      0.99      0.95     82332\n",
            "           2       0.94      0.93      0.93     81172\n",
            "           3       0.94      0.89      0.91     84005\n",
            "           4       0.87      0.97      0.91     68776\n",
            "           5       0.80      0.97      0.88     57420\n",
            "           6       0.94      0.96      0.95     73475\n",
            "           7       0.91      0.97      0.94     75512\n",
            "           8       0.96      0.78      0.86     93399\n",
            "           9       0.94      0.87      0.90     85241\n",
            "\n",
            "    accuracy                           0.92    780000\n",
            "   macro avg       0.92      0.93      0.92    780000\n",
            "weighted avg       0.93      0.92      0.92    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.949 Loss: 0.165\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     79664\n",
            "           1       0.92      0.99      0.95     83444\n",
            "           2       0.94      0.93      0.93     82213\n",
            "           3       0.95      0.89      0.91     85127\n",
            "           4       0.87      0.97      0.92     69715\n",
            "           5       0.80      0.97      0.88     58200\n",
            "           6       0.94      0.96      0.95     74437\n",
            "           7       0.91      0.97      0.94     76504\n",
            "           8       0.96      0.78      0.86     94410\n",
            "           9       0.94      0.87      0.90     86286\n",
            "\n",
            "    accuracy                           0.92    790000\n",
            "   macro avg       0.92      0.93      0.92    790000\n",
            "weighted avg       0.93      0.92      0.92    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.949 Loss: 0.164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     80658\n",
            "           1       0.92      0.99      0.95     84547\n",
            "           2       0.94      0.93      0.93     83271\n",
            "           3       0.95      0.89      0.91     86236\n",
            "           4       0.87      0.97      0.92     70643\n",
            "           5       0.80      0.97      0.88     59003\n",
            "           6       0.95      0.96      0.95     75404\n",
            "           7       0.91      0.97      0.94     77506\n",
            "           8       0.96      0.79      0.87     95413\n",
            "           9       0.94      0.87      0.90     87319\n",
            "\n",
            "    accuracy                           0.92    800000\n",
            "   macro avg       0.92      0.93      0.92    800000\n",
            "weighted avg       0.93      0.92      0.92    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.948 Loss: 0.164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     81653\n",
            "           1       0.92      0.99      0.95     85640\n",
            "           2       0.94      0.93      0.93     84299\n",
            "           3       0.95      0.89      0.91     87335\n",
            "           4       0.87      0.97      0.92     71579\n",
            "           5       0.80      0.97      0.88     59830\n",
            "           6       0.95      0.96      0.95     76360\n",
            "           7       0.91      0.97      0.94     78506\n",
            "           8       0.96      0.79      0.87     96442\n",
            "           9       0.94      0.87      0.90     88356\n",
            "\n",
            "    accuracy                           0.92    810000\n",
            "   macro avg       0.92      0.93      0.92    810000\n",
            "weighted avg       0.93      0.92      0.92    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.950 Loss: 0.162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     82651\n",
            "           1       0.92      0.99      0.95     86747\n",
            "           2       0.94      0.93      0.93     85338\n",
            "           3       0.95      0.89      0.92     88429\n",
            "           4       0.87      0.97      0.92     72526\n",
            "           5       0.80      0.97      0.88     60610\n",
            "           6       0.95      0.96      0.95     77322\n",
            "           7       0.91      0.97      0.94     79510\n",
            "           8       0.96      0.79      0.87     97465\n",
            "           9       0.94      0.87      0.90     89402\n",
            "\n",
            "    accuracy                           0.92    820000\n",
            "   macro avg       0.92      0.93      0.92    820000\n",
            "weighted avg       0.93      0.92      0.92    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.951 Loss: 0.163\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     83645\n",
            "           1       0.92      0.99      0.95     87835\n",
            "           2       0.94      0.93      0.93     86394\n",
            "           3       0.95      0.89      0.92     89512\n",
            "           4       0.87      0.97      0.92     73463\n",
            "           5       0.80      0.97      0.88     61431\n",
            "           6       0.95      0.96      0.95     78274\n",
            "           7       0.91      0.97      0.94     80515\n",
            "           8       0.96      0.79      0.87     98482\n",
            "           9       0.94      0.87      0.90     90449\n",
            "\n",
            "    accuracy                           0.92    830000\n",
            "   macro avg       0.92      0.93      0.92    830000\n",
            "weighted avg       0.93      0.92      0.92    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.947 Loss: 0.169\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     84640\n",
            "           1       0.92      0.99      0.95     88903\n",
            "           2       0.94      0.93      0.94     87423\n",
            "           3       0.95      0.89      0.92     90602\n",
            "           4       0.87      0.97      0.92     74396\n",
            "           5       0.80      0.97      0.88     62273\n",
            "           6       0.95      0.96      0.95     79221\n",
            "           7       0.91      0.97      0.94     81515\n",
            "           8       0.96      0.79      0.87     99549\n",
            "           9       0.94      0.87      0.91     91478\n",
            "\n",
            "    accuracy                           0.92    840000\n",
            "   macro avg       0.92      0.93      0.92    840000\n",
            "weighted avg       0.93      0.92      0.92    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.950 Loss: 0.161\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     85638\n",
            "           1       0.92      0.99      0.95     90000\n",
            "           2       0.94      0.93      0.94     88456\n",
            "           3       0.95      0.89      0.92     91713\n",
            "           4       0.87      0.97      0.92     75343\n",
            "           5       0.81      0.97      0.88     63093\n",
            "           6       0.95      0.96      0.95     80170\n",
            "           7       0.91      0.97      0.94     82500\n",
            "           8       0.96      0.79      0.87    100566\n",
            "           9       0.94      0.87      0.91     92521\n",
            "\n",
            "    accuracy                           0.92    850000\n",
            "   macro avg       0.92      0.93      0.92    850000\n",
            "weighted avg       0.93      0.92      0.92    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.948 Loss: 0.165\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     86629\n",
            "           1       0.92      0.99      0.95     91101\n",
            "           2       0.94      0.93      0.94     89506\n",
            "           3       0.95      0.89      0.92     92827\n",
            "           4       0.87      0.97      0.92     76269\n",
            "           5       0.81      0.97      0.88     63902\n",
            "           6       0.95      0.96      0.95     81131\n",
            "           7       0.91      0.97      0.94     83478\n",
            "           8       0.96      0.79      0.87    101583\n",
            "           9       0.94      0.87      0.91     93574\n",
            "\n",
            "    accuracy                           0.92    860000\n",
            "   macro avg       0.92      0.93      0.92    860000\n",
            "weighted avg       0.93      0.92      0.92    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.945 Loss: 0.169\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     87625\n",
            "           1       0.92      0.99      0.95     92195\n",
            "           2       0.94      0.93      0.94     90535\n",
            "           3       0.95      0.89      0.92     93949\n",
            "           4       0.87      0.97      0.92     77196\n",
            "           5       0.81      0.97      0.88     64725\n",
            "           6       0.95      0.96      0.95     82090\n",
            "           7       0.91      0.97      0.94     84467\n",
            "           8       0.96      0.79      0.87    102595\n",
            "           9       0.94      0.87      0.91     94623\n",
            "\n",
            "    accuracy                           0.92    870000\n",
            "   macro avg       0.92      0.93      0.92    870000\n",
            "weighted avg       0.93      0.92      0.92    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.952 Loss: 0.156\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     88635\n",
            "           1       0.92      0.99      0.95     93323\n",
            "           2       0.94      0.93      0.94     91573\n",
            "           3       0.95      0.89      0.92     95044\n",
            "           4       0.87      0.97      0.92     78131\n",
            "           5       0.81      0.97      0.88     65555\n",
            "           6       0.95      0.96      0.95     83040\n",
            "           7       0.91      0.97      0.94     85460\n",
            "           8       0.96      0.80      0.87    103572\n",
            "           9       0.94      0.87      0.91     95667\n",
            "\n",
            "    accuracy                           0.93    880000\n",
            "   macro avg       0.92      0.93      0.92    880000\n",
            "weighted avg       0.93      0.93      0.92    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.949 Loss: 0.162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97     89633\n",
            "           1       0.92      0.99      0.95     94430\n",
            "           2       0.94      0.93      0.94     92622\n",
            "           3       0.95      0.89      0.92     96149\n",
            "           4       0.87      0.97      0.92     79055\n",
            "           5       0.81      0.97      0.88     66368\n",
            "           6       0.95      0.96      0.95     84003\n",
            "           7       0.91      0.97      0.94     86460\n",
            "           8       0.96      0.80      0.87    104564\n",
            "           9       0.94      0.87      0.91     96716\n",
            "\n",
            "    accuracy                           0.93    890000\n",
            "   macro avg       0.92      0.93      0.93    890000\n",
            "weighted avg       0.93      0.93      0.93    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.945 Loss: 0.167\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     90629\n",
            "           1       0.92      0.99      0.95     95513\n",
            "           2       0.94      0.93      0.94     93647\n",
            "           3       0.95      0.89      0.92     97239\n",
            "           4       0.88      0.97      0.92     79974\n",
            "           5       0.81      0.97      0.88     67201\n",
            "           6       0.95      0.96      0.95     84951\n",
            "           7       0.91      0.97      0.94     87462\n",
            "           8       0.96      0.80      0.87    105619\n",
            "           9       0.94      0.87      0.91     97765\n",
            "\n",
            "    accuracy                           0.93    900000\n",
            "   macro avg       0.92      0.93      0.93    900000\n",
            "weighted avg       0.93      0.93      0.93    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.drop1 = nn.Dropout(p=0.7)\n",
        "    self.drop2 = nn.Dropout(p=0.5)\n",
        "    self.linear2 = nn.Linear(100, 100)\n",
        "    self.batch = nn.BatchNorm1d(100)\n",
        "    self.act = nn.Sigmoid()\n",
        "    self.linear3 = nn.Linear(100,10)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop1(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop2(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear3(out)\n",
        "    out = self.act(out)\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "22yshUYGgXtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "xNVSjSVYgZt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_UHXvl3gcXM",
        "outputId": "c8447008-2379-4b88-b374-4a0dc01770bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.788 Loss: 2.021\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       937\n",
            "           1       0.93      0.96      0.95      1095\n",
            "           2       0.80      0.79      0.79      1038\n",
            "           3       0.94      0.48      0.64      1973\n",
            "           4       0.81      0.86      0.84       931\n",
            "           5       0.23      0.99      0.37       206\n",
            "           6       0.91      0.90      0.91       967\n",
            "           7       0.75      0.96      0.85       804\n",
            "           8       0.62      0.82      0.71       743\n",
            "           9       0.89      0.69      0.78      1306\n",
            "\n",
            "    accuracy                           0.79     10000\n",
            "   macro avg       0.78      0.84      0.78     10000\n",
            "weighted avg       0.85      0.79      0.80     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.825 Loss: 1.866\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      1836\n",
            "           1       0.93      0.96      0.95      2185\n",
            "           2       0.83      0.82      0.82      2090\n",
            "           3       0.93      0.54      0.68      3500\n",
            "           4       0.81      0.88      0.84      1802\n",
            "           5       0.24      0.99      0.39       442\n",
            "           6       0.93      0.90      0.92      1972\n",
            "           7       0.78      0.97      0.86      1660\n",
            "           8       0.75      0.74      0.75      1969\n",
            "           9       0.90      0.71      0.79      2544\n",
            "\n",
            "    accuracy                           0.81     20000\n",
            "   macro avg       0.80      0.85      0.79     20000\n",
            "weighted avg       0.86      0.81      0.81     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.834 Loss: 1.804\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94      2812\n",
            "           1       0.91      0.97      0.94      3218\n",
            "           2       0.84      0.83      0.84      3117\n",
            "           3       0.93      0.58      0.71      4862\n",
            "           4       0.80      0.89      0.84      2635\n",
            "           5       0.27      0.98      0.42       736\n",
            "           6       0.93      0.92      0.92      2917\n",
            "           7       0.79      0.97      0.87      2518\n",
            "           8       0.81      0.69      0.75      3399\n",
            "           9       0.90      0.72      0.80      3786\n",
            "\n",
            "    accuracy                           0.82     30000\n",
            "   macro avg       0.81      0.85      0.80     30000\n",
            "weighted avg       0.86      0.82      0.82     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.850 Loss: 1.749\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94      3803\n",
            "           1       0.92      0.96      0.94      4334\n",
            "           2       0.85      0.84      0.85      4185\n",
            "           3       0.93      0.61      0.74      6121\n",
            "           4       0.78      0.90      0.84      3391\n",
            "           5       0.30      0.99      0.46      1090\n",
            "           6       0.93      0.92      0.93      3891\n",
            "           7       0.79      0.97      0.87      3374\n",
            "           8       0.84      0.69      0.76      4725\n",
            "           9       0.91      0.72      0.80      5086\n",
            "\n",
            "    accuracy                           0.82     40000\n",
            "   macro avg       0.82      0.86      0.81     40000\n",
            "weighted avg       0.87      0.82      0.83     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.835 Loss: 1.721\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94      4764\n",
            "           1       0.92      0.97      0.94      5412\n",
            "           2       0.86      0.84      0.85      5288\n",
            "           3       0.92      0.64      0.75      7261\n",
            "           4       0.74      0.91      0.81      4007\n",
            "           5       0.32      0.99      0.49      1453\n",
            "           6       0.94      0.92      0.93      4854\n",
            "           7       0.80      0.97      0.88      4243\n",
            "           8       0.86      0.68      0.76      6166\n",
            "           9       0.91      0.70      0.79      6552\n",
            "\n",
            "    accuracy                           0.83     50000\n",
            "   macro avg       0.82      0.86      0.82     50000\n",
            "weighted avg       0.87      0.83      0.83     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.846 Loss: 1.718\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95      5739\n",
            "           1       0.91      0.97      0.94      6450\n",
            "           2       0.87      0.85      0.86      6318\n",
            "           3       0.92      0.66      0.77      8433\n",
            "           4       0.73      0.91      0.81      4741\n",
            "           5       0.35      0.99      0.51      1869\n",
            "           6       0.94      0.93      0.93      5814\n",
            "           7       0.80      0.97      0.88      5094\n",
            "           8       0.88      0.67      0.76      7673\n",
            "           9       0.91      0.70      0.79      7869\n",
            "\n",
            "    accuracy                           0.83     60000\n",
            "   macro avg       0.82      0.86      0.82     60000\n",
            "weighted avg       0.87      0.83      0.84     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.851 Loss: 1.683\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95      6715\n",
            "           1       0.92      0.96      0.94      7574\n",
            "           2       0.87      0.85      0.86      7379\n",
            "           3       0.91      0.68      0.78      9477\n",
            "           4       0.72      0.91      0.80      5422\n",
            "           5       0.37      0.99      0.54      2318\n",
            "           6       0.94      0.93      0.93      6799\n",
            "           7       0.80      0.97      0.88      5971\n",
            "           8       0.89      0.67      0.76      9064\n",
            "           9       0.91      0.70      0.79      9281\n",
            "\n",
            "    accuracy                           0.83     70000\n",
            "   macro avg       0.83      0.86      0.82     70000\n",
            "weighted avg       0.87      0.83      0.84     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.839 Loss: 1.676\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95      7674\n",
            "           1       0.92      0.96      0.94      8679\n",
            "           2       0.87      0.86      0.87      8425\n",
            "           3       0.91      0.70      0.79     10574\n",
            "           4       0.69      0.91      0.79      5972\n",
            "           5       0.38      0.99      0.55      2747\n",
            "           6       0.94      0.93      0.93      7767\n",
            "           7       0.81      0.97      0.88      6850\n",
            "           8       0.90      0.66      0.76     10522\n",
            "           9       0.92      0.68      0.78     10790\n",
            "\n",
            "    accuracy                           0.83     80000\n",
            "   macro avg       0.83      0.86      0.82     80000\n",
            "weighted avg       0.87      0.83      0.84     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.825 Loss: 1.675\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95      8638\n",
            "           1       0.92      0.97      0.94      9711\n",
            "           2       0.88      0.86      0.87      9475\n",
            "           3       0.91      0.71      0.80     11620\n",
            "           4       0.66      0.91      0.77      6412\n",
            "           5       0.40      0.99      0.57      3214\n",
            "           6       0.94      0.93      0.93      8731\n",
            "           7       0.81      0.97      0.88      7713\n",
            "           8       0.90      0.66      0.76     12063\n",
            "           9       0.92      0.67      0.77     12423\n",
            "\n",
            "    accuracy                           0.83     90000\n",
            "   macro avg       0.83      0.86      0.82     90000\n",
            "weighted avg       0.87      0.83      0.84     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.832 Loss: 1.663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95      9622\n",
            "           1       0.92      0.96      0.94     10820\n",
            "           2       0.88      0.86      0.87     10522\n",
            "           3       0.91      0.72      0.81     12688\n",
            "           4       0.63      0.91      0.75      6821\n",
            "           5       0.41      0.99      0.58      3671\n",
            "           6       0.94      0.93      0.94      9693\n",
            "           7       0.81      0.97      0.88      8598\n",
            "           8       0.91      0.66      0.76     13484\n",
            "           9       0.92      0.66      0.77     14081\n",
            "\n",
            "    accuracy                           0.83    100000\n",
            "   macro avg       0.83      0.86      0.82    100000\n",
            "weighted avg       0.87      0.83      0.84    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.815 Loss: 1.660\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95     10563\n",
            "           1       0.92      0.97      0.94     11892\n",
            "           2       0.88      0.86      0.87     11547\n",
            "           3       0.91      0.73      0.81     13769\n",
            "           4       0.61      0.91      0.73      7177\n",
            "           5       0.41      0.99      0.58      4079\n",
            "           6       0.94      0.93      0.94     10658\n",
            "           7       0.81      0.97      0.88      9470\n",
            "           8       0.91      0.65      0.76     15074\n",
            "           9       0.92      0.65      0.76     15771\n",
            "\n",
            "    accuracy                           0.83    110000\n",
            "   macro avg       0.83      0.86      0.82    110000\n",
            "weighted avg       0.87      0.83      0.84    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.809 Loss: 1.660\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95     11520\n",
            "           1       0.92      0.97      0.94     12913\n",
            "           2       0.88      0.87      0.87     12538\n",
            "           3       0.90      0.74      0.82     14774\n",
            "           4       0.58      0.91      0.71      7509\n",
            "           5       0.42      0.99      0.59      4551\n",
            "           6       0.94      0.93      0.94     11624\n",
            "           7       0.81      0.97      0.88     10325\n",
            "           8       0.92      0.64      0.76     16745\n",
            "           9       0.92      0.64      0.75     17501\n",
            "\n",
            "    accuracy                           0.83    120000\n",
            "   macro avg       0.82      0.86      0.82    120000\n",
            "weighted avg       0.87      0.83      0.83    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.816 Loss: 1.644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     12491\n",
            "           1       0.92      0.97      0.94     14028\n",
            "           2       0.88      0.87      0.88     13566\n",
            "           3       0.90      0.75      0.82     15793\n",
            "           4       0.55      0.91      0.69      7748\n",
            "           5       0.43      0.99      0.60      5035\n",
            "           6       0.94      0.93      0.94     12577\n",
            "           7       0.81      0.97      0.88     11219\n",
            "           8       0.92      0.64      0.76     18243\n",
            "           9       0.92      0.63      0.74     19300\n",
            "\n",
            "    accuracy                           0.83    130000\n",
            "   macro avg       0.82      0.86      0.82    130000\n",
            "weighted avg       0.87      0.83      0.83    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.817 Loss: 1.644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     13431\n",
            "           1       0.92      0.97      0.94     15091\n",
            "           2       0.88      0.87      0.88     14551\n",
            "           3       0.90      0.76      0.82     16850\n",
            "           4       0.53      0.91      0.67      8062\n",
            "           5       0.44      0.99      0.61      5501\n",
            "           6       0.94      0.93      0.94     13555\n",
            "           7       0.82      0.97      0.89     12115\n",
            "           8       0.93      0.64      0.76     19795\n",
            "           9       0.92      0.62      0.74     21049\n",
            "\n",
            "    accuracy                           0.83    140000\n",
            "   macro avg       0.82      0.86      0.82    140000\n",
            "weighted avg       0.87      0.83      0.83    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.806 Loss: 1.648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     14375\n",
            "           1       0.92      0.97      0.94     16126\n",
            "           2       0.88      0.88      0.88     15530\n",
            "           3       0.90      0.76      0.83     17901\n",
            "           4       0.51      0.91      0.66      8335\n",
            "           5       0.44      0.99      0.61      5939\n",
            "           6       0.94      0.93      0.94     14501\n",
            "           7       0.82      0.97      0.89     13002\n",
            "           8       0.93      0.63      0.75     21479\n",
            "           9       0.92      0.61      0.74     22812\n",
            "\n",
            "    accuracy                           0.83    150000\n",
            "   macro avg       0.82      0.86      0.82    150000\n",
            "weighted avg       0.87      0.83      0.83    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.815 Loss: 1.641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     15313\n",
            "           1       0.92      0.97      0.94     17220\n",
            "           2       0.88      0.88      0.88     16552\n",
            "           3       0.90      0.77      0.83     18920\n",
            "           4       0.50      0.91      0.64      8595\n",
            "           5       0.45      0.99      0.62      6422\n",
            "           6       0.94      0.93      0.94     15440\n",
            "           7       0.82      0.97      0.89     13908\n",
            "           8       0.93      0.63      0.75     23067\n",
            "           9       0.92      0.61      0.73     24563\n",
            "\n",
            "    accuracy                           0.83    160000\n",
            "   macro avg       0.82      0.86      0.82    160000\n",
            "weighted avg       0.87      0.83      0.83    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.812 Loss: 1.641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     16219\n",
            "           1       0.92      0.97      0.94     18275\n",
            "           2       0.88      0.88      0.88     17550\n",
            "           3       0.90      0.78      0.83     19919\n",
            "           4       0.49      0.91      0.63      8969\n",
            "           5       0.45      0.99      0.62      6851\n",
            "           6       0.94      0.93      0.94     16389\n",
            "           7       0.82      0.97      0.89     14817\n",
            "           8       0.93      0.62      0.75     24769\n",
            "           9       0.92      0.60      0.73     26242\n",
            "\n",
            "    accuracy                           0.82    170000\n",
            "   macro avg       0.82      0.86      0.82    170000\n",
            "weighted avg       0.87      0.82      0.83    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.802 Loss: 1.642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     17163\n",
            "           1       0.91      0.97      0.94     19316\n",
            "           2       0.88      0.88      0.88     18500\n",
            "           3       0.90      0.78      0.84     20923\n",
            "           4       0.47      0.91      0.62      9204\n",
            "           5       0.45      0.99      0.62      7292\n",
            "           6       0.94      0.94      0.94     17325\n",
            "           7       0.82      0.97      0.89     15721\n",
            "           8       0.94      0.62      0.75     26474\n",
            "           9       0.92      0.60      0.72     28082\n",
            "\n",
            "    accuracy                           0.82    180000\n",
            "   macro avg       0.82      0.86      0.82    180000\n",
            "weighted avg       0.87      0.82      0.83    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.822 Loss: 1.631\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     18141\n",
            "           1       0.91      0.97      0.94     20398\n",
            "           2       0.88      0.89      0.88     19449\n",
            "           3       0.90      0.79      0.84     21900\n",
            "           4       0.46      0.91      0.61      9526\n",
            "           5       0.46      0.99      0.62      7782\n",
            "           6       0.94      0.94      0.94     18286\n",
            "           7       0.82      0.97      0.89     16624\n",
            "           8       0.94      0.62      0.75     28086\n",
            "           9       0.92      0.59      0.72     29808\n",
            "\n",
            "    accuracy                           0.82    190000\n",
            "   macro avg       0.82      0.86      0.82    190000\n",
            "weighted avg       0.87      0.82      0.83    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.800 Loss: 1.634\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     19050\n",
            "           1       0.91      0.97      0.94     21451\n",
            "           2       0.88      0.89      0.88     20390\n",
            "           3       0.90      0.79      0.84     22933\n",
            "           4       0.45      0.91      0.60      9743\n",
            "           5       0.46      0.99      0.63      8226\n",
            "           6       0.94      0.94      0.94     19241\n",
            "           7       0.83      0.97      0.89     17514\n",
            "           8       0.94      0.61      0.74     29853\n",
            "           9       0.92      0.59      0.72     31599\n",
            "\n",
            "    accuracy                           0.82    200000\n",
            "   macro avg       0.82      0.86      0.81    200000\n",
            "weighted avg       0.87      0.82      0.83    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.802 Loss: 1.636\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     19991\n",
            "           1       0.91      0.97      0.94     22464\n",
            "           2       0.88      0.89      0.88     21307\n",
            "           3       0.90      0.79      0.84     23976\n",
            "           4       0.44      0.91      0.59     10014\n",
            "           5       0.46      0.99      0.63      8708\n",
            "           6       0.94      0.94      0.94     20167\n",
            "           7       0.83      0.97      0.89     18390\n",
            "           8       0.94      0.61      0.74     31660\n",
            "           9       0.92      0.59      0.72     33323\n",
            "\n",
            "    accuracy                           0.82    210000\n",
            "   macro avg       0.82      0.86      0.81    210000\n",
            "weighted avg       0.87      0.82      0.83    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.813 Loss: 1.624\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     20962\n",
            "           1       0.91      0.97      0.94     23562\n",
            "           2       0.88      0.89      0.89     22288\n",
            "           3       0.90      0.80      0.84     25018\n",
            "           4       0.43      0.91      0.58     10185\n",
            "           5       0.46      0.99      0.63      9196\n",
            "           6       0.94      0.94      0.94     21094\n",
            "           7       0.83      0.97      0.89     19302\n",
            "           8       0.94      0.61      0.74     33215\n",
            "           9       0.92      0.58      0.71     35178\n",
            "\n",
            "    accuracy                           0.82    220000\n",
            "   macro avg       0.82      0.86      0.81    220000\n",
            "weighted avg       0.87      0.82      0.82    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.814 Loss: 1.623\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     21884\n",
            "           1       0.91      0.97      0.94     24648\n",
            "           2       0.88      0.90      0.89     23274\n",
            "           3       0.90      0.80      0.85     26098\n",
            "           4       0.42      0.91      0.57     10381\n",
            "           5       0.47      0.99      0.64      9700\n",
            "           6       0.94      0.94      0.94     22048\n",
            "           7       0.83      0.97      0.89     20201\n",
            "           8       0.95      0.61      0.74     34778\n",
            "           9       0.92      0.58      0.71     36988\n",
            "\n",
            "    accuracy                           0.82    230000\n",
            "   macro avg       0.82      0.86      0.81    230000\n",
            "weighted avg       0.87      0.82      0.82    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.792 Loss: 1.630\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     22823\n",
            "           1       0.91      0.97      0.94     25670\n",
            "           2       0.88      0.90      0.89     24196\n",
            "           3       0.90      0.80      0.85     27065\n",
            "           4       0.40      0.90      0.56     10488\n",
            "           5       0.47      0.99      0.64     10211\n",
            "           6       0.94      0.94      0.94     22981\n",
            "           7       0.83      0.97      0.89     21091\n",
            "           8       0.95      0.61      0.74     36531\n",
            "           9       0.92      0.57      0.71     38944\n",
            "\n",
            "    accuracy                           0.82    240000\n",
            "   macro avg       0.81      0.86      0.81    240000\n",
            "weighted avg       0.87      0.82      0.82    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.817 Loss: 1.618\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     23784\n",
            "           1       0.91      0.97      0.94     26749\n",
            "           2       0.88      0.90      0.89     25175\n",
            "           3       0.90      0.81      0.85     28123\n",
            "           4       0.39      0.90      0.55     10654\n",
            "           5       0.48      0.99      0.64     10746\n",
            "           6       0.94      0.94      0.94     23920\n",
            "           7       0.83      0.97      0.89     22005\n",
            "           8       0.95      0.61      0.74     38028\n",
            "           9       0.92      0.57      0.71     40816\n",
            "\n",
            "    accuracy                           0.82    250000\n",
            "   macro avg       0.81      0.86      0.81    250000\n",
            "weighted avg       0.87      0.82      0.82    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.819 Loss: 1.612\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     24716\n",
            "           1       0.91      0.97      0.94     27813\n",
            "           2       0.88      0.90      0.89     26158\n",
            "           3       0.90      0.81      0.85     29115\n",
            "           4       0.39      0.90      0.54     10893\n",
            "           5       0.48      0.99      0.65     11261\n",
            "           6       0.94      0.94      0.94     24880\n",
            "           7       0.83      0.97      0.90     22936\n",
            "           8       0.95      0.61      0.74     39620\n",
            "           9       0.92      0.57      0.70     42608\n",
            "\n",
            "    accuracy                           0.82    260000\n",
            "   macro avg       0.81      0.86      0.81    260000\n",
            "weighted avg       0.87      0.82      0.82    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.803 Loss: 1.627\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     25654\n",
            "           1       0.91      0.97      0.94     28816\n",
            "           2       0.88      0.90      0.89     27078\n",
            "           3       0.90      0.81      0.85     30133\n",
            "           4       0.38      0.90      0.53     11066\n",
            "           5       0.49      0.99      0.65     11781\n",
            "           6       0.94      0.94      0.94     25815\n",
            "           7       0.83      0.97      0.90     23857\n",
            "           8       0.95      0.61      0.74     41325\n",
            "           9       0.92      0.57      0.70     44475\n",
            "\n",
            "    accuracy                           0.82    270000\n",
            "   macro avg       0.81      0.86      0.81    270000\n",
            "weighted avg       0.87      0.82      0.82    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.826 Loss: 1.603\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     26617\n",
            "           1       0.91      0.97      0.94     29913\n",
            "           2       0.88      0.90      0.89     28089\n",
            "           3       0.90      0.81      0.85     31160\n",
            "           4       0.37      0.90      0.52     11265\n",
            "           5       0.49      0.99      0.66     12337\n",
            "           6       0.94      0.94      0.94     26758\n",
            "           7       0.83      0.97      0.90     24797\n",
            "           8       0.95      0.61      0.74     42782\n",
            "           9       0.92      0.56      0.70     46282\n",
            "\n",
            "    accuracy                           0.82    280000\n",
            "   macro avg       0.81      0.86      0.81    280000\n",
            "weighted avg       0.87      0.82      0.82    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.815 Loss: 1.611\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     27539\n",
            "           1       0.91      0.97      0.94     30958\n",
            "           2       0.88      0.90      0.89     29048\n",
            "           3       0.90      0.82      0.86     32222\n",
            "           4       0.36      0.90      0.52     11480\n",
            "           5       0.49      0.99      0.66     12850\n",
            "           6       0.94      0.94      0.94     27702\n",
            "           7       0.84      0.97      0.90     25730\n",
            "           8       0.95      0.61      0.74     44401\n",
            "           9       0.92      0.56      0.70     48070\n",
            "\n",
            "    accuracy                           0.82    290000\n",
            "   macro avg       0.81      0.86      0.81    290000\n",
            "weighted avg       0.87      0.82      0.82    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.809 Loss: 1.615\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     28466\n",
            "           1       0.91      0.97      0.94     31977\n",
            "           2       0.88      0.91      0.89     29990\n",
            "           3       0.90      0.82      0.86     33258\n",
            "           4       0.36      0.90      0.51     11679\n",
            "           5       0.50      0.99      0.66     13390\n",
            "           6       0.94      0.94      0.94     28630\n",
            "           7       0.84      0.97      0.90     26654\n",
            "           8       0.95      0.60      0.74     46077\n",
            "           9       0.92      0.56      0.70     49879\n",
            "\n",
            "    accuracy                           0.82    300000\n",
            "   macro avg       0.81      0.86      0.81    300000\n",
            "weighted avg       0.87      0.82      0.82    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.829 Loss: 1.604\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     29433\n",
            "           1       0.91      0.97      0.94     33086\n",
            "           2       0.88      0.91      0.89     31000\n",
            "           3       0.90      0.82      0.86     34236\n",
            "           4       0.35      0.90      0.50     11855\n",
            "           5       0.50      0.99      0.67     14016\n",
            "           6       0.94      0.94      0.94     29567\n",
            "           7       0.84      0.97      0.90     27566\n",
            "           8       0.95      0.61      0.74     47511\n",
            "           9       0.92      0.56      0.70     51730\n",
            "\n",
            "    accuracy                           0.82    310000\n",
            "   macro avg       0.81      0.86      0.81    310000\n",
            "weighted avg       0.87      0.82      0.82    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.822 Loss: 1.603\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     30356\n",
            "           1       0.91      0.97      0.94     34139\n",
            "           2       0.88      0.91      0.89     31982\n",
            "           3       0.90      0.82      0.86     35245\n",
            "           4       0.35      0.90      0.50     12112\n",
            "           5       0.51      0.99      0.67     14573\n",
            "           6       0.94      0.94      0.94     30514\n",
            "           7       0.84      0.97      0.90     28481\n",
            "           8       0.96      0.61      0.74     49116\n",
            "           9       0.92      0.56      0.70     53482\n",
            "\n",
            "    accuracy                           0.82    320000\n",
            "   macro avg       0.81      0.86      0.81    320000\n",
            "weighted avg       0.87      0.82      0.82    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.816 Loss: 1.609\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     31314\n",
            "           1       0.91      0.97      0.94     35157\n",
            "           2       0.88      0.91      0.89     32919\n",
            "           3       0.90      0.83      0.86     36257\n",
            "           4       0.34      0.90      0.49     12295\n",
            "           5       0.51      0.99      0.67     15163\n",
            "           6       0.94      0.94      0.94     31455\n",
            "           7       0.84      0.97      0.90     29395\n",
            "           8       0.96      0.61      0.74     50713\n",
            "           9       0.92      0.56      0.69     55332\n",
            "\n",
            "    accuracy                           0.82    330000\n",
            "   macro avg       0.81      0.86      0.81    330000\n",
            "weighted avg       0.87      0.82      0.82    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.826 Loss: 1.602\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     32273\n",
            "           1       0.91      0.97      0.94     36248\n",
            "           2       0.88      0.91      0.89     33915\n",
            "           3       0.90      0.83      0.86     37258\n",
            "           4       0.34      0.90      0.49     12469\n",
            "           5       0.51      0.99      0.68     15762\n",
            "           6       0.94      0.94      0.94     32407\n",
            "           7       0.84      0.97      0.90     30319\n",
            "           8       0.96      0.61      0.74     52175\n",
            "           9       0.92      0.55      0.69     57174\n",
            "\n",
            "    accuracy                           0.82    340000\n",
            "   macro avg       0.81      0.86      0.81    340000\n",
            "weighted avg       0.88      0.82      0.82    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.815 Loss: 1.606\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     33207\n",
            "           1       0.91      0.97      0.94     37306\n",
            "           2       0.88      0.91      0.89     34897\n",
            "           3       0.90      0.83      0.86     38273\n",
            "           4       0.33      0.90      0.48     12668\n",
            "           5       0.52      0.99      0.68     16277\n",
            "           6       0.94      0.94      0.94     33354\n",
            "           7       0.84      0.97      0.90     31246\n",
            "           8       0.96      0.61      0.74     53804\n",
            "           9       0.92      0.55      0.69     58968\n",
            "\n",
            "    accuracy                           0.82    350000\n",
            "   macro avg       0.81      0.86      0.81    350000\n",
            "weighted avg       0.88      0.82      0.82    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.809 Loss: 1.607\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     34155\n",
            "           1       0.91      0.97      0.94     38348\n",
            "           2       0.88      0.91      0.89     35840\n",
            "           3       0.90      0.83      0.86     39248\n",
            "           4       0.32      0.90      0.48     12778\n",
            "           5       0.52      0.99      0.68     16849\n",
            "           6       0.94      0.94      0.94     34294\n",
            "           7       0.84      0.97      0.90     32164\n",
            "           8       0.96      0.61      0.74     55424\n",
            "           9       0.92      0.55      0.69     60900\n",
            "\n",
            "    accuracy                           0.82    360000\n",
            "   macro avg       0.81      0.86      0.81    360000\n",
            "weighted avg       0.88      0.82      0.82    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.842 Loss: 1.589\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     35134\n",
            "           1       0.91      0.97      0.94     39455\n",
            "           2       0.88      0.91      0.90     36828\n",
            "           3       0.90      0.83      0.86     40291\n",
            "           4       0.32      0.90      0.47     12954\n",
            "           5       0.53      0.99      0.69     17520\n",
            "           6       0.94      0.94      0.94     35261\n",
            "           7       0.84      0.97      0.90     33112\n",
            "           8       0.96      0.61      0.74     56707\n",
            "           9       0.92      0.55      0.69     62738\n",
            "\n",
            "    accuracy                           0.82    370000\n",
            "   macro avg       0.81      0.86      0.81    370000\n",
            "weighted avg       0.88      0.82      0.83    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.817 Loss: 1.595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     36079\n",
            "           1       0.91      0.97      0.94     40506\n",
            "           2       0.88      0.91      0.90     37809\n",
            "           3       0.90      0.84      0.87     41279\n",
            "           4       0.31      0.90      0.47     13079\n",
            "           5       0.53      0.99      0.69     18093\n",
            "           6       0.94      0.94      0.94     36223\n",
            "           7       0.84      0.97      0.90     34061\n",
            "           8       0.96      0.61      0.75     58247\n",
            "           9       0.93      0.55      0.69     64624\n",
            "\n",
            "    accuracy                           0.82    380000\n",
            "   macro avg       0.81      0.86      0.81    380000\n",
            "weighted avg       0.88      0.82      0.83    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.804 Loss: 1.609\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     36999\n",
            "           1       0.91      0.97      0.94     41530\n",
            "           2       0.88      0.91      0.90     38727\n",
            "           3       0.90      0.84      0.87     42304\n",
            "           4       0.31      0.90      0.46     13195\n",
            "           5       0.53      0.99      0.69     18641\n",
            "           6       0.94      0.94      0.94     37162\n",
            "           7       0.85      0.97      0.90     34996\n",
            "           8       0.96      0.61      0.74     59922\n",
            "           9       0.93      0.55      0.69     66524\n",
            "\n",
            "    accuracy                           0.82    390000\n",
            "   macro avg       0.81      0.86      0.81    390000\n",
            "weighted avg       0.88      0.82      0.83    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.834 Loss: 1.589\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     37966\n",
            "           1       0.91      0.97      0.94     42641\n",
            "           2       0.88      0.91      0.90     39720\n",
            "           3       0.90      0.84      0.87     43300\n",
            "           4       0.30      0.89      0.45     13354\n",
            "           5       0.54      0.99      0.69     19272\n",
            "           6       0.94      0.95      0.94     38116\n",
            "           7       0.85      0.97      0.90     35941\n",
            "           8       0.96      0.61      0.75     61322\n",
            "           9       0.93      0.55      0.69     68368\n",
            "\n",
            "    accuracy                           0.82    400000\n",
            "   macro avg       0.81      0.87      0.81    400000\n",
            "weighted avg       0.88      0.82      0.83    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.818 Loss: 1.588\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     38916\n",
            "           1       0.91      0.97      0.94     43713\n",
            "           2       0.88      0.91      0.90     40692\n",
            "           3       0.90      0.84      0.87     44288\n",
            "           4       0.30      0.89      0.45     13460\n",
            "           5       0.54      0.99      0.70     19850\n",
            "           6       0.94      0.95      0.94     39059\n",
            "           7       0.85      0.97      0.90     36891\n",
            "           8       0.96      0.61      0.75     62854\n",
            "           9       0.93      0.54      0.69     70277\n",
            "\n",
            "    accuracy                           0.82    410000\n",
            "   macro avg       0.81      0.87      0.81    410000\n",
            "weighted avg       0.88      0.82      0.83    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.816 Loss: 1.596\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     39862\n",
            "           1       0.91      0.97      0.94     44763\n",
            "           2       0.88      0.92      0.90     41623\n",
            "           3       0.90      0.84      0.87     45314\n",
            "           4       0.29      0.89      0.44     13549\n",
            "           5       0.54      0.99      0.70     20480\n",
            "           6       0.94      0.95      0.94     40001\n",
            "           7       0.85      0.97      0.90     37817\n",
            "           8       0.96      0.61      0.75     64364\n",
            "           9       0.93      0.54      0.68     72227\n",
            "\n",
            "    accuracy                           0.82    420000\n",
            "   macro avg       0.81      0.87      0.81    420000\n",
            "weighted avg       0.88      0.82      0.83    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.826 Loss: 1.590\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     40825\n",
            "           1       0.91      0.97      0.94     45849\n",
            "           2       0.88      0.92      0.90     42605\n",
            "           3       0.90      0.84      0.87     46311\n",
            "           4       0.29      0.89      0.44     13639\n",
            "           5       0.55      0.99      0.70     21115\n",
            "           6       0.94      0.95      0.94     40957\n",
            "           7       0.85      0.97      0.90     38752\n",
            "           8       0.96      0.61      0.75     65781\n",
            "           9       0.93      0.54      0.68     74166\n",
            "\n",
            "    accuracy                           0.82    430000\n",
            "   macro avg       0.81      0.87      0.81    430000\n",
            "weighted avg       0.88      0.82      0.83    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.823 Loss: 1.587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     41759\n",
            "           1       0.91      0.97      0.94     46928\n",
            "           2       0.88      0.92      0.90     43580\n",
            "           3       0.90      0.85      0.87     47308\n",
            "           4       0.28      0.89      0.43     13734\n",
            "           5       0.55      0.99      0.71     21743\n",
            "           6       0.94      0.95      0.94     41918\n",
            "           7       0.85      0.97      0.90     39694\n",
            "           8       0.96      0.61      0.75     67248\n",
            "           9       0.93      0.54      0.68     76088\n",
            "\n",
            "    accuracy                           0.82    440000\n",
            "   macro avg       0.81      0.87      0.81    440000\n",
            "weighted avg       0.88      0.82      0.83    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.811 Loss: 1.598\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     42690\n",
            "           1       0.91      0.97      0.94     47979\n",
            "           2       0.88      0.92      0.90     44492\n",
            "           3       0.90      0.85      0.87     48352\n",
            "           4       0.28      0.89      0.42     13798\n",
            "           5       0.55      0.99      0.71     22360\n",
            "           6       0.94      0.95      0.94     42856\n",
            "           7       0.85      0.97      0.91     40615\n",
            "           8       0.96      0.61      0.75     68787\n",
            "           9       0.93      0.54      0.68     78071\n",
            "\n",
            "    accuracy                           0.82    450000\n",
            "   macro avg       0.81      0.87      0.81    450000\n",
            "weighted avg       0.88      0.82      0.83    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.826 Loss: 1.583\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     43643\n",
            "           1       0.91      0.97      0.94     49091\n",
            "           2       0.88      0.92      0.90     45450\n",
            "           3       0.90      0.85      0.87     49366\n",
            "           4       0.27      0.89      0.42     13862\n",
            "           5       0.56      0.99      0.71     23015\n",
            "           6       0.94      0.95      0.94     43803\n",
            "           7       0.85      0.97      0.91     41555\n",
            "           8       0.96      0.61      0.75     70187\n",
            "           9       0.93      0.54      0.68     80028\n",
            "\n",
            "    accuracy                           0.82    460000\n",
            "   macro avg       0.81      0.87      0.81    460000\n",
            "weighted avg       0.88      0.82      0.83    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.821 Loss: 1.586\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     44568\n",
            "           1       0.91      0.97      0.94     50177\n",
            "           2       0.88      0.92      0.90     46395\n",
            "           3       0.90      0.85      0.87     50384\n",
            "           4       0.27      0.89      0.41     13964\n",
            "           5       0.56      0.99      0.71     23636\n",
            "           6       0.94      0.95      0.94     44762\n",
            "           7       0.85      0.97      0.91     42489\n",
            "           8       0.96      0.62      0.75     71647\n",
            "           9       0.93      0.54      0.68     81978\n",
            "\n",
            "    accuracy                           0.82    470000\n",
            "   macro avg       0.81      0.87      0.81    470000\n",
            "weighted avg       0.88      0.82      0.83    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.818 Loss: 1.590\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     45505\n",
            "           1       0.91      0.97      0.94     51240\n",
            "           2       0.88      0.92      0.90     47328\n",
            "           3       0.90      0.85      0.88     51439\n",
            "           4       0.26      0.89      0.41     14039\n",
            "           5       0.56      0.99      0.72     24257\n",
            "           6       0.94      0.95      0.94     45714\n",
            "           7       0.85      0.97      0.91     43445\n",
            "           8       0.96      0.62      0.75     73079\n",
            "           9       0.93      0.54      0.68     83954\n",
            "\n",
            "    accuracy                           0.82    480000\n",
            "   macro avg       0.81      0.87      0.81    480000\n",
            "weighted avg       0.88      0.82      0.83    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.833 Loss: 1.577\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     46451\n",
            "           1       0.92      0.97      0.94     52345\n",
            "           2       0.88      0.92      0.90     48315\n",
            "           3       0.90      0.85      0.88     52486\n",
            "           4       0.26      0.89      0.40     14119\n",
            "           5       0.56      0.99      0.72     24918\n",
            "           6       0.94      0.95      0.94     46662\n",
            "           7       0.85      0.97      0.91     44407\n",
            "           8       0.96      0.62      0.75     74396\n",
            "           9       0.93      0.53      0.68     85901\n",
            "\n",
            "    accuracy                           0.82    490000\n",
            "   macro avg       0.82      0.87      0.81    490000\n",
            "weighted avg       0.88      0.82      0.83    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.827 Loss: 1.578\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     47390\n",
            "           1       0.92      0.97      0.94     53445\n",
            "           2       0.88      0.92      0.90     49281\n",
            "           3       0.90      0.85      0.88     53505\n",
            "           4       0.26      0.89      0.40     14202\n",
            "           5       0.57      0.99      0.72     25560\n",
            "           6       0.94      0.95      0.94     47622\n",
            "           7       0.85      0.97      0.91     45350\n",
            "           8       0.96      0.62      0.75     75794\n",
            "           9       0.93      0.53      0.68     87851\n",
            "\n",
            "    accuracy                           0.82    500000\n",
            "   macro avg       0.82      0.87      0.81    500000\n",
            "weighted avg       0.88      0.82      0.83    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.817 Loss: 1.586\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     48344\n",
            "           1       0.92      0.97      0.94     54493\n",
            "           2       0.88      0.92      0.90     50214\n",
            "           3       0.90      0.85      0.88     54542\n",
            "           4       0.25      0.89      0.39     14274\n",
            "           5       0.57      0.99      0.72     26200\n",
            "           6       0.94      0.95      0.94     48557\n",
            "           7       0.85      0.97      0.91     46280\n",
            "           8       0.96      0.62      0.75     77293\n",
            "           9       0.93      0.53      0.68     89803\n",
            "\n",
            "    accuracy                           0.82    510000\n",
            "   macro avg       0.82      0.87      0.81    510000\n",
            "weighted avg       0.88      0.82      0.83    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.836 Loss: 1.571\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     49327\n",
            "           1       0.92      0.97      0.94     55617\n",
            "           2       0.88      0.92      0.90     51201\n",
            "           3       0.90      0.85      0.88     55549\n",
            "           4       0.25      0.89      0.39     14350\n",
            "           5       0.57      0.99      0.73     26886\n",
            "           6       0.94      0.95      0.94     49509\n",
            "           7       0.85      0.97      0.91     47224\n",
            "           8       0.96      0.62      0.76     78586\n",
            "           9       0.93      0.53      0.68     91751\n",
            "\n",
            "    accuracy                           0.82    520000\n",
            "   macro avg       0.82      0.87      0.81    520000\n",
            "weighted avg       0.88      0.82      0.83    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.836 Loss: 1.569\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     50288\n",
            "           1       0.92      0.97      0.94     56705\n",
            "           2       0.88      0.92      0.90     52159\n",
            "           3       0.90      0.86      0.88     56584\n",
            "           4       0.25      0.89      0.38     14434\n",
            "           5       0.58      0.99      0.73     27570\n",
            "           6       0.94      0.95      0.94     50481\n",
            "           7       0.86      0.97      0.91     48194\n",
            "           8       0.96      0.62      0.76     79919\n",
            "           9       0.93      0.53      0.68     93666\n",
            "\n",
            "    accuracy                           0.82    530000\n",
            "   macro avg       0.82      0.87      0.81    530000\n",
            "weighted avg       0.88      0.82      0.83    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.826 Loss: 1.580\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     51258\n",
            "           1       0.92      0.97      0.94     57788\n",
            "           2       0.88      0.92      0.90     53110\n",
            "           3       0.90      0.86      0.88     57636\n",
            "           4       0.24      0.88      0.38     14495\n",
            "           5       0.58      0.99      0.73     28226\n",
            "           6       0.94      0.95      0.94     51424\n",
            "           7       0.86      0.97      0.91     49132\n",
            "           8       0.96      0.62      0.76     81310\n",
            "           9       0.93      0.53      0.67     95621\n",
            "\n",
            "    accuracy                           0.82    540000\n",
            "   macro avg       0.82      0.87      0.81    540000\n",
            "weighted avg       0.89      0.82      0.83    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.839 Loss: 1.569\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     52239\n",
            "           1       0.92      0.97      0.94     58884\n",
            "           2       0.88      0.92      0.90     54104\n",
            "           3       0.90      0.86      0.88     58642\n",
            "           4       0.24      0.88      0.38     14581\n",
            "           5       0.58      0.99      0.73     28932\n",
            "           6       0.94      0.95      0.94     52388\n",
            "           7       0.86      0.97      0.91     50103\n",
            "           8       0.96      0.63      0.76     82603\n",
            "           9       0.93      0.53      0.67     97524\n",
            "\n",
            "    accuracy                           0.82    550000\n",
            "   macro avg       0.82      0.87      0.81    550000\n",
            "weighted avg       0.89      0.82      0.83    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.828 Loss: 1.573\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     53183\n",
            "           1       0.92      0.97      0.94     59963\n",
            "           2       0.88      0.93      0.90     55067\n",
            "           3       0.90      0.86      0.88     59651\n",
            "           4       0.24      0.88      0.37     14654\n",
            "           5       0.59      0.99      0.74     29601\n",
            "           6       0.94      0.95      0.94     53336\n",
            "           7       0.86      0.97      0.91     51075\n",
            "           8       0.97      0.63      0.76     84031\n",
            "           9       0.93      0.53      0.67     99439\n",
            "\n",
            "    accuracy                           0.82    560000\n",
            "   macro avg       0.82      0.87      0.81    560000\n",
            "weighted avg       0.89      0.82      0.83    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.827 Loss: 1.576\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     54157\n",
            "           1       0.92      0.97      0.94     61028\n",
            "           2       0.88      0.93      0.90     56004\n",
            "           3       0.91      0.86      0.88     60654\n",
            "           4       0.23      0.88      0.37     14704\n",
            "           5       0.59      0.99      0.74     30318\n",
            "           6       0.94      0.95      0.94     54283\n",
            "           7       0.86      0.97      0.91     52014\n",
            "           8       0.97      0.63      0.76     85425\n",
            "           9       0.93      0.53      0.67    101413\n",
            "\n",
            "    accuracy                           0.82    570000\n",
            "   macro avg       0.82      0.87      0.81    570000\n",
            "weighted avg       0.89      0.82      0.83    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.839 Loss: 1.570\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     55127\n",
            "           1       0.92      0.97      0.94     62144\n",
            "           2       0.88      0.93      0.90     56964\n",
            "           3       0.91      0.86      0.88     61660\n",
            "           4       0.23      0.88      0.36     14767\n",
            "           5       0.59      0.99      0.74     31050\n",
            "           6       0.94      0.95      0.94     55238\n",
            "           7       0.86      0.97      0.91     52974\n",
            "           8       0.97      0.63      0.76     86719\n",
            "           9       0.93      0.53      0.67    103357\n",
            "\n",
            "    accuracy                           0.82    580000\n",
            "   macro avg       0.82      0.87      0.81    580000\n",
            "weighted avg       0.89      0.82      0.83    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.833 Loss: 1.570\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96     56086\n",
            "           1       0.92      0.97      0.95     63223\n",
            "           2       0.88      0.93      0.90     57929\n",
            "           3       0.91      0.86      0.88     62640\n",
            "           4       0.23      0.88      0.36     14848\n",
            "           5       0.60      0.99      0.74     31763\n",
            "           6       0.94      0.95      0.94     56199\n",
            "           7       0.86      0.97      0.91     53929\n",
            "           8       0.97      0.63      0.76     88087\n",
            "           9       0.93      0.53      0.67    105296\n",
            "\n",
            "    accuracy                           0.82    590000\n",
            "   macro avg       0.82      0.87      0.81    590000\n",
            "weighted avg       0.89      0.82      0.83    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.829 Loss: 1.577\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     57043\n",
            "           1       0.92      0.97      0.95     64271\n",
            "           2       0.88      0.93      0.90     58870\n",
            "           3       0.91      0.86      0.88     63661\n",
            "           4       0.22      0.88      0.36     14914\n",
            "           5       0.60      0.99      0.75     32486\n",
            "           6       0.94      0.95      0.94     57153\n",
            "           7       0.86      0.97      0.91     54868\n",
            "           8       0.97      0.63      0.76     89482\n",
            "           9       0.93      0.52      0.67    107252\n",
            "\n",
            "    accuracy                           0.82    600000\n",
            "   macro avg       0.82      0.87      0.81    600000\n",
            "weighted avg       0.89      0.82      0.83    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.842 Loss: 1.564\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     58029\n",
            "           1       0.92      0.97      0.95     65382\n",
            "           2       0.88      0.93      0.90     59841\n",
            "           3       0.91      0.86      0.88     64665\n",
            "           4       0.22      0.88      0.35     14974\n",
            "           5       0.60      0.99      0.75     33221\n",
            "           6       0.94      0.95      0.95     58131\n",
            "           7       0.86      0.97      0.91     55828\n",
            "           8       0.97      0.63      0.76     90726\n",
            "           9       0.93      0.52      0.67    109203\n",
            "\n",
            "    accuracy                           0.82    610000\n",
            "   macro avg       0.82      0.87      0.81    610000\n",
            "weighted avg       0.89      0.82      0.83    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.843 Loss: 1.560\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     59002\n",
            "           1       0.92      0.97      0.95     66476\n",
            "           2       0.88      0.93      0.91     60814\n",
            "           3       0.91      0.86      0.89     65692\n",
            "           4       0.22      0.88      0.35     15042\n",
            "           5       0.61      0.99      0.75     33970\n",
            "           6       0.94      0.95      0.95     59108\n",
            "           7       0.86      0.97      0.91     56792\n",
            "           8       0.97      0.63      0.77     91949\n",
            "           9       0.93      0.52      0.67    111155\n",
            "\n",
            "    accuracy                           0.82    620000\n",
            "   macro avg       0.82      0.87      0.81    620000\n",
            "weighted avg       0.89      0.82      0.83    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.832 Loss: 1.570\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     59973\n",
            "           1       0.92      0.97      0.95     67553\n",
            "           2       0.88      0.93      0.91     61773\n",
            "           3       0.91      0.87      0.89     66721\n",
            "           4       0.21      0.88      0.35     15090\n",
            "           5       0.61      0.99      0.75     34693\n",
            "           6       0.94      0.95      0.95     60039\n",
            "           7       0.86      0.97      0.91     57739\n",
            "           8       0.97      0.64      0.77     93302\n",
            "           9       0.93      0.52      0.67    113117\n",
            "\n",
            "    accuracy                           0.82    630000\n",
            "   macro avg       0.82      0.87      0.81    630000\n",
            "weighted avg       0.89      0.82      0.83    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.846 Loss: 1.560\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     60955\n",
            "           1       0.92      0.97      0.95     68670\n",
            "           2       0.88      0.93      0.91     62753\n",
            "           3       0.91      0.87      0.89     67739\n",
            "           4       0.21      0.88      0.34     15147\n",
            "           5       0.61      0.99      0.76     35455\n",
            "           6       0.94      0.95      0.95     61002\n",
            "           7       0.86      0.97      0.91     58719\n",
            "           8       0.97      0.64      0.77     94526\n",
            "           9       0.93      0.52      0.67    115034\n",
            "\n",
            "    accuracy                           0.82    640000\n",
            "   macro avg       0.82      0.87      0.81    640000\n",
            "weighted avg       0.89      0.82      0.83    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.836 Loss: 1.561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     61925\n",
            "           1       0.92      0.97      0.95     69757\n",
            "           2       0.88      0.93      0.91     63706\n",
            "           3       0.91      0.87      0.89     68788\n",
            "           4       0.21      0.88      0.34     15190\n",
            "           5       0.62      0.99      0.76     36182\n",
            "           6       0.94      0.95      0.95     61951\n",
            "           7       0.86      0.97      0.91     59673\n",
            "           8       0.97      0.64      0.77     95841\n",
            "           9       0.93      0.52      0.67    116987\n",
            "\n",
            "    accuracy                           0.82    650000\n",
            "   macro avg       0.82      0.87      0.81    650000\n",
            "weighted avg       0.89      0.82      0.83    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.832 Loss: 1.567\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     62890\n",
            "           1       0.92      0.97      0.95     70831\n",
            "           2       0.88      0.93      0.91     64661\n",
            "           3       0.91      0.87      0.89     69860\n",
            "           4       0.21      0.88      0.33     15229\n",
            "           5       0.62      0.99      0.76     36875\n",
            "           6       0.94      0.95      0.95     62885\n",
            "           7       0.86      0.97      0.91     60650\n",
            "           8       0.97      0.64      0.77     97189\n",
            "           9       0.93      0.52      0.67    118930\n",
            "\n",
            "    accuracy                           0.82    660000\n",
            "   macro avg       0.82      0.87      0.81    660000\n",
            "weighted avg       0.89      0.82      0.83    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.844 Loss: 1.559\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     63867\n",
            "           1       0.92      0.97      0.95     71940\n",
            "           2       0.88      0.93      0.91     65657\n",
            "           3       0.91      0.87      0.89     70905\n",
            "           4       0.20      0.88      0.33     15293\n",
            "           5       0.62      0.99      0.76     37598\n",
            "           6       0.94      0.95      0.95     63854\n",
            "           7       0.86      0.97      0.91     61619\n",
            "           8       0.97      0.64      0.77     98426\n",
            "           9       0.93      0.52      0.67    120841\n",
            "\n",
            "    accuracy                           0.82    670000\n",
            "   macro avg       0.82      0.87      0.81    670000\n",
            "weighted avg       0.89      0.82      0.83    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.841 Loss: 1.561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     64827\n",
            "           1       0.92      0.97      0.95     73020\n",
            "           2       0.88      0.93      0.91     66631\n",
            "           3       0.91      0.87      0.89     71973\n",
            "           4       0.20      0.88      0.33     15361\n",
            "           5       0.62      0.99      0.76     38336\n",
            "           6       0.94      0.95      0.95     64818\n",
            "           7       0.87      0.97      0.91     62569\n",
            "           8       0.97      0.64      0.77     99685\n",
            "           9       0.93      0.52      0.67    122780\n",
            "\n",
            "    accuracy                           0.82    680000\n",
            "   macro avg       0.82      0.87      0.81    680000\n",
            "weighted avg       0.89      0.82      0.83    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.836 Loss: 1.565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     65787\n",
            "           1       0.92      0.97      0.95     74102\n",
            "           2       0.88      0.93      0.91     67598\n",
            "           3       0.91      0.87      0.89     73006\n",
            "           4       0.20      0.88      0.33     15410\n",
            "           5       0.63      0.99      0.77     39056\n",
            "           6       0.94      0.95      0.95     65771\n",
            "           7       0.87      0.97      0.91     63547\n",
            "           8       0.97      0.64      0.77    101011\n",
            "           9       0.93      0.52      0.67    124712\n",
            "\n",
            "    accuracy                           0.82    690000\n",
            "   macro avg       0.82      0.87      0.81    690000\n",
            "weighted avg       0.89      0.82      0.83    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.846 Loss: 1.555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     66770\n",
            "           1       0.92      0.97      0.95     75206\n",
            "           2       0.88      0.93      0.91     68572\n",
            "           3       0.91      0.87      0.89     74057\n",
            "           4       0.20      0.88      0.32     15482\n",
            "           5       0.63      0.99      0.77     39809\n",
            "           6       0.94      0.95      0.95     66731\n",
            "           7       0.87      0.97      0.91     64524\n",
            "           8       0.97      0.65      0.77    102215\n",
            "           9       0.93      0.52      0.67    126634\n",
            "\n",
            "    accuracy                           0.82    700000\n",
            "   macro avg       0.82      0.87      0.81    700000\n",
            "weighted avg       0.89      0.82      0.84    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.842 Loss: 1.554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     67733\n",
            "           1       0.92      0.97      0.95     76303\n",
            "           2       0.88      0.93      0.91     69555\n",
            "           3       0.91      0.87      0.89     75123\n",
            "           4       0.20      0.88      0.32     15536\n",
            "           5       0.63      0.99      0.77     40550\n",
            "           6       0.94      0.95      0.95     67688\n",
            "           7       0.87      0.97      0.91     65491\n",
            "           8       0.97      0.65      0.78    103457\n",
            "           9       0.93      0.52      0.67    128564\n",
            "\n",
            "    accuracy                           0.82    710000\n",
            "   macro avg       0.82      0.87      0.81    710000\n",
            "weighted avg       0.89      0.82      0.84    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.838 Loss: 1.558\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     68700\n",
            "           1       0.92      0.97      0.95     77392\n",
            "           2       0.88      0.93      0.91     70500\n",
            "           3       0.91      0.87      0.89     76177\n",
            "           4       0.19      0.88      0.32     15570\n",
            "           5       0.63      0.99      0.77     41295\n",
            "           6       0.94      0.95      0.95     68640\n",
            "           7       0.87      0.97      0.91     66455\n",
            "           8       0.97      0.65      0.78    104744\n",
            "           9       0.93      0.52      0.67    130527\n",
            "\n",
            "    accuracy                           0.82    720000\n",
            "   macro avg       0.82      0.87      0.81    720000\n",
            "weighted avg       0.89      0.82      0.84    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.848 Loss: 1.552\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     69680\n",
            "           1       0.92      0.97      0.95     78503\n",
            "           2       0.88      0.93      0.91     71468\n",
            "           3       0.91      0.87      0.89     77224\n",
            "           4       0.19      0.88      0.31     15651\n",
            "           5       0.64      0.99      0.77     42063\n",
            "           6       0.94      0.95      0.95     69598\n",
            "           7       0.87      0.97      0.91     67435\n",
            "           8       0.97      0.65      0.78    105944\n",
            "           9       0.93      0.52      0.67    132434\n",
            "\n",
            "    accuracy                           0.82    730000\n",
            "   macro avg       0.82      0.87      0.81    730000\n",
            "weighted avg       0.89      0.82      0.84    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.841 Loss: 1.557\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     70643\n",
            "           1       0.92      0.97      0.95     79588\n",
            "           2       0.89      0.93      0.91     72427\n",
            "           3       0.91      0.87      0.89     78297\n",
            "           4       0.19      0.88      0.31     15707\n",
            "           5       0.64      0.99      0.78     42796\n",
            "           6       0.94      0.95      0.95     70555\n",
            "           7       0.87      0.97      0.92     68398\n",
            "           8       0.97      0.65      0.78    107216\n",
            "           9       0.93      0.52      0.67    134373\n",
            "\n",
            "    accuracy                           0.83    740000\n",
            "   macro avg       0.82      0.87      0.81    740000\n",
            "weighted avg       0.89      0.83      0.84    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.840 Loss: 1.560\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     71613\n",
            "           1       0.92      0.97      0.95     80659\n",
            "           2       0.89      0.93      0.91     73384\n",
            "           3       0.91      0.87      0.89     79352\n",
            "           4       0.19      0.88      0.31     15764\n",
            "           5       0.64      0.99      0.78     43561\n",
            "           6       0.94      0.95      0.95     71498\n",
            "           7       0.87      0.97      0.92     69355\n",
            "           8       0.97      0.65      0.78    108498\n",
            "           9       0.93      0.52      0.67    136316\n",
            "\n",
            "    accuracy                           0.83    750000\n",
            "   macro avg       0.82      0.87      0.81    750000\n",
            "weighted avg       0.89      0.83      0.84    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.846 Loss: 1.554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     72584\n",
            "           1       0.92      0.97      0.95     81771\n",
            "           2       0.89      0.93      0.91     74360\n",
            "           3       0.91      0.87      0.89     80390\n",
            "           4       0.19      0.88      0.31     15822\n",
            "           5       0.64      0.99      0.78     44333\n",
            "           6       0.94      0.95      0.95     72446\n",
            "           7       0.87      0.97      0.92     70324\n",
            "           8       0.97      0.65      0.78    109705\n",
            "           9       0.93      0.52      0.67    138265\n",
            "\n",
            "    accuracy                           0.83    760000\n",
            "   macro avg       0.82      0.87      0.81    760000\n",
            "weighted avg       0.89      0.83      0.84    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.847 Loss: 1.551\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     73553\n",
            "           1       0.92      0.97      0.95     82880\n",
            "           2       0.89      0.93      0.91     75337\n",
            "           3       0.91      0.87      0.89     81460\n",
            "           4       0.18      0.88      0.30     15881\n",
            "           5       0.65      0.99      0.78     45067\n",
            "           6       0.94      0.95      0.95     73411\n",
            "           7       0.87      0.97      0.92     71309\n",
            "           8       0.97      0.65      0.78    110919\n",
            "           9       0.93      0.52      0.66    140183\n",
            "\n",
            "    accuracy                           0.83    770000\n",
            "   macro avg       0.82      0.87      0.81    770000\n",
            "weighted avg       0.90      0.83      0.84    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.839 Loss: 1.558\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     74516\n",
            "           1       0.92      0.97      0.95     83976\n",
            "           2       0.89      0.93      0.91     76269\n",
            "           3       0.91      0.87      0.89     82524\n",
            "           4       0.18      0.88      0.30     15930\n",
            "           5       0.65      0.99      0.78     45828\n",
            "           6       0.94      0.95      0.95     74355\n",
            "           7       0.87      0.97      0.92     72257\n",
            "           8       0.97      0.66      0.78    112166\n",
            "           9       0.93      0.52      0.66    142179\n",
            "\n",
            "    accuracy                           0.83    780000\n",
            "   macro avg       0.82      0.87      0.81    780000\n",
            "weighted avg       0.90      0.83      0.84    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.855 Loss: 1.550\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     75493\n",
            "           1       0.93      0.97      0.95     85094\n",
            "           2       0.89      0.93      0.91     77260\n",
            "           3       0.92      0.87      0.89     83574\n",
            "           4       0.18      0.88      0.30     16045\n",
            "           5       0.65      0.99      0.78     46606\n",
            "           6       0.94      0.95      0.95     75311\n",
            "           7       0.87      0.97      0.92     73240\n",
            "           8       0.97      0.66      0.78    113315\n",
            "           9       0.93      0.52      0.66    144062\n",
            "\n",
            "    accuracy                           0.83    790000\n",
            "   macro avg       0.82      0.87      0.81    790000\n",
            "weighted avg       0.90      0.83      0.84    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.852 Loss: 1.550\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     76464\n",
            "           1       0.93      0.97      0.95     86187\n",
            "           2       0.89      0.94      0.91     78224\n",
            "           3       0.92      0.87      0.89     84654\n",
            "           4       0.18      0.88      0.30     16156\n",
            "           5       0.65      0.99      0.79     47388\n",
            "           6       0.94      0.95      0.95     76278\n",
            "           7       0.87      0.97      0.92     74210\n",
            "           8       0.97      0.66      0.78    114482\n",
            "           9       0.93      0.52      0.66    145957\n",
            "\n",
            "    accuracy                           0.83    800000\n",
            "   macro avg       0.82      0.87      0.81    800000\n",
            "weighted avg       0.90      0.83      0.84    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.837 Loss: 1.557\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     77447\n",
            "           1       0.93      0.98      0.95     87231\n",
            "           2       0.89      0.94      0.91     79199\n",
            "           3       0.92      0.87      0.89     85746\n",
            "           4       0.18      0.88      0.30     16217\n",
            "           5       0.66      0.99      0.79     48116\n",
            "           6       0.94      0.95      0.95     77225\n",
            "           7       0.87      0.97      0.92     75150\n",
            "           8       0.97      0.66      0.78    115766\n",
            "           9       0.93      0.52      0.66    147903\n",
            "\n",
            "    accuracy                           0.83    810000\n",
            "   macro avg       0.82      0.87      0.81    810000\n",
            "weighted avg       0.90      0.83      0.84    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.853 Loss: 1.547\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     78428\n",
            "           1       0.93      0.98      0.95     88342\n",
            "           2       0.89      0.94      0.91     80198\n",
            "           3       0.92      0.87      0.90     86805\n",
            "           4       0.18      0.88      0.29     16321\n",
            "           5       0.66      0.99      0.79     48881\n",
            "           6       0.94      0.95      0.95     78178\n",
            "           7       0.87      0.97      0.92     76128\n",
            "           8       0.97      0.66      0.79    116942\n",
            "           9       0.93      0.52      0.66    149777\n",
            "\n",
            "    accuracy                           0.83    820000\n",
            "   macro avg       0.82      0.87      0.81    820000\n",
            "weighted avg       0.90      0.83      0.84    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.855 Loss: 1.544\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     79403\n",
            "           1       0.93      0.98      0.95     89443\n",
            "           2       0.89      0.94      0.91     81176\n",
            "           3       0.92      0.88      0.90     87863\n",
            "           4       0.18      0.88      0.29     16428\n",
            "           5       0.66      0.99      0.79     49656\n",
            "           6       0.94      0.95      0.95     79157\n",
            "           7       0.87      0.97      0.92     77114\n",
            "           8       0.97      0.66      0.79    118107\n",
            "           9       0.93      0.52      0.66    151653\n",
            "\n",
            "    accuracy                           0.83    830000\n",
            "   macro avg       0.82      0.87      0.81    830000\n",
            "weighted avg       0.90      0.83      0.84    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.839 Loss: 1.555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     80368\n",
            "           1       0.93      0.98      0.95     90488\n",
            "           2       0.89      0.94      0.91     82123\n",
            "           3       0.92      0.88      0.90     88918\n",
            "           4       0.17      0.88      0.29     16494\n",
            "           5       0.66      0.99      0.79     50428\n",
            "           6       0.94      0.95      0.95     80103\n",
            "           7       0.87      0.97      0.92     78087\n",
            "           8       0.97      0.66      0.79    119407\n",
            "           9       0.93      0.52      0.66    153584\n",
            "\n",
            "    accuracy                           0.83    840000\n",
            "   macro avg       0.82      0.87      0.81    840000\n",
            "weighted avg       0.90      0.83      0.84    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.857 Loss: 1.544\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     81352\n",
            "           1       0.93      0.98      0.95     91590\n",
            "           2       0.89      0.94      0.91     83118\n",
            "           3       0.92      0.88      0.90     89969\n",
            "           4       0.17      0.87      0.29     16591\n",
            "           5       0.67      0.98      0.79     51229\n",
            "           6       0.94      0.95      0.95     81048\n",
            "           7       0.87      0.97      0.92     79085\n",
            "           8       0.97      0.67      0.79    120538\n",
            "           9       0.93      0.52      0.66    155480\n",
            "\n",
            "    accuracy                           0.83    850000\n",
            "   macro avg       0.82      0.87      0.81    850000\n",
            "weighted avg       0.90      0.83      0.84    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.853 Loss: 1.546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     82320\n",
            "           1       0.93      0.98      0.95     92683\n",
            "           2       0.89      0.94      0.91     84084\n",
            "           3       0.92      0.88      0.90     91048\n",
            "           4       0.17      0.88      0.29     16705\n",
            "           5       0.67      0.98      0.80     51991\n",
            "           6       0.94      0.95      0.95     82020\n",
            "           7       0.87      0.97      0.92     80073\n",
            "           8       0.97      0.67      0.79    121734\n",
            "           9       0.93      0.51      0.66    157342\n",
            "\n",
            "    accuracy                           0.83    860000\n",
            "   macro avg       0.82      0.87      0.81    860000\n",
            "weighted avg       0.90      0.83      0.84    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.848 Loss: 1.549\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     83298\n",
            "           1       0.93      0.98      0.95     93758\n",
            "           2       0.89      0.94      0.91     85049\n",
            "           3       0.92      0.88      0.90     92123\n",
            "           4       0.17      0.87      0.29     16794\n",
            "           5       0.67      0.98      0.80     52757\n",
            "           6       0.94      0.95      0.95     82971\n",
            "           7       0.88      0.97      0.92     81052\n",
            "           8       0.97      0.67      0.79    122964\n",
            "           9       0.93      0.51      0.66    159234\n",
            "\n",
            "    accuracy                           0.83    870000\n",
            "   macro avg       0.82      0.87      0.81    870000\n",
            "weighted avg       0.90      0.83      0.84    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.864 Loss: 1.543\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     84279\n",
            "           1       0.93      0.98      0.95     94867\n",
            "           2       0.89      0.94      0.91     86032\n",
            "           3       0.92      0.88      0.90     93185\n",
            "           4       0.17      0.87      0.29     16948\n",
            "           5       0.67      0.98      0.80     53566\n",
            "           6       0.95      0.95      0.95     83929\n",
            "           7       0.88      0.97      0.92     82036\n",
            "           8       0.97      0.67      0.79    124093\n",
            "           9       0.93      0.51      0.66    161065\n",
            "\n",
            "    accuracy                           0.83    880000\n",
            "   macro avg       0.83      0.87      0.81    880000\n",
            "weighted avg       0.90      0.83      0.84    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.864 Loss: 1.542\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     85259\n",
            "           1       0.93      0.98      0.95     95967\n",
            "           2       0.89      0.94      0.91     87014\n",
            "           3       0.92      0.88      0.90     94241\n",
            "           4       0.17      0.88      0.29     17140\n",
            "           5       0.67      0.98      0.80     54334\n",
            "           6       0.95      0.95      0.95     84918\n",
            "           7       0.88      0.97      0.92     83016\n",
            "           8       0.97      0.67      0.79    125248\n",
            "           9       0.93      0.52      0.66    162863\n",
            "\n",
            "    accuracy                           0.83    890000\n",
            "   macro avg       0.83      0.87      0.81    890000\n",
            "weighted avg       0.90      0.83      0.84    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.846 Loss: 1.549\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     86227\n",
            "           1       0.93      0.98      0.95     97042\n",
            "           2       0.89      0.94      0.91     87985\n",
            "           3       0.92      0.88      0.90     95341\n",
            "           4       0.17      0.87      0.29     17222\n",
            "           5       0.68      0.98      0.80     55103\n",
            "           6       0.95      0.95      0.95     85873\n",
            "           7       0.88      0.97      0.92     83979\n",
            "           8       0.97      0.67      0.79    126464\n",
            "           9       0.93      0.51      0.66    164764\n",
            "\n",
            "    accuracy                           0.83    900000\n",
            "   macro avg       0.83      0.87      0.81    900000\n",
            "weighted avg       0.90      0.83      0.84    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Mnist_nn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.linear1 = nn.Linear(28*28, 100)\n",
        "    self.drop1 = nn.Dropout(p=0.7)\n",
        "    self.drop2 = nn.Dropout(p=0.5)\n",
        "    self.linear2 = nn.Linear(100, 100)\n",
        "    self.batch = nn.BatchNorm1d(100)\n",
        "    self.act = nn.Sigmoid()\n",
        "    self.linear3 = nn.Linear(100,10)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.flat(x)\n",
        "    out = self.linear1(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop1(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.batch(out)\n",
        "    out = self.drop2(out)\n",
        "    out = self.act2(out)\n",
        "    out = self.linear3(out)\n",
        "    return out\n",
        "model = Mnist_nn()"
      ],
      "metadata": {
        "id": "kyWeEG8mhAbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "jpHNCLYMhARN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT8Q-6dkgpGr",
        "outputId": "059bd866-b226-46d4-c19e-1582902113d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.872 Loss: 0.645\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95      1054\n",
            "           1       0.90      0.98      0.94      1045\n",
            "           2       0.87      0.89      0.88      1014\n",
            "           3       0.90      0.84      0.87      1075\n",
            "           4       0.89      0.89      0.89       979\n",
            "           5       0.65      0.95      0.77       608\n",
            "           6       0.91      0.94      0.92       924\n",
            "           7       0.79      0.95      0.86       855\n",
            "           8       0.92      0.68      0.78      1314\n",
            "           9       0.89      0.79      0.84      1132\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.88      0.87     10000\n",
            "weighted avg       0.88      0.87      0.87     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.873 Loss: 0.522\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      2024\n",
            "           1       0.90      0.98      0.94      2089\n",
            "           2       0.89      0.87      0.88      2115\n",
            "           3       0.90      0.85      0.87      2123\n",
            "           4       0.87      0.92      0.89      1865\n",
            "           5       0.62      0.96      0.75      1155\n",
            "           6       0.92      0.94      0.93      1888\n",
            "           7       0.80      0.96      0.87      1701\n",
            "           8       0.94      0.66      0.78      2764\n",
            "           9       0.89      0.79      0.84      2276\n",
            "\n",
            "    accuracy                           0.87     20000\n",
            "   macro avg       0.87      0.89      0.87     20000\n",
            "weighted avg       0.89      0.87      0.87     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.882 Loss: 0.458\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      3040\n",
            "           1       0.90      0.98      0.94      3102\n",
            "           2       0.89      0.88      0.88      3144\n",
            "           3       0.90      0.86      0.88      3172\n",
            "           4       0.88      0.92      0.90      2825\n",
            "           5       0.61      0.97      0.75      1700\n",
            "           6       0.93      0.94      0.93      2854\n",
            "           7       0.82      0.96      0.88      2620\n",
            "           8       0.94      0.65      0.77      4222\n",
            "           9       0.90      0.82      0.85      3321\n",
            "\n",
            "    accuracy                           0.88     30000\n",
            "   macro avg       0.87      0.89      0.87     30000\n",
            "weighted avg       0.89      0.88      0.88     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.898 Loss: 0.397\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      4077\n",
            "           1       0.91      0.98      0.94      4183\n",
            "           2       0.90      0.88      0.89      4225\n",
            "           3       0.90      0.86      0.88      4264\n",
            "           4       0.89      0.92      0.90      3781\n",
            "           5       0.62      0.97      0.76      2270\n",
            "           6       0.94      0.93      0.94      3834\n",
            "           7       0.82      0.96      0.89      3520\n",
            "           8       0.95      0.67      0.79      5464\n",
            "           9       0.90      0.83      0.86      4382\n",
            "\n",
            "    accuracy                           0.88     40000\n",
            "   macro avg       0.88      0.89      0.88     40000\n",
            "weighted avg       0.89      0.88      0.88     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.903 Loss: 0.379\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      5078\n",
            "           1       0.91      0.98      0.95      5293\n",
            "           2       0.91      0.88      0.89      5346\n",
            "           3       0.91      0.85      0.88      5363\n",
            "           4       0.89      0.93      0.91      4703\n",
            "           5       0.63      0.98      0.77      2886\n",
            "           6       0.94      0.93      0.94      4813\n",
            "           7       0.83      0.97      0.89      4388\n",
            "           8       0.94      0.69      0.80      6665\n",
            "           9       0.90      0.83      0.86      5465\n",
            "\n",
            "    accuracy                           0.89     50000\n",
            "   macro avg       0.88      0.90      0.88     50000\n",
            "weighted avg       0.90      0.89      0.89     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.899 Loss: 0.366\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      6128\n",
            "           1       0.92      0.98      0.95      6363\n",
            "           2       0.91      0.88      0.89      6438\n",
            "           3       0.91      0.85      0.88      6470\n",
            "           4       0.88      0.93      0.91      5598\n",
            "           5       0.64      0.98      0.77      3506\n",
            "           6       0.94      0.94      0.94      5768\n",
            "           7       0.83      0.97      0.89      5284\n",
            "           8       0.94      0.70      0.80      7890\n",
            "           9       0.90      0.83      0.87      6555\n",
            "\n",
            "    accuracy                           0.89     60000\n",
            "   macro avg       0.89      0.90      0.89     60000\n",
            "weighted avg       0.90      0.89      0.89     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.912 Loss: 0.333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.95      7158\n",
            "           1       0.92      0.98      0.95      7488\n",
            "           2       0.92      0.88      0.90      7510\n",
            "           3       0.91      0.85      0.88      7619\n",
            "           4       0.89      0.93      0.91      6561\n",
            "           5       0.65      0.98      0.78      4139\n",
            "           6       0.94      0.94      0.94      6727\n",
            "           7       0.83      0.97      0.90      6195\n",
            "           8       0.94      0.72      0.81      8982\n",
            "           9       0.91      0.84      0.87      7621\n",
            "\n",
            "    accuracy                           0.89     70000\n",
            "   macro avg       0.89      0.90      0.89     70000\n",
            "weighted avg       0.90      0.89      0.89     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.911 Loss: 0.338\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      8172\n",
            "           1       0.93      0.98      0.95      8584\n",
            "           2       0.92      0.88      0.90      8614\n",
            "           3       0.92      0.85      0.88      8744\n",
            "           4       0.89      0.93      0.91      7482\n",
            "           5       0.66      0.98      0.79      4793\n",
            "           6       0.94      0.94      0.94      7699\n",
            "           7       0.83      0.97      0.90      7065\n",
            "           8       0.94      0.73      0.82     10111\n",
            "           9       0.91      0.84      0.87      8736\n",
            "\n",
            "    accuracy                           0.89     80000\n",
            "   macro avg       0.89      0.90      0.89     80000\n",
            "weighted avg       0.90      0.89      0.89     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.906 Loss: 0.339\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      9205\n",
            "           1       0.92      0.98      0.95      9640\n",
            "           2       0.92      0.88      0.90      9665\n",
            "           3       0.92      0.84      0.88      9921\n",
            "           4       0.89      0.94      0.91      8372\n",
            "           5       0.66      0.98      0.79      5437\n",
            "           6       0.94      0.94      0.94      8656\n",
            "           7       0.84      0.97      0.90      7986\n",
            "           8       0.94      0.73      0.82     11246\n",
            "           9       0.91      0.84      0.87      9872\n",
            "\n",
            "    accuracy                           0.90     90000\n",
            "   macro avg       0.89      0.90      0.89     90000\n",
            "weighted avg       0.90      0.90      0.90     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.923 Loss: 0.290\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     10218\n",
            "           1       0.93      0.98      0.95     10769\n",
            "           2       0.92      0.89      0.90     10731\n",
            "           3       0.92      0.84      0.88     11064\n",
            "           4       0.89      0.94      0.91      9312\n",
            "           5       0.67      0.98      0.80      6110\n",
            "           6       0.94      0.94      0.94      9626\n",
            "           7       0.84      0.97      0.90      8921\n",
            "           8       0.94      0.74      0.83     12297\n",
            "           9       0.91      0.84      0.88     10952\n",
            "\n",
            "    accuracy                           0.90    100000\n",
            "   macro avg       0.90      0.91      0.90    100000\n",
            "weighted avg       0.91      0.90      0.90    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.919 Loss: 0.292\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     11234\n",
            "           1       0.93      0.98      0.95     11887\n",
            "           2       0.92      0.89      0.91     11830\n",
            "           3       0.93      0.84      0.88     12241\n",
            "           4       0.89      0.94      0.91     10224\n",
            "           5       0.68      0.98      0.80      6769\n",
            "           6       0.95      0.94      0.94     10603\n",
            "           7       0.85      0.97      0.91      9876\n",
            "           8       0.94      0.76      0.84     13295\n",
            "           9       0.92      0.84      0.88     12041\n",
            "\n",
            "    accuracy                           0.90    110000\n",
            "   macro avg       0.90      0.91      0.90    110000\n",
            "weighted avg       0.91      0.90      0.90    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.919 Loss: 0.286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     12241\n",
            "           1       0.93      0.98      0.95     12997\n",
            "           2       0.93      0.89      0.91     12883\n",
            "           3       0.93      0.84      0.88     13428\n",
            "           4       0.89      0.94      0.91     11129\n",
            "           5       0.68      0.98      0.81      7460\n",
            "           6       0.95      0.94      0.94     11563\n",
            "           7       0.85      0.97      0.91     10820\n",
            "           8       0.94      0.76      0.84     14341\n",
            "           9       0.92      0.85      0.88     13138\n",
            "\n",
            "    accuracy                           0.90    120000\n",
            "   macro avg       0.90      0.91      0.90    120000\n",
            "weighted avg       0.91      0.90      0.90    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.922 Loss: 0.263\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     13269\n",
            "           1       0.94      0.98      0.96     14130\n",
            "           2       0.93      0.89      0.91     13953\n",
            "           3       0.93      0.84      0.88     14558\n",
            "           4       0.89      0.94      0.91     12009\n",
            "           5       0.69      0.98      0.81      8155\n",
            "           6       0.95      0.94      0.94     12516\n",
            "           7       0.86      0.97      0.91     11782\n",
            "           8       0.93      0.77      0.84     15351\n",
            "           9       0.92      0.85      0.88     14277\n",
            "\n",
            "    accuracy                           0.90    130000\n",
            "   macro avg       0.90      0.91      0.90    130000\n",
            "weighted avg       0.91      0.90      0.90    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.924 Loss: 0.264\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     14255\n",
            "           1       0.94      0.98      0.96     15260\n",
            "           2       0.93      0.89      0.91     15057\n",
            "           3       0.93      0.84      0.88     15720\n",
            "           4       0.89      0.94      0.91     12926\n",
            "           5       0.70      0.98      0.81      8836\n",
            "           6       0.95      0.94      0.94     13497\n",
            "           7       0.86      0.97      0.91     12758\n",
            "           8       0.93      0.78      0.85     16344\n",
            "           9       0.92      0.85      0.88     15347\n",
            "\n",
            "    accuracy                           0.90    140000\n",
            "   macro avg       0.90      0.91      0.90    140000\n",
            "weighted avg       0.91      0.90      0.90    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.923 Loss: 0.266\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     15249\n",
            "           1       0.94      0.98      0.96     16371\n",
            "           2       0.93      0.89      0.91     16119\n",
            "           3       0.93      0.84      0.88     16876\n",
            "           4       0.89      0.94      0.91     13802\n",
            "           5       0.70      0.98      0.82      9538\n",
            "           6       0.95      0.94      0.94     14479\n",
            "           7       0.86      0.97      0.91     13719\n",
            "           8       0.93      0.78      0.85     17393\n",
            "           9       0.92      0.85      0.88     16454\n",
            "\n",
            "    accuracy                           0.91    150000\n",
            "   macro avg       0.90      0.91      0.90    150000\n",
            "weighted avg       0.91      0.91      0.91    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.930 Loss: 0.258\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96     16260\n",
            "           1       0.94      0.98      0.96     17500\n",
            "           2       0.93      0.90      0.91     17174\n",
            "           3       0.93      0.84      0.88     18025\n",
            "           4       0.89      0.95      0.92     14741\n",
            "           5       0.71      0.98      0.82     10245\n",
            "           6       0.95      0.94      0.95     15446\n",
            "           7       0.87      0.97      0.91     14706\n",
            "           8       0.93      0.79      0.85     18355\n",
            "           9       0.92      0.85      0.89     17548\n",
            "\n",
            "    accuracy                           0.91    160000\n",
            "   macro avg       0.90      0.91      0.91    160000\n",
            "weighted avg       0.91      0.91      0.91    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.926 Loss: 0.258\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     17262\n",
            "           1       0.94      0.98      0.96     18628\n",
            "           2       0.93      0.90      0.91     18238\n",
            "           3       0.94      0.84      0.88     19232\n",
            "           4       0.89      0.95      0.92     15637\n",
            "           5       0.71      0.98      0.82     10940\n",
            "           6       0.95      0.94      0.95     16412\n",
            "           7       0.87      0.97      0.92     15663\n",
            "           8       0.93      0.80      0.86     19338\n",
            "           9       0.93      0.85      0.89     18650\n",
            "\n",
            "    accuracy                           0.91    170000\n",
            "   macro avg       0.91      0.91      0.91    170000\n",
            "weighted avg       0.91      0.91      0.91    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.928 Loss: 0.253\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     18264\n",
            "           1       0.94      0.98      0.96     19751\n",
            "           2       0.93      0.90      0.92     19282\n",
            "           3       0.94      0.84      0.88     20395\n",
            "           4       0.89      0.95      0.92     16555\n",
            "           5       0.71      0.98      0.83     11651\n",
            "           6       0.95      0.94      0.95     17371\n",
            "           7       0.87      0.97      0.92     16641\n",
            "           8       0.93      0.80      0.86     20357\n",
            "           9       0.93      0.85      0.89     19733\n",
            "\n",
            "    accuracy                           0.91    180000\n",
            "   macro avg       0.91      0.92      0.91    180000\n",
            "weighted avg       0.92      0.91      0.91    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.931 Loss: 0.240\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     19280\n",
            "           1       0.95      0.98      0.96     20886\n",
            "           2       0.93      0.90      0.92     20317\n",
            "           3       0.94      0.84      0.88     21553\n",
            "           4       0.89      0.95      0.92     17468\n",
            "           5       0.72      0.98      0.83     12384\n",
            "           6       0.95      0.94      0.95     18348\n",
            "           7       0.87      0.97      0.92     17623\n",
            "           8       0.93      0.81      0.86     21290\n",
            "           9       0.93      0.85      0.89     20851\n",
            "\n",
            "    accuracy                           0.91    190000\n",
            "   macro avg       0.91      0.92      0.91    190000\n",
            "weighted avg       0.92      0.91      0.91    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.928 Loss: 0.250\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     20286\n",
            "           1       0.95      0.98      0.96     22010\n",
            "           2       0.93      0.90      0.92     21407\n",
            "           3       0.94      0.83      0.88     22759\n",
            "           4       0.89      0.95      0.92     18382\n",
            "           5       0.72      0.98      0.83     13094\n",
            "           6       0.95      0.94      0.95     19317\n",
            "           7       0.88      0.97      0.92     18581\n",
            "           8       0.93      0.81      0.86     22240\n",
            "           9       0.93      0.85      0.89     21924\n",
            "\n",
            "    accuracy                           0.91    200000\n",
            "   macro avg       0.91      0.92      0.91    200000\n",
            "weighted avg       0.92      0.91      0.91    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.927 Loss: 0.255\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     21293\n",
            "           1       0.95      0.98      0.96     23126\n",
            "           2       0.94      0.90      0.92     22464\n",
            "           3       0.94      0.83      0.88     23963\n",
            "           4       0.89      0.95      0.92     19274\n",
            "           5       0.72      0.98      0.83     13784\n",
            "           6       0.95      0.94      0.95     20267\n",
            "           7       0.88      0.97      0.92     19572\n",
            "           8       0.93      0.82      0.87     23214\n",
            "           9       0.93      0.85      0.89     23043\n",
            "\n",
            "    accuracy                           0.91    210000\n",
            "   macro avg       0.91      0.92      0.91    210000\n",
            "weighted avg       0.92      0.91      0.91    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.933 Loss: 0.240\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     22307\n",
            "           1       0.95      0.98      0.96     24270\n",
            "           2       0.94      0.90      0.92     23521\n",
            "           3       0.94      0.83      0.89     25128\n",
            "           4       0.89      0.95      0.92     20193\n",
            "           5       0.73      0.98      0.84     14493\n",
            "           6       0.95      0.94      0.95     21244\n",
            "           7       0.88      0.97      0.92     20561\n",
            "           8       0.92      0.82      0.87     24143\n",
            "           9       0.93      0.86      0.89     24140\n",
            "\n",
            "    accuracy                           0.91    220000\n",
            "   macro avg       0.91      0.92      0.91    220000\n",
            "weighted avg       0.92      0.91      0.91    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.933 Loss: 0.234\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     23308\n",
            "           1       0.95      0.98      0.96     25411\n",
            "           2       0.94      0.90      0.92     24594\n",
            "           3       0.94      0.83      0.89     26301\n",
            "           4       0.89      0.95      0.92     21095\n",
            "           5       0.73      0.98      0.84     15247\n",
            "           6       0.95      0.94      0.95     22222\n",
            "           7       0.88      0.97      0.92     21512\n",
            "           8       0.92      0.82      0.87     25077\n",
            "           9       0.93      0.86      0.89     25233\n",
            "\n",
            "    accuracy                           0.91    230000\n",
            "   macro avg       0.91      0.92      0.91    230000\n",
            "weighted avg       0.92      0.91      0.91    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.929 Loss: 0.241\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     24321\n",
            "           1       0.95      0.98      0.96     26512\n",
            "           2       0.94      0.91      0.92     25659\n",
            "           3       0.94      0.83      0.89     27476\n",
            "           4       0.89      0.95      0.92     21980\n",
            "           5       0.73      0.98      0.84     15971\n",
            "           6       0.95      0.94      0.95     23196\n",
            "           7       0.88      0.97      0.92     22488\n",
            "           8       0.92      0.83      0.87     26063\n",
            "           9       0.93      0.86      0.89     26334\n",
            "\n",
            "    accuracy                           0.91    240000\n",
            "   macro avg       0.91      0.92      0.91    240000\n",
            "weighted avg       0.92      0.91      0.91    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.933 Loss: 0.240\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96     25325\n",
            "           1       0.95      0.98      0.96     27665\n",
            "           2       0.94      0.91      0.92     26710\n",
            "           3       0.95      0.83      0.89     28658\n",
            "           4       0.89      0.95      0.92     22897\n",
            "           5       0.74      0.98      0.84     16701\n",
            "           6       0.95      0.94      0.95     24172\n",
            "           7       0.88      0.97      0.92     23466\n",
            "           8       0.92      0.83      0.87     26961\n",
            "           9       0.93      0.86      0.89     27445\n",
            "\n",
            "    accuracy                           0.92    250000\n",
            "   macro avg       0.91      0.92      0.91    250000\n",
            "weighted avg       0.92      0.92      0.92    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.933 Loss: 0.229\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     26334\n",
            "           1       0.95      0.98      0.96     28799\n",
            "           2       0.94      0.91      0.92     27803\n",
            "           3       0.95      0.83      0.89     29835\n",
            "           4       0.89      0.95      0.92     23812\n",
            "           5       0.74      0.98      0.84     17426\n",
            "           6       0.95      0.95      0.95     25148\n",
            "           7       0.88      0.97      0.92     24457\n",
            "           8       0.92      0.84      0.88     27871\n",
            "           9       0.93      0.86      0.89     28515\n",
            "\n",
            "    accuracy                           0.92    260000\n",
            "   macro avg       0.91      0.92      0.91    260000\n",
            "weighted avg       0.92      0.92      0.92    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.927 Loss: 0.244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     27339\n",
            "           1       0.95      0.98      0.96     29905\n",
            "           2       0.94      0.91      0.92     28851\n",
            "           3       0.95      0.83      0.89     31014\n",
            "           4       0.89      0.95      0.92     24667\n",
            "           5       0.74      0.98      0.85     18144\n",
            "           6       0.95      0.94      0.95     26124\n",
            "           7       0.89      0.97      0.92     25463\n",
            "           8       0.92      0.84      0.88     28861\n",
            "           9       0.93      0.86      0.89     29632\n",
            "\n",
            "    accuracy                           0.92    270000\n",
            "   macro avg       0.91      0.92      0.91    270000\n",
            "weighted avg       0.92      0.92      0.92    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.933 Loss: 0.231\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     28334\n",
            "           1       0.95      0.98      0.96     31042\n",
            "           2       0.94      0.91      0.92     29942\n",
            "           3       0.95      0.83      0.89     32169\n",
            "           4       0.89      0.95      0.92     25547\n",
            "           5       0.74      0.98      0.85     18871\n",
            "           6       0.95      0.95      0.95     27094\n",
            "           7       0.89      0.97      0.93     26470\n",
            "           8       0.92      0.84      0.88     29787\n",
            "           9       0.93      0.86      0.89     30744\n",
            "\n",
            "    accuracy                           0.92    280000\n",
            "   macro avg       0.91      0.92      0.92    280000\n",
            "weighted avg       0.92      0.92      0.92    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.938 Loss: 0.212\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     29332\n",
            "           1       0.95      0.98      0.96     32170\n",
            "           2       0.94      0.91      0.92     31038\n",
            "           3       0.95      0.83      0.89     33301\n",
            "           4       0.89      0.96      0.92     26468\n",
            "           5       0.75      0.98      0.85     19619\n",
            "           6       0.96      0.94      0.95     28085\n",
            "           7       0.89      0.97      0.93     27465\n",
            "           8       0.92      0.84      0.88     30720\n",
            "           9       0.93      0.86      0.90     31802\n",
            "\n",
            "    accuracy                           0.92    290000\n",
            "   macro avg       0.92      0.92      0.92    290000\n",
            "weighted avg       0.92      0.92      0.92    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.935 Loss: 0.222\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     30334\n",
            "           1       0.95      0.98      0.97     33297\n",
            "           2       0.94      0.91      0.92     32090\n",
            "           3       0.95      0.83      0.89     34456\n",
            "           4       0.89      0.96      0.92     27369\n",
            "           5       0.75      0.98      0.85     20349\n",
            "           6       0.96      0.94      0.95     29067\n",
            "           7       0.89      0.97      0.93     28474\n",
            "           8       0.92      0.85      0.88     31689\n",
            "           9       0.93      0.86      0.90     32875\n",
            "\n",
            "    accuracy                           0.92    300000\n",
            "   macro avg       0.92      0.92      0.92    300000\n",
            "weighted avg       0.92      0.92      0.92    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.933 Loss: 0.228\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     31346\n",
            "           1       0.96      0.98      0.97     34436\n",
            "           2       0.94      0.91      0.93     33154\n",
            "           3       0.95      0.83      0.89     35625\n",
            "           4       0.89      0.96      0.92     28255\n",
            "           5       0.75      0.98      0.85     21084\n",
            "           6       0.96      0.94      0.95     30055\n",
            "           7       0.89      0.96      0.93     29479\n",
            "           8       0.92      0.85      0.88     32572\n",
            "           9       0.94      0.86      0.90     33994\n",
            "\n",
            "    accuracy                           0.92    310000\n",
            "   macro avg       0.92      0.92      0.92    310000\n",
            "weighted avg       0.92      0.92      0.92    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.934 Loss: 0.226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     32347\n",
            "           1       0.96      0.98      0.97     35557\n",
            "           2       0.94      0.91      0.93     34242\n",
            "           3       0.95      0.83      0.89     36806\n",
            "           4       0.89      0.96      0.92     29143\n",
            "           5       0.75      0.98      0.85     21814\n",
            "           6       0.96      0.94      0.95     31040\n",
            "           7       0.89      0.96      0.93     30466\n",
            "           8       0.92      0.85      0.88     33490\n",
            "           9       0.94      0.86      0.90     35095\n",
            "\n",
            "    accuracy                           0.92    320000\n",
            "   macro avg       0.92      0.92      0.92    320000\n",
            "weighted avg       0.92      0.92      0.92    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.940 Loss: 0.209\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     33353\n",
            "           1       0.96      0.98      0.97     36664\n",
            "           2       0.94      0.91      0.93     35288\n",
            "           3       0.95      0.84      0.89     37914\n",
            "           4       0.89      0.96      0.92     30043\n",
            "           5       0.75      0.98      0.85     22585\n",
            "           6       0.96      0.94      0.95     32030\n",
            "           7       0.89      0.96      0.93     31464\n",
            "           8       0.92      0.85      0.88     34476\n",
            "           9       0.94      0.86      0.90     36183\n",
            "\n",
            "    accuracy                           0.92    330000\n",
            "   macro avg       0.92      0.92      0.92    330000\n",
            "weighted avg       0.92      0.92      0.92    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.935 Loss: 0.214\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     34366\n",
            "           1       0.96      0.98      0.97     37798\n",
            "           2       0.94      0.91      0.93     36352\n",
            "           3       0.95      0.84      0.89     39052\n",
            "           4       0.89      0.96      0.92     30939\n",
            "           5       0.76      0.98      0.86     23344\n",
            "           6       0.96      0.94      0.95     33001\n",
            "           7       0.90      0.96      0.93     32453\n",
            "           8       0.91      0.86      0.88     35379\n",
            "           9       0.94      0.86      0.90     37316\n",
            "\n",
            "    accuracy                           0.92    340000\n",
            "   macro avg       0.92      0.92      0.92    340000\n",
            "weighted avg       0.92      0.92      0.92    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.937 Loss: 0.208\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     35361\n",
            "           1       0.96      0.98      0.97     38924\n",
            "           2       0.94      0.91      0.93     37405\n",
            "           3       0.95      0.84      0.89     40245\n",
            "           4       0.89      0.96      0.92     31858\n",
            "           5       0.76      0.98      0.86     24107\n",
            "           6       0.96      0.94      0.95     33999\n",
            "           7       0.90      0.96      0.93     33432\n",
            "           8       0.91      0.86      0.89     36289\n",
            "           9       0.94      0.86      0.90     38380\n",
            "\n",
            "    accuracy                           0.92    350000\n",
            "   macro avg       0.92      0.92      0.92    350000\n",
            "weighted avg       0.93      0.92      0.92    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.936 Loss: 0.220\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     36374\n",
            "           1       0.96      0.98      0.97     40050\n",
            "           2       0.94      0.91      0.93     38466\n",
            "           3       0.95      0.84      0.89     41414\n",
            "           4       0.89      0.96      0.92     32764\n",
            "           5       0.76      0.98      0.86     24851\n",
            "           6       0.96      0.94      0.95     34960\n",
            "           7       0.90      0.96      0.93     34438\n",
            "           8       0.91      0.86      0.89     37214\n",
            "           9       0.94      0.86      0.90     39469\n",
            "\n",
            "    accuracy                           0.92    360000\n",
            "   macro avg       0.92      0.93      0.92    360000\n",
            "weighted avg       0.93      0.92      0.92    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.933 Loss: 0.220\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     37384\n",
            "           1       0.96      0.98      0.97     41198\n",
            "           2       0.94      0.91      0.93     39509\n",
            "           3       0.95      0.84      0.89     42580\n",
            "           4       0.89      0.96      0.92     33623\n",
            "           5       0.76      0.98      0.86     25615\n",
            "           6       0.96      0.94      0.95     35939\n",
            "           7       0.90      0.96      0.93     35456\n",
            "           8       0.91      0.86      0.89     38089\n",
            "           9       0.94      0.86      0.90     40607\n",
            "\n",
            "    accuracy                           0.92    370000\n",
            "   macro avg       0.92      0.93      0.92    370000\n",
            "weighted avg       0.93      0.92      0.92    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.936 Loss: 0.211\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     38387\n",
            "           1       0.96      0.98      0.97     42323\n",
            "           2       0.95      0.91      0.93     40592\n",
            "           3       0.95      0.84      0.89     43732\n",
            "           4       0.89      0.96      0.92     34515\n",
            "           5       0.76      0.98      0.86     26370\n",
            "           6       0.96      0.94      0.95     36919\n",
            "           7       0.90      0.96      0.93     36466\n",
            "           8       0.91      0.87      0.89     38983\n",
            "           9       0.94      0.86      0.90     41713\n",
            "\n",
            "    accuracy                           0.92    380000\n",
            "   macro avg       0.92      0.93      0.92    380000\n",
            "weighted avg       0.93      0.92      0.92    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.936 Loss: 0.209\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     39399\n",
            "           1       0.96      0.98      0.97     43442\n",
            "           2       0.95      0.91      0.93     41663\n",
            "           3       0.95      0.84      0.89     44869\n",
            "           4       0.89      0.96      0.92     35400\n",
            "           5       0.77      0.98      0.86     27132\n",
            "           6       0.96      0.94      0.95     37889\n",
            "           7       0.90      0.96      0.93     37476\n",
            "           8       0.91      0.87      0.89     39909\n",
            "           9       0.94      0.86      0.90     42821\n",
            "\n",
            "    accuracy                           0.92    390000\n",
            "   macro avg       0.92      0.93      0.92    390000\n",
            "weighted avg       0.93      0.92      0.92    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.939 Loss: 0.208\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     40403\n",
            "           1       0.96      0.98      0.97     44582\n",
            "           2       0.95      0.91      0.93     42740\n",
            "           3       0.95      0.84      0.89     46019\n",
            "           4       0.89      0.96      0.92     36317\n",
            "           5       0.77      0.98      0.86     27873\n",
            "           6       0.96      0.94      0.95     38865\n",
            "           7       0.90      0.96      0.93     38492\n",
            "           8       0.91      0.87      0.89     40801\n",
            "           9       0.94      0.86      0.90     43908\n",
            "\n",
            "    accuracy                           0.92    400000\n",
            "   macro avg       0.92      0.93      0.92    400000\n",
            "weighted avg       0.93      0.92      0.92    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.940 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     41397\n",
            "           1       0.96      0.98      0.97     45718\n",
            "           2       0.95      0.91      0.93     43821\n",
            "           3       0.95      0.84      0.89     47164\n",
            "           4       0.89      0.96      0.92     37225\n",
            "           5       0.77      0.98      0.86     28660\n",
            "           6       0.96      0.94      0.95     39850\n",
            "           7       0.90      0.96      0.93     39497\n",
            "           8       0.91      0.87      0.89     41697\n",
            "           9       0.94      0.86      0.90     44971\n",
            "\n",
            "    accuracy                           0.92    410000\n",
            "   macro avg       0.92      0.93      0.92    410000\n",
            "weighted avg       0.93      0.92      0.92    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.941 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     42413\n",
            "           1       0.96      0.98      0.97     46852\n",
            "           2       0.95      0.91      0.93     44867\n",
            "           3       0.95      0.84      0.89     48300\n",
            "           4       0.89      0.96      0.92     38147\n",
            "           5       0.77      0.98      0.86     29443\n",
            "           6       0.96      0.95      0.95     40820\n",
            "           7       0.90      0.96      0.93     40500\n",
            "           8       0.91      0.87      0.89     42614\n",
            "           9       0.94      0.86      0.90     46044\n",
            "\n",
            "    accuracy                           0.92    420000\n",
            "   macro avg       0.92      0.93      0.92    420000\n",
            "weighted avg       0.93      0.92      0.92    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.939 Loss: 0.210\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     43415\n",
            "           1       0.96      0.98      0.97     47996\n",
            "           2       0.95      0.92      0.93     45930\n",
            "           3       0.95      0.84      0.89     49468\n",
            "           4       0.89      0.96      0.92     39058\n",
            "           5       0.77      0.98      0.87     30209\n",
            "           6       0.96      0.95      0.95     41785\n",
            "           7       0.90      0.96      0.93     41515\n",
            "           8       0.91      0.88      0.89     43502\n",
            "           9       0.94      0.87      0.90     47122\n",
            "\n",
            "    accuracy                           0.92    430000\n",
            "   macro avg       0.92      0.93      0.92    430000\n",
            "weighted avg       0.93      0.92      0.92    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.941 Loss: 0.204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     44402\n",
            "           1       0.96      0.98      0.97     49126\n",
            "           2       0.95      0.91      0.93     47033\n",
            "           3       0.95      0.84      0.89     50618\n",
            "           4       0.89      0.96      0.92     39974\n",
            "           5       0.78      0.98      0.87     30980\n",
            "           6       0.96      0.95      0.95     42758\n",
            "           7       0.91      0.96      0.93     42527\n",
            "           8       0.91      0.88      0.89     44396\n",
            "           9       0.94      0.87      0.90     48186\n",
            "\n",
            "    accuracy                           0.92    440000\n",
            "   macro avg       0.92      0.93      0.92    440000\n",
            "weighted avg       0.93      0.92      0.92    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.939 Loss: 0.202\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     45410\n",
            "           1       0.96      0.98      0.97     50252\n",
            "           2       0.95      0.92      0.93     48101\n",
            "           3       0.96      0.84      0.89     51782\n",
            "           4       0.89      0.96      0.92     40904\n",
            "           5       0.78      0.98      0.87     31747\n",
            "           6       0.96      0.95      0.95     43719\n",
            "           7       0.91      0.96      0.93     43541\n",
            "           8       0.91      0.88      0.89     45295\n",
            "           9       0.94      0.87      0.90     49249\n",
            "\n",
            "    accuracy                           0.92    450000\n",
            "   macro avg       0.92      0.93      0.92    450000\n",
            "weighted avg       0.93      0.92      0.92    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.939 Loss: 0.203\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97     46413\n",
            "           1       0.96      0.98      0.97     51397\n",
            "           2       0.95      0.92      0.93     49174\n",
            "           3       0.96      0.84      0.89     52938\n",
            "           4       0.89      0.96      0.92     41798\n",
            "           5       0.78      0.98      0.87     32524\n",
            "           6       0.96      0.95      0.95     44688\n",
            "           7       0.91      0.96      0.93     44547\n",
            "           8       0.91      0.88      0.89     46176\n",
            "           9       0.94      0.87      0.90     50345\n",
            "\n",
            "    accuracy                           0.92    460000\n",
            "   macro avg       0.92      0.93      0.92    460000\n",
            "weighted avg       0.93      0.92      0.93    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.937 Loss: 0.207\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     47406\n",
            "           1       0.96      0.98      0.97     52533\n",
            "           2       0.95      0.92      0.93     50263\n",
            "           3       0.96      0.84      0.89     54108\n",
            "           4       0.89      0.96      0.92     42709\n",
            "           5       0.78      0.98      0.87     33279\n",
            "           6       0.96      0.95      0.95     45675\n",
            "           7       0.91      0.96      0.93     45545\n",
            "           8       0.91      0.88      0.89     47068\n",
            "           9       0.94      0.87      0.90     51414\n",
            "\n",
            "    accuracy                           0.93    470000\n",
            "   macro avg       0.92      0.93      0.92    470000\n",
            "weighted avg       0.93      0.93      0.93    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.936 Loss: 0.210\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     48411\n",
            "           1       0.96      0.98      0.97     53650\n",
            "           2       0.95      0.92      0.93     51324\n",
            "           3       0.96      0.84      0.89     55313\n",
            "           4       0.89      0.96      0.92     43616\n",
            "           5       0.78      0.98      0.87     34044\n",
            "           6       0.96      0.95      0.95     46638\n",
            "           7       0.91      0.96      0.93     46561\n",
            "           8       0.91      0.88      0.89     47981\n",
            "           9       0.94      0.87      0.90     52462\n",
            "\n",
            "    accuracy                           0.93    480000\n",
            "   macro avg       0.92      0.93      0.92    480000\n",
            "weighted avg       0.93      0.93      0.93    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.940 Loss: 0.207\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     49421\n",
            "           1       0.96      0.98      0.97     54781\n",
            "           2       0.95      0.92      0.93     52429\n",
            "           3       0.96      0.84      0.89     56468\n",
            "           4       0.89      0.96      0.92     44526\n",
            "           5       0.78      0.98      0.87     34820\n",
            "           6       0.96      0.95      0.95     47605\n",
            "           7       0.91      0.96      0.93     47569\n",
            "           8       0.91      0.88      0.90     48855\n",
            "           9       0.94      0.87      0.90     53526\n",
            "\n",
            "    accuracy                           0.93    490000\n",
            "   macro avg       0.92      0.93      0.92    490000\n",
            "weighted avg       0.93      0.93      0.93    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.939 Loss: 0.207\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     50420\n",
            "           1       0.96      0.98      0.97     55906\n",
            "           2       0.95      0.92      0.93     53532\n",
            "           3       0.96      0.84      0.89     57628\n",
            "           4       0.89      0.96      0.92     45441\n",
            "           5       0.78      0.98      0.87     35594\n",
            "           6       0.96      0.95      0.95     48588\n",
            "           7       0.91      0.96      0.93     48580\n",
            "           8       0.90      0.89      0.90     49719\n",
            "           9       0.94      0.87      0.90     54592\n",
            "\n",
            "    accuracy                           0.93    500000\n",
            "   macro avg       0.92      0.93      0.92    500000\n",
            "weighted avg       0.93      0.93      0.93    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.936 Loss: 0.215\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     51421\n",
            "           1       0.96      0.98      0.97     57026\n",
            "           2       0.95      0.92      0.93     54614\n",
            "           3       0.96      0.84      0.89     58828\n",
            "           4       0.89      0.96      0.92     46354\n",
            "           5       0.78      0.98      0.87     36331\n",
            "           6       0.96      0.95      0.95     49556\n",
            "           7       0.91      0.96      0.94     49616\n",
            "           8       0.90      0.89      0.90     50609\n",
            "           9       0.94      0.87      0.90     55645\n",
            "\n",
            "    accuracy                           0.93    510000\n",
            "   macro avg       0.92      0.93      0.92    510000\n",
            "weighted avg       0.93      0.93      0.93    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.938 Loss: 0.207\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     52424\n",
            "           1       0.96      0.98      0.97     58152\n",
            "           2       0.95      0.92      0.93     55668\n",
            "           3       0.96      0.84      0.89     59987\n",
            "           4       0.89      0.96      0.92     47241\n",
            "           5       0.79      0.98      0.87     37105\n",
            "           6       0.96      0.95      0.95     50540\n",
            "           7       0.91      0.96      0.94     50626\n",
            "           8       0.90      0.89      0.90     51488\n",
            "           9       0.94      0.87      0.90     56769\n",
            "\n",
            "    accuracy                           0.93    520000\n",
            "   macro avg       0.92      0.93      0.93    520000\n",
            "weighted avg       0.93      0.93      0.93    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.941 Loss: 0.194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     53412\n",
            "           1       0.96      0.98      0.97     59278\n",
            "           2       0.95      0.92      0.93     56775\n",
            "           3       0.96      0.84      0.89     61135\n",
            "           4       0.89      0.96      0.92     48173\n",
            "           5       0.79      0.98      0.87     37874\n",
            "           6       0.96      0.95      0.95     51525\n",
            "           7       0.91      0.96      0.94     51629\n",
            "           8       0.90      0.89      0.90     52389\n",
            "           9       0.94      0.87      0.90     57810\n",
            "\n",
            "    accuracy                           0.93    530000\n",
            "   macro avg       0.92      0.93      0.93    530000\n",
            "weighted avg       0.93      0.93      0.93    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.939 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     54425\n",
            "           1       0.96      0.98      0.97     60384\n",
            "           2       0.95      0.92      0.93     57835\n",
            "           3       0.96      0.84      0.89     62300\n",
            "           4       0.89      0.96      0.92     49103\n",
            "           5       0.79      0.98      0.87     38616\n",
            "           6       0.96      0.95      0.95     52493\n",
            "           7       0.91      0.96      0.94     52650\n",
            "           8       0.90      0.89      0.90     53341\n",
            "           9       0.94      0.87      0.90     58853\n",
            "\n",
            "    accuracy                           0.93    540000\n",
            "   macro avg       0.93      0.93      0.93    540000\n",
            "weighted avg       0.93      0.93      0.93    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.941 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     55424\n",
            "           1       0.96      0.98      0.97     61516\n",
            "           2       0.95      0.92      0.93     58920\n",
            "           3       0.96      0.84      0.89     63465\n",
            "           4       0.89      0.96      0.93     50029\n",
            "           5       0.79      0.98      0.87     39375\n",
            "           6       0.96      0.95      0.95     53465\n",
            "           7       0.91      0.96      0.94     53673\n",
            "           8       0.90      0.89      0.90     54218\n",
            "           9       0.94      0.87      0.90     59915\n",
            "\n",
            "    accuracy                           0.93    550000\n",
            "   macro avg       0.93      0.93      0.93    550000\n",
            "weighted avg       0.93      0.93      0.93    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.941 Loss: 0.193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     56417\n",
            "           1       0.96      0.98      0.97     62646\n",
            "           2       0.95      0.92      0.93     60007\n",
            "           3       0.96      0.84      0.89     64636\n",
            "           4       0.89      0.96      0.93     50948\n",
            "           5       0.79      0.98      0.87     40127\n",
            "           6       0.96      0.95      0.95     54445\n",
            "           7       0.91      0.96      0.94     54700\n",
            "           8       0.90      0.89      0.90     55113\n",
            "           9       0.94      0.87      0.91     60961\n",
            "\n",
            "    accuracy                           0.93    560000\n",
            "   macro avg       0.93      0.93      0.93    560000\n",
            "weighted avg       0.93      0.93      0.93    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.940 Loss: 0.196\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     57418\n",
            "           1       0.96      0.98      0.97     63772\n",
            "           2       0.95      0.92      0.93     61073\n",
            "           3       0.96      0.84      0.89     65791\n",
            "           4       0.89      0.96      0.93     51840\n",
            "           5       0.79      0.98      0.88     40904\n",
            "           6       0.96      0.95      0.95     55409\n",
            "           7       0.91      0.96      0.94     55726\n",
            "           8       0.90      0.89      0.90     56033\n",
            "           9       0.94      0.87      0.91     62034\n",
            "\n",
            "    accuracy                           0.93    570000\n",
            "   macro avg       0.93      0.93      0.93    570000\n",
            "weighted avg       0.93      0.93      0.93    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.937 Loss: 0.203\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     58422\n",
            "           1       0.96      0.98      0.97     64908\n",
            "           2       0.95      0.92      0.93     62133\n",
            "           3       0.96      0.84      0.89     66974\n",
            "           4       0.89      0.96      0.93     52726\n",
            "           5       0.79      0.98      0.88     41649\n",
            "           6       0.96      0.95      0.95     56395\n",
            "           7       0.91      0.96      0.94     56748\n",
            "           8       0.90      0.90      0.90     56917\n",
            "           9       0.94      0.87      0.91     63128\n",
            "\n",
            "    accuracy                           0.93    580000\n",
            "   macro avg       0.93      0.93      0.93    580000\n",
            "weighted avg       0.93      0.93      0.93    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.941 Loss: 0.191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     59415\n",
            "           1       0.96      0.98      0.97     66043\n",
            "           2       0.95      0.92      0.94     63210\n",
            "           3       0.96      0.84      0.89     68114\n",
            "           4       0.89      0.96      0.93     53622\n",
            "           5       0.79      0.98      0.88     42426\n",
            "           6       0.96      0.95      0.95     57381\n",
            "           7       0.92      0.96      0.94     57753\n",
            "           8       0.90      0.90      0.90     57827\n",
            "           9       0.94      0.87      0.91     64209\n",
            "\n",
            "    accuracy                           0.93    590000\n",
            "   macro avg       0.93      0.93      0.93    590000\n",
            "weighted avg       0.93      0.93      0.93    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.938 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     60417\n",
            "           1       0.96      0.98      0.97     67171\n",
            "           2       0.95      0.92      0.94     64274\n",
            "           3       0.96      0.84      0.89     69274\n",
            "           4       0.89      0.96      0.93     54513\n",
            "           5       0.79      0.98      0.88     43183\n",
            "           6       0.96      0.95      0.95     58346\n",
            "           7       0.92      0.96      0.94     58773\n",
            "           8       0.90      0.90      0.90     58752\n",
            "           9       0.94      0.87      0.91     65297\n",
            "\n",
            "    accuracy                           0.93    600000\n",
            "   macro avg       0.93      0.93      0.93    600000\n",
            "weighted avg       0.93      0.93      0.93    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.940 Loss: 0.201\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     61417\n",
            "           1       0.96      0.98      0.97     68297\n",
            "           2       0.95      0.92      0.94     65344\n",
            "           3       0.96      0.84      0.89     70404\n",
            "           4       0.89      0.96      0.93     55409\n",
            "           5       0.79      0.98      0.88     43951\n",
            "           6       0.96      0.95      0.95     59324\n",
            "           7       0.92      0.96      0.94     59787\n",
            "           8       0.90      0.90      0.90     59670\n",
            "           9       0.94      0.87      0.91     66397\n",
            "\n",
            "    accuracy                           0.93    610000\n",
            "   macro avg       0.93      0.93      0.93    610000\n",
            "weighted avg       0.93      0.93      0.93    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.939 Loss: 0.200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     62407\n",
            "           1       0.96      0.98      0.97     69430\n",
            "           2       0.95      0.92      0.94     66439\n",
            "           3       0.96      0.84      0.89     71538\n",
            "           4       0.89      0.96      0.93     56281\n",
            "           5       0.79      0.98      0.88     44732\n",
            "           6       0.96      0.95      0.95     60306\n",
            "           7       0.92      0.96      0.94     60819\n",
            "           8       0.90      0.90      0.90     60568\n",
            "           9       0.94      0.87      0.91     67480\n",
            "\n",
            "    accuracy                           0.93    620000\n",
            "   macro avg       0.93      0.93      0.93    620000\n",
            "weighted avg       0.93      0.93      0.93    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.940 Loss: 0.192\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     63411\n",
            "           1       0.96      0.98      0.97     70560\n",
            "           2       0.95      0.92      0.94     67496\n",
            "           3       0.96      0.84      0.90     72692\n",
            "           4       0.89      0.96      0.93     57178\n",
            "           5       0.79      0.98      0.88     45490\n",
            "           6       0.96      0.95      0.95     61270\n",
            "           7       0.92      0.96      0.94     61842\n",
            "           8       0.90      0.90      0.90     61504\n",
            "           9       0.94      0.87      0.91     68557\n",
            "\n",
            "    accuracy                           0.93    630000\n",
            "   macro avg       0.93      0.93      0.93    630000\n",
            "weighted avg       0.93      0.93      0.93    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.945 Loss: 0.181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     64416\n",
            "           1       0.96      0.98      0.97     71693\n",
            "           2       0.95      0.92      0.94     68571\n",
            "           3       0.96      0.84      0.90     73811\n",
            "           4       0.89      0.96      0.93     58103\n",
            "           5       0.80      0.98      0.88     46285\n",
            "           6       0.96      0.95      0.95     62230\n",
            "           7       0.92      0.96      0.94     62855\n",
            "           8       0.90      0.90      0.90     62411\n",
            "           9       0.94      0.87      0.91     69625\n",
            "\n",
            "    accuracy                           0.93    640000\n",
            "   macro avg       0.93      0.93      0.93    640000\n",
            "weighted avg       0.93      0.93      0.93    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.943 Loss: 0.195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     65412\n",
            "           1       0.97      0.98      0.97     72828\n",
            "           2       0.95      0.92      0.94     69678\n",
            "           3       0.96      0.84      0.90     74946\n",
            "           4       0.89      0.96      0.93     59025\n",
            "           5       0.80      0.98      0.88     47074\n",
            "           6       0.96      0.95      0.95     63207\n",
            "           7       0.92      0.96      0.94     63875\n",
            "           8       0.90      0.90      0.90     63315\n",
            "           9       0.94      0.87      0.91     70640\n",
            "\n",
            "    accuracy                           0.93    650000\n",
            "   macro avg       0.93      0.93      0.93    650000\n",
            "weighted avg       0.93      0.93      0.93    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.941 Loss: 0.190\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     66413\n",
            "           1       0.97      0.98      0.97     73955\n",
            "           2       0.95      0.92      0.94     70753\n",
            "           3       0.96      0.84      0.90     76100\n",
            "           4       0.89      0.96      0.93     59945\n",
            "           5       0.80      0.98      0.88     47837\n",
            "           6       0.96      0.95      0.95     64171\n",
            "           7       0.92      0.96      0.94     64905\n",
            "           8       0.90      0.90      0.90     64232\n",
            "           9       0.94      0.87      0.91     71689\n",
            "\n",
            "    accuracy                           0.93    660000\n",
            "   macro avg       0.93      0.93      0.93    660000\n",
            "weighted avg       0.93      0.93      0.93    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.940 Loss: 0.197\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     67410\n",
            "           1       0.97      0.98      0.97     75096\n",
            "           2       0.95      0.92      0.94     71839\n",
            "           3       0.96      0.84      0.90     77264\n",
            "           4       0.89      0.96      0.93     60864\n",
            "           5       0.80      0.98      0.88     48571\n",
            "           6       0.96      0.95      0.95     65145\n",
            "           7       0.92      0.96      0.94     65943\n",
            "           8       0.90      0.90      0.90     65124\n",
            "           9       0.94      0.88      0.91     72744\n",
            "\n",
            "    accuracy                           0.93    670000\n",
            "   macro avg       0.93      0.93      0.93    670000\n",
            "weighted avg       0.93      0.93      0.93    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.937 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     68407\n",
            "           1       0.97      0.98      0.97     76233\n",
            "           2       0.96      0.92      0.94     72926\n",
            "           3       0.96      0.84      0.90     78438\n",
            "           4       0.89      0.96      0.93     61747\n",
            "           5       0.80      0.98      0.88     49339\n",
            "           6       0.96      0.95      0.95     66129\n",
            "           7       0.92      0.96      0.94     66951\n",
            "           8       0.90      0.90      0.90     65997\n",
            "           9       0.94      0.88      0.91     73833\n",
            "\n",
            "    accuracy                           0.93    680000\n",
            "   macro avg       0.93      0.93      0.93    680000\n",
            "weighted avg       0.93      0.93      0.93    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.939 Loss: 0.201\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     69414\n",
            "           1       0.97      0.98      0.97     77362\n",
            "           2       0.96      0.92      0.94     74016\n",
            "           3       0.96      0.84      0.90     79593\n",
            "           4       0.89      0.96      0.93     62636\n",
            "           5       0.80      0.98      0.88     50089\n",
            "           6       0.96      0.95      0.95     67094\n",
            "           7       0.92      0.96      0.94     67985\n",
            "           8       0.90      0.90      0.90     66914\n",
            "           9       0.94      0.88      0.91     74897\n",
            "\n",
            "    accuracy                           0.93    690000\n",
            "   macro avg       0.93      0.93      0.93    690000\n",
            "weighted avg       0.93      0.93      0.93    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.942 Loss: 0.188\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     70419\n",
            "           1       0.97      0.98      0.97     78497\n",
            "           2       0.96      0.92      0.94     75099\n",
            "           3       0.96      0.84      0.90     80726\n",
            "           4       0.89      0.96      0.93     63538\n",
            "           5       0.80      0.98      0.88     50861\n",
            "           6       0.96      0.95      0.96     68075\n",
            "           7       0.92      0.96      0.94     69012\n",
            "           8       0.90      0.91      0.90     67808\n",
            "           9       0.94      0.88      0.91     75965\n",
            "\n",
            "    accuracy                           0.93    700000\n",
            "   macro avg       0.93      0.93      0.93    700000\n",
            "weighted avg       0.93      0.93      0.93    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.940 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     71406\n",
            "           1       0.97      0.98      0.97     79628\n",
            "           2       0.96      0.92      0.94     76181\n",
            "           3       0.96      0.84      0.90     81923\n",
            "           4       0.89      0.96      0.93     64444\n",
            "           5       0.80      0.98      0.88     51631\n",
            "           6       0.96      0.95      0.96     69058\n",
            "           7       0.92      0.96      0.94     70035\n",
            "           8       0.90      0.91      0.90     68681\n",
            "           9       0.94      0.88      0.91     77013\n",
            "\n",
            "    accuracy                           0.93    710000\n",
            "   macro avg       0.93      0.93      0.93    710000\n",
            "weighted avg       0.93      0.93      0.93    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.944 Loss: 0.185\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     72413\n",
            "           1       0.97      0.98      0.97     80757\n",
            "           2       0.96      0.92      0.94     77239\n",
            "           3       0.96      0.84      0.90     83053\n",
            "           4       0.89      0.97      0.93     65343\n",
            "           5       0.80      0.98      0.88     52427\n",
            "           6       0.96      0.95      0.96     70032\n",
            "           7       0.92      0.96      0.94     71046\n",
            "           8       0.90      0.91      0.90     69610\n",
            "           9       0.94      0.88      0.91     78080\n",
            "\n",
            "    accuracy                           0.93    720000\n",
            "   macro avg       0.93      0.93      0.93    720000\n",
            "weighted avg       0.93      0.93      0.93    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.942 Loss: 0.192\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     73416\n",
            "           1       0.97      0.98      0.97     81894\n",
            "           2       0.96      0.92      0.94     78315\n",
            "           3       0.96      0.84      0.90     84196\n",
            "           4       0.89      0.97      0.93     66249\n",
            "           5       0.80      0.98      0.88     53212\n",
            "           6       0.96      0.95      0.96     71003\n",
            "           7       0.92      0.96      0.94     72053\n",
            "           8       0.90      0.91      0.90     70503\n",
            "           9       0.94      0.88      0.91     79159\n",
            "\n",
            "    accuracy                           0.93    730000\n",
            "   macro avg       0.93      0.93      0.93    730000\n",
            "weighted avg       0.93      0.93      0.93    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.944 Loss: 0.186\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     74411\n",
            "           1       0.97      0.98      0.97     83031\n",
            "           2       0.96      0.92      0.94     79395\n",
            "           3       0.96      0.84      0.90     85371\n",
            "           4       0.89      0.97      0.93     67180\n",
            "           5       0.80      0.98      0.88     53979\n",
            "           6       0.96      0.95      0.96     71979\n",
            "           7       0.92      0.96      0.94     73072\n",
            "           8       0.90      0.91      0.90     71393\n",
            "           9       0.94      0.88      0.91     80189\n",
            "\n",
            "    accuracy                           0.93    740000\n",
            "   macro avg       0.93      0.93      0.93    740000\n",
            "weighted avg       0.93      0.93      0.93    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.944 Loss: 0.190\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     75409\n",
            "           1       0.97      0.98      0.97     84160\n",
            "           2       0.96      0.92      0.94     80453\n",
            "           3       0.96      0.84      0.90     86529\n",
            "           4       0.89      0.97      0.93     68100\n",
            "           5       0.80      0.98      0.88     54744\n",
            "           6       0.96      0.95      0.96     72955\n",
            "           7       0.92      0.96      0.94     74102\n",
            "           8       0.90      0.91      0.90     72318\n",
            "           9       0.94      0.88      0.91     81230\n",
            "\n",
            "    accuracy                           0.93    750000\n",
            "   macro avg       0.93      0.93      0.93    750000\n",
            "weighted avg       0.93      0.93      0.93    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.944 Loss: 0.185\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     76413\n",
            "           1       0.97      0.98      0.97     85298\n",
            "           2       0.96      0.92      0.94     81534\n",
            "           3       0.96      0.84      0.90     87662\n",
            "           4       0.89      0.97      0.93     69015\n",
            "           5       0.80      0.98      0.88     55515\n",
            "           6       0.96      0.95      0.96     73936\n",
            "           7       0.92      0.96      0.94     75114\n",
            "           8       0.90      0.91      0.90     73211\n",
            "           9       0.94      0.88      0.91     82302\n",
            "\n",
            "    accuracy                           0.93    760000\n",
            "   macro avg       0.93      0.93      0.93    760000\n",
            "weighted avg       0.93      0.93      0.93    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.946 Loss: 0.184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     77401\n",
            "           1       0.97      0.98      0.97     86440\n",
            "           2       0.96      0.92      0.94     82608\n",
            "           3       0.96      0.84      0.90     88797\n",
            "           4       0.89      0.97      0.93     69932\n",
            "           5       0.80      0.98      0.88     56298\n",
            "           6       0.96      0.95      0.96     74922\n",
            "           7       0.92      0.96      0.94     76140\n",
            "           8       0.90      0.91      0.91     74119\n",
            "           9       0.94      0.88      0.91     83343\n",
            "\n",
            "    accuracy                           0.93    770000\n",
            "   macro avg       0.93      0.93      0.93    770000\n",
            "weighted avg       0.93      0.93      0.93    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.942 Loss: 0.188\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     78396\n",
            "           1       0.97      0.98      0.97     87563\n",
            "           2       0.96      0.92      0.94     83664\n",
            "           3       0.96      0.84      0.90     89947\n",
            "           4       0.89      0.97      0.93     70849\n",
            "           5       0.80      0.98      0.88     57084\n",
            "           6       0.96      0.95      0.96     75881\n",
            "           7       0.92      0.96      0.94     77175\n",
            "           8       0.90      0.91      0.91     75042\n",
            "           9       0.94      0.88      0.91     84399\n",
            "\n",
            "    accuracy                           0.93    780000\n",
            "   macro avg       0.93      0.93      0.93    780000\n",
            "weighted avg       0.93      0.93      0.93    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.943 Loss: 0.195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     79398\n",
            "           1       0.97      0.98      0.97     88705\n",
            "           2       0.96      0.92      0.94     84738\n",
            "           3       0.96      0.84      0.90     91114\n",
            "           4       0.89      0.97      0.93     71766\n",
            "           5       0.81      0.98      0.88     57846\n",
            "           6       0.96      0.95      0.96     76843\n",
            "           7       0.92      0.96      0.94     78219\n",
            "           8       0.90      0.91      0.91     75921\n",
            "           9       0.94      0.88      0.91     85450\n",
            "\n",
            "    accuracy                           0.93    790000\n",
            "   macro avg       0.93      0.93      0.93    790000\n",
            "weighted avg       0.93      0.93      0.93    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.944 Loss: 0.189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     80384\n",
            "           1       0.97      0.98      0.97     89836\n",
            "           2       0.96      0.92      0.94     85832\n",
            "           3       0.96      0.84      0.90     92274\n",
            "           4       0.89      0.97      0.93     72680\n",
            "           5       0.81      0.98      0.88     58648\n",
            "           6       0.96      0.95      0.96     77820\n",
            "           7       0.92      0.96      0.94     79246\n",
            "           8       0.90      0.91      0.91     76789\n",
            "           9       0.94      0.88      0.91     86491\n",
            "\n",
            "    accuracy                           0.93    800000\n",
            "   macro avg       0.93      0.93      0.93    800000\n",
            "weighted avg       0.93      0.93      0.93    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.937 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     81387\n",
            "           1       0.97      0.98      0.97     90969\n",
            "           2       0.96      0.92      0.94     86902\n",
            "           3       0.96      0.84      0.90     93473\n",
            "           4       0.89      0.97      0.93     73590\n",
            "           5       0.81      0.98      0.89     59407\n",
            "           6       0.96      0.95      0.96     78770\n",
            "           7       0.92      0.96      0.94     80270\n",
            "           8       0.90      0.91      0.91     77677\n",
            "           9       0.94      0.88      0.91     87555\n",
            "\n",
            "    accuracy                           0.93    810000\n",
            "   macro avg       0.93      0.93      0.93    810000\n",
            "weighted avg       0.93      0.93      0.93    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.945 Loss: 0.185\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     82388\n",
            "           1       0.97      0.98      0.97     92108\n",
            "           2       0.96      0.92      0.94     87978\n",
            "           3       0.96      0.84      0.90     94613\n",
            "           4       0.89      0.97      0.93     74528\n",
            "           5       0.81      0.98      0.89     60183\n",
            "           6       0.96      0.95      0.96     79736\n",
            "           7       0.92      0.96      0.94     81293\n",
            "           8       0.90      0.91      0.91     78572\n",
            "           9       0.94      0.88      0.91     88601\n",
            "\n",
            "    accuracy                           0.93    820000\n",
            "   macro avg       0.93      0.93      0.93    820000\n",
            "weighted avg       0.93      0.93      0.93    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.943 Loss: 0.183\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     83390\n",
            "           1       0.97      0.98      0.97     93232\n",
            "           2       0.96      0.92      0.94     89063\n",
            "           3       0.96      0.84      0.90     95752\n",
            "           4       0.89      0.97      0.93     75437\n",
            "           5       0.81      0.98      0.89     60965\n",
            "           6       0.96      0.95      0.96     80716\n",
            "           7       0.93      0.96      0.94     82324\n",
            "           8       0.90      0.91      0.91     79460\n",
            "           9       0.94      0.88      0.91     89661\n",
            "\n",
            "    accuracy                           0.93    830000\n",
            "   macro avg       0.93      0.93      0.93    830000\n",
            "weighted avg       0.94      0.93      0.93    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.942 Loss: 0.190\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     84397\n",
            "           1       0.97      0.98      0.97     94361\n",
            "           2       0.96      0.92      0.94     90112\n",
            "           3       0.96      0.84      0.90     96897\n",
            "           4       0.89      0.97      0.93     76343\n",
            "           5       0.81      0.98      0.89     61760\n",
            "           6       0.96      0.95      0.96     81663\n",
            "           7       0.93      0.96      0.94     83360\n",
            "           8       0.90      0.91      0.91     80382\n",
            "           9       0.94      0.88      0.91     90725\n",
            "\n",
            "    accuracy                           0.93    840000\n",
            "   macro avg       0.93      0.94      0.93    840000\n",
            "weighted avg       0.94      0.93      0.93    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.941 Loss: 0.191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     85396\n",
            "           1       0.97      0.98      0.97     95502\n",
            "           2       0.96      0.92      0.94     91189\n",
            "           3       0.96      0.84      0.90     98061\n",
            "           4       0.89      0.97      0.93     77255\n",
            "           5       0.81      0.98      0.89     62536\n",
            "           6       0.96      0.95      0.96     82628\n",
            "           7       0.93      0.96      0.94     84375\n",
            "           8       0.90      0.92      0.91     81264\n",
            "           9       0.94      0.88      0.91     91794\n",
            "\n",
            "    accuracy                           0.93    850000\n",
            "   macro avg       0.93      0.94      0.93    850000\n",
            "weighted avg       0.94      0.93      0.93    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.939 Loss: 0.198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     86388\n",
            "           1       0.97      0.98      0.97     96629\n",
            "           2       0.96      0.92      0.94     92283\n",
            "           3       0.96      0.84      0.90     99225\n",
            "           4       0.89      0.97      0.93     78156\n",
            "           5       0.81      0.98      0.89     63318\n",
            "           6       0.96      0.95      0.96     83612\n",
            "           7       0.93      0.96      0.94     85396\n",
            "           8       0.90      0.92      0.91     82135\n",
            "           9       0.94      0.88      0.91     92858\n",
            "\n",
            "    accuracy                           0.93    860000\n",
            "   macro avg       0.93      0.94      0.93    860000\n",
            "weighted avg       0.94      0.93      0.93    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.939 Loss: 0.192\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     87390\n",
            "           1       0.97      0.98      0.97     97752\n",
            "           2       0.96      0.92      0.94     93338\n",
            "           3       0.96      0.84      0.90    100385\n",
            "           4       0.89      0.97      0.93     79047\n",
            "           5       0.81      0.98      0.89     64088\n",
            "           6       0.96      0.95      0.96     84574\n",
            "           7       0.93      0.96      0.94     86421\n",
            "           8       0.90      0.92      0.91     83051\n",
            "           9       0.94      0.88      0.91     93954\n",
            "\n",
            "    accuracy                           0.93    870000\n",
            "   macro avg       0.93      0.94      0.93    870000\n",
            "weighted avg       0.94      0.93      0.93    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.942 Loss: 0.185\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     88392\n",
            "           1       0.97      0.98      0.97     98891\n",
            "           2       0.96      0.92      0.94     94399\n",
            "           3       0.96      0.84      0.90    101524\n",
            "           4       0.89      0.97      0.93     79947\n",
            "           5       0.81      0.98      0.89     64869\n",
            "           6       0.96      0.95      0.96     85547\n",
            "           7       0.93      0.96      0.94     87465\n",
            "           8       0.90      0.92      0.91     83931\n",
            "           9       0.94      0.88      0.91     95035\n",
            "\n",
            "    accuracy                           0.93    880000\n",
            "   macro avg       0.93      0.94      0.93    880000\n",
            "weighted avg       0.94      0.93      0.93    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.938 Loss: 0.195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     89382\n",
            "           1       0.97      0.98      0.97    100020\n",
            "           2       0.96      0.92      0.94     95501\n",
            "           3       0.96      0.84      0.90    102688\n",
            "           4       0.89      0.97      0.93     80844\n",
            "           5       0.81      0.98      0.89     65636\n",
            "           6       0.96      0.95      0.96     86533\n",
            "           7       0.93      0.96      0.94     88480\n",
            "           8       0.90      0.92      0.91     84801\n",
            "           9       0.94      0.88      0.91     96115\n",
            "\n",
            "    accuracy                           0.93    890000\n",
            "   macro avg       0.93      0.94      0.93    890000\n",
            "weighted avg       0.94      0.93      0.93    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.942 Loss: 0.187\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     90378\n",
            "           1       0.97      0.98      0.97    101158\n",
            "           2       0.96      0.92      0.94     96569\n",
            "           3       0.96      0.84      0.90    103819\n",
            "           4       0.89      0.97      0.93     81730\n",
            "           5       0.81      0.98      0.89     66421\n",
            "           6       0.96      0.95      0.96     87507\n",
            "           7       0.93      0.96      0.94     89527\n",
            "           8       0.90      0.92      0.91     85693\n",
            "           9       0.94      0.88      0.91     97198\n",
            "\n",
            "    accuracy                           0.93    900000\n",
            "   macro avg       0.93      0.94      0.93    900000\n",
            "weighted avg       0.94      0.93      0.93    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(28*28, 128)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.linear3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.batch_norm1(self.linear1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.batch_norm2(self.linear2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "model = MnistModel()"
      ],
      "metadata": {
        "id": "k-2kxnUgj-ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "5uItqRpGkSNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtJ7hlAlkWNC",
        "outputId": "f7159ecd-2e91-4eef-e43b-ba7856e9deba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.930 Loss: 0.244\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96      1000\n",
            "           1       0.99      0.95      0.97      1184\n",
            "           2       0.92      0.93      0.92      1024\n",
            "           3       0.91      0.92      0.92       998\n",
            "           4       0.95      0.91      0.93      1025\n",
            "           5       0.90      0.90      0.90       891\n",
            "           6       0.94      0.96      0.95       938\n",
            "           7       0.93      0.93      0.93      1027\n",
            "           8       0.89      0.93      0.91       924\n",
            "           9       0.90      0.92      0.91       989\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.945 Loss: 0.181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      2014\n",
            "           1       0.99      0.95      0.97      2348\n",
            "           2       0.92      0.94      0.93      2040\n",
            "           3       0.93      0.92      0.93      2048\n",
            "           4       0.94      0.93      0.93      1995\n",
            "           5       0.91      0.93      0.92      1740\n",
            "           6       0.95      0.96      0.95      1906\n",
            "           7       0.93      0.94      0.93      2023\n",
            "           8       0.90      0.94      0.92      1869\n",
            "           9       0.92      0.92      0.92      2017\n",
            "\n",
            "    accuracy                           0.94     20000\n",
            "   macro avg       0.94      0.94      0.94     20000\n",
            "weighted avg       0.94      0.94      0.94     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.952 Loss: 0.154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      3015\n",
            "           1       0.99      0.96      0.97      3506\n",
            "           2       0.93      0.94      0.94      3038\n",
            "           3       0.94      0.93      0.93      3062\n",
            "           4       0.94      0.93      0.94      2975\n",
            "           5       0.92      0.93      0.93      2654\n",
            "           6       0.95      0.95      0.95      2867\n",
            "           7       0.93      0.95      0.94      3028\n",
            "           8       0.91      0.94      0.93      2833\n",
            "           9       0.92      0.93      0.92      3022\n",
            "\n",
            "    accuracy                           0.94     30000\n",
            "   macro avg       0.94      0.94      0.94     30000\n",
            "weighted avg       0.94      0.94      0.94     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.962 Loss: 0.130\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      4016\n",
            "           1       0.99      0.96      0.97      4663\n",
            "           2       0.93      0.95      0.94      4072\n",
            "           3       0.94      0.94      0.94      4058\n",
            "           4       0.95      0.94      0.94      3983\n",
            "           5       0.93      0.94      0.93      3521\n",
            "           6       0.96      0.96      0.96      3826\n",
            "           7       0.94      0.95      0.94      4062\n",
            "           8       0.92      0.95      0.93      3794\n",
            "           9       0.93      0.94      0.93      4005\n",
            "\n",
            "    accuracy                           0.95     40000\n",
            "   macro avg       0.95      0.95      0.95     40000\n",
            "weighted avg       0.95      0.95      0.95     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.962 Loss: 0.120\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      5011\n",
            "           1       0.99      0.96      0.98      5814\n",
            "           2       0.94      0.95      0.94      5103\n",
            "           3       0.95      0.94      0.94      5110\n",
            "           4       0.95      0.94      0.95      4958\n",
            "           5       0.93      0.95      0.94      4367\n",
            "           6       0.96      0.96      0.96      4807\n",
            "           7       0.94      0.95      0.95      5062\n",
            "           8       0.93      0.95      0.94      4766\n",
            "           9       0.93      0.94      0.94      5002\n",
            "\n",
            "    accuracy                           0.95     50000\n",
            "   macro avg       0.95      0.95      0.95     50000\n",
            "weighted avg       0.95      0.95      0.95     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.963 Loss: 0.123\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      6012\n",
            "           1       0.99      0.97      0.98      6965\n",
            "           2       0.94      0.95      0.95      6107\n",
            "           3       0.95      0.94      0.94      6132\n",
            "           4       0.95      0.95      0.95      5920\n",
            "           5       0.94      0.95      0.94      5280\n",
            "           6       0.96      0.96      0.96      5753\n",
            "           7       0.94      0.95      0.95      6083\n",
            "           8       0.93      0.95      0.94      5735\n",
            "           9       0.94      0.94      0.94      6013\n",
            "\n",
            "    accuracy                           0.95     60000\n",
            "   macro avg       0.95      0.95      0.95     60000\n",
            "weighted avg       0.95      0.95      0.95     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.970 Loss: 0.105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      6999\n",
            "           1       0.99      0.97      0.98      8111\n",
            "           2       0.94      0.95      0.95      7154\n",
            "           3       0.95      0.94      0.95      7130\n",
            "           4       0.96      0.95      0.95      6909\n",
            "           5       0.94      0.95      0.95      6164\n",
            "           6       0.96      0.96      0.96      6715\n",
            "           7       0.94      0.95      0.95      7109\n",
            "           8       0.94      0.95      0.94      6700\n",
            "           9       0.94      0.95      0.94      7009\n",
            "\n",
            "    accuracy                           0.95     70000\n",
            "   macro avg       0.95      0.95      0.95     70000\n",
            "weighted avg       0.96      0.95      0.95     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.970 Loss: 0.096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      7988\n",
            "           1       0.99      0.97      0.98      9256\n",
            "           2       0.95      0.96      0.95      8192\n",
            "           3       0.95      0.95      0.95      8158\n",
            "           4       0.96      0.95      0.96      7880\n",
            "           5       0.94      0.96      0.95      7027\n",
            "           6       0.96      0.96      0.96      7671\n",
            "           7       0.95      0.96      0.95      8134\n",
            "           8       0.94      0.95      0.95      7678\n",
            "           9       0.94      0.95      0.94      8016\n",
            "\n",
            "    accuracy                           0.96     80000\n",
            "   macro avg       0.96      0.96      0.96     80000\n",
            "weighted avg       0.96      0.96      0.96     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.970 Loss: 0.100\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97      8978\n",
            "           1       0.99      0.97      0.98     10400\n",
            "           2       0.95      0.96      0.95      9204\n",
            "           3       0.96      0.95      0.95      9187\n",
            "           4       0.96      0.96      0.96      8851\n",
            "           5       0.95      0.96      0.95      7925\n",
            "           6       0.96      0.96      0.96      8625\n",
            "           7       0.95      0.96      0.95      9145\n",
            "           8       0.94      0.95      0.95      8650\n",
            "           9       0.94      0.95      0.95      9035\n",
            "\n",
            "    accuracy                           0.96     90000\n",
            "   macro avg       0.96      0.96      0.96     90000\n",
            "weighted avg       0.96      0.96      0.96     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.972 Loss: 0.092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98      9973\n",
            "           1       0.99      0.97      0.98     11554\n",
            "           2       0.95      0.96      0.96     10230\n",
            "           3       0.96      0.95      0.95     10193\n",
            "           4       0.96      0.96      0.96      9860\n",
            "           5       0.95      0.96      0.95      8811\n",
            "           6       0.96      0.96      0.96      9576\n",
            "           7       0.95      0.96      0.95     10166\n",
            "           8       0.94      0.96      0.95      9612\n",
            "           9       0.95      0.95      0.95     10025\n",
            "\n",
            "    accuracy                           0.96    100000\n",
            "   macro avg       0.96      0.96      0.96    100000\n",
            "weighted avg       0.96      0.96      0.96    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.972 Loss: 0.089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98     10964\n",
            "           1       0.99      0.97      0.98     12691\n",
            "           2       0.95      0.96      0.96     11277\n",
            "           3       0.96      0.95      0.96     11227\n",
            "           4       0.96      0.96      0.96     10826\n",
            "           5       0.95      0.96      0.95      9671\n",
            "           6       0.97      0.97      0.97     10543\n",
            "           7       0.95      0.96      0.95     11170\n",
            "           8       0.95      0.96      0.95     10583\n",
            "           9       0.95      0.95      0.95     11048\n",
            "\n",
            "    accuracy                           0.96    110000\n",
            "   macro avg       0.96      0.96      0.96    110000\n",
            "weighted avg       0.96      0.96      0.96    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.973 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98     11963\n",
            "           1       0.99      0.97      0.98     13836\n",
            "           2       0.95      0.96      0.96     12285\n",
            "           3       0.96      0.95      0.96     12247\n",
            "           4       0.96      0.96      0.96     11799\n",
            "           5       0.95      0.96      0.96     10563\n",
            "           6       0.97      0.97      0.97     11502\n",
            "           7       0.95      0.96      0.96     12192\n",
            "           8       0.95      0.96      0.95     11551\n",
            "           9       0.95      0.95      0.95     12062\n",
            "\n",
            "    accuracy                           0.96    120000\n",
            "   macro avg       0.96      0.96      0.96    120000\n",
            "weighted avg       0.96      0.96      0.96    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.974 Loss: 0.087\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     12960\n",
            "           1       0.99      0.97      0.98     14980\n",
            "           2       0.96      0.96      0.96     13314\n",
            "           3       0.96      0.95      0.96     13267\n",
            "           4       0.96      0.96      0.96     12789\n",
            "           5       0.95      0.96      0.96     11438\n",
            "           6       0.97      0.97      0.97     12450\n",
            "           7       0.95      0.96      0.96     13237\n",
            "           8       0.95      0.96      0.95     12507\n",
            "           9       0.95      0.95      0.95     13058\n",
            "\n",
            "    accuracy                           0.96    130000\n",
            "   macro avg       0.96      0.96      0.96    130000\n",
            "weighted avg       0.96      0.96      0.96    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.975 Loss: 0.080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     13952\n",
            "           1       0.99      0.97      0.98     16125\n",
            "           2       0.96      0.96      0.96     14357\n",
            "           3       0.96      0.95      0.96     14286\n",
            "           4       0.96      0.96      0.96     13766\n",
            "           5       0.95      0.97      0.96     12314\n",
            "           6       0.97      0.97      0.97     13410\n",
            "           7       0.95      0.96      0.96     14250\n",
            "           8       0.95      0.96      0.95     13487\n",
            "           9       0.95      0.96      0.95     14053\n",
            "\n",
            "    accuracy                           0.96    140000\n",
            "   macro avg       0.96      0.96      0.96    140000\n",
            "weighted avg       0.96      0.96      0.96    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.975 Loss: 0.086\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     14955\n",
            "           1       0.99      0.98      0.98     17271\n",
            "           2       0.96      0.96      0.96     15375\n",
            "           3       0.97      0.96      0.96     15303\n",
            "           4       0.96      0.96      0.96     14741\n",
            "           5       0.95      0.97      0.96     13201\n",
            "           6       0.97      0.97      0.97     14360\n",
            "           7       0.95      0.96      0.96     15276\n",
            "           8       0.95      0.96      0.96     14454\n",
            "           9       0.95      0.96      0.95     15064\n",
            "\n",
            "    accuracy                           0.96    150000\n",
            "   macro avg       0.96      0.96      0.96    150000\n",
            "weighted avg       0.96      0.96      0.96    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.974 Loss: 0.082\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     15948\n",
            "           1       0.99      0.98      0.98     18417\n",
            "           2       0.96      0.97      0.96     16403\n",
            "           3       0.97      0.96      0.96     16333\n",
            "           4       0.96      0.96      0.96     15730\n",
            "           5       0.95      0.97      0.96     14063\n",
            "           6       0.97      0.97      0.97     15313\n",
            "           7       0.95      0.96      0.96     16313\n",
            "           8       0.95      0.96      0.96     15416\n",
            "           9       0.95      0.96      0.96     16064\n",
            "\n",
            "    accuracy                           0.97    160000\n",
            "   macro avg       0.96      0.96      0.96    160000\n",
            "weighted avg       0.97      0.97      0.97    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.976 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     16940\n",
            "           1       0.99      0.98      0.98     19558\n",
            "           2       0.96      0.97      0.96     17448\n",
            "           3       0.97      0.96      0.96     17352\n",
            "           4       0.96      0.96      0.96     16716\n",
            "           5       0.95      0.97      0.96     14925\n",
            "           6       0.97      0.97      0.97     16272\n",
            "           7       0.96      0.96      0.96     17336\n",
            "           8       0.95      0.96      0.96     16393\n",
            "           9       0.95      0.96      0.96     17060\n",
            "\n",
            "    accuracy                           0.97    170000\n",
            "   macro avg       0.97      0.97      0.97    170000\n",
            "weighted avg       0.97      0.97      0.97    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.975 Loss: 0.084\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     17937\n",
            "           1       0.99      0.98      0.98     20705\n",
            "           2       0.96      0.97      0.96     18462\n",
            "           3       0.97      0.96      0.96     18365\n",
            "           4       0.97      0.96      0.96     17694\n",
            "           5       0.95      0.97      0.96     15816\n",
            "           6       0.97      0.97      0.97     17226\n",
            "           7       0.96      0.96      0.96     18366\n",
            "           8       0.95      0.96      0.96     17361\n",
            "           9       0.95      0.96      0.96     18068\n",
            "\n",
            "    accuracy                           0.97    180000\n",
            "   macro avg       0.97      0.97      0.97    180000\n",
            "weighted avg       0.97      0.97      0.97    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.977 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     18929\n",
            "           1       0.99      0.98      0.98     21848\n",
            "           2       0.96      0.97      0.96     19497\n",
            "           3       0.97      0.96      0.96     19381\n",
            "           4       0.97      0.96      0.97     18684\n",
            "           5       0.96      0.97      0.96     16691\n",
            "           6       0.97      0.97      0.97     18189\n",
            "           7       0.96      0.96      0.96     19398\n",
            "           8       0.95      0.96      0.96     18325\n",
            "           9       0.95      0.96      0.96     19058\n",
            "\n",
            "    accuracy                           0.97    190000\n",
            "   macro avg       0.97      0.97      0.97    190000\n",
            "weighted avg       0.97      0.97      0.97    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.979 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     19922\n",
            "           1       0.99      0.98      0.98     22985\n",
            "           2       0.96      0.97      0.97     20539\n",
            "           3       0.97      0.96      0.96     20399\n",
            "           4       0.97      0.97      0.97     19664\n",
            "           5       0.96      0.97      0.96     17563\n",
            "           6       0.97      0.97      0.97     19152\n",
            "           7       0.96      0.96      0.96     20424\n",
            "           8       0.95      0.96      0.96     19293\n",
            "           9       0.96      0.96      0.96     20059\n",
            "\n",
            "    accuracy                           0.97    200000\n",
            "   macro avg       0.97      0.97      0.97    200000\n",
            "weighted avg       0.97      0.97      0.97    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.977 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     20917\n",
            "           1       0.99      0.98      0.98     24123\n",
            "           2       0.96      0.97      0.97     21561\n",
            "           3       0.97      0.96      0.97     21413\n",
            "           4       0.97      0.97      0.97     20647\n",
            "           5       0.96      0.97      0.96     18453\n",
            "           6       0.97      0.97      0.97     20120\n",
            "           7       0.96      0.96      0.96     21452\n",
            "           8       0.96      0.96      0.96     20256\n",
            "           9       0.96      0.96      0.96     21058\n",
            "\n",
            "    accuracy                           0.97    210000\n",
            "   macro avg       0.97      0.97      0.97    210000\n",
            "weighted avg       0.97      0.97      0.97    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.978 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     21911\n",
            "           1       0.99      0.98      0.98     25264\n",
            "           2       0.96      0.97      0.97     22599\n",
            "           3       0.97      0.96      0.97     22440\n",
            "           4       0.97      0.97      0.97     21633\n",
            "           5       0.96      0.97      0.96     19328\n",
            "           6       0.97      0.97      0.97     21074\n",
            "           7       0.96      0.97      0.96     22480\n",
            "           8       0.96      0.97      0.96     21222\n",
            "           9       0.96      0.96      0.96     22049\n",
            "\n",
            "    accuracy                           0.97    220000\n",
            "   macro avg       0.97      0.97      0.97    220000\n",
            "weighted avg       0.97      0.97      0.97    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     22902\n",
            "           1       0.99      0.98      0.98     26401\n",
            "           2       0.96      0.97      0.97     23639\n",
            "           3       0.97      0.96      0.97     23455\n",
            "           4       0.97      0.97      0.97     22609\n",
            "           5       0.96      0.97      0.96     20203\n",
            "           6       0.97      0.97      0.97     22034\n",
            "           7       0.96      0.97      0.96     23516\n",
            "           8       0.96      0.97      0.96     22187\n",
            "           9       0.96      0.96      0.96     23054\n",
            "\n",
            "    accuracy                           0.97    230000\n",
            "   macro avg       0.97      0.97      0.97    230000\n",
            "weighted avg       0.97      0.97      0.97    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.977 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     23903\n",
            "           1       0.99      0.98      0.98     27546\n",
            "           2       0.97      0.97      0.97     24658\n",
            "           3       0.97      0.96      0.97     24478\n",
            "           4       0.97      0.97      0.97     23584\n",
            "           5       0.96      0.97      0.97     21085\n",
            "           6       0.97      0.97      0.97     22990\n",
            "           7       0.96      0.97      0.96     24542\n",
            "           8       0.96      0.97      0.96     23149\n",
            "           9       0.96      0.96      0.96     24065\n",
            "\n",
            "    accuracy                           0.97    240000\n",
            "   macro avg       0.97      0.97      0.97    240000\n",
            "weighted avg       0.97      0.97      0.97    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     24894\n",
            "           1       0.99      0.98      0.98     28686\n",
            "           2       0.97      0.97      0.97     25689\n",
            "           3       0.97      0.96      0.97     25508\n",
            "           4       0.97      0.97      0.97     24564\n",
            "           5       0.96      0.97      0.97     21960\n",
            "           6       0.97      0.97      0.97     23949\n",
            "           7       0.96      0.97      0.96     25575\n",
            "           8       0.96      0.97      0.96     24103\n",
            "           9       0.96      0.96      0.96     25072\n",
            "\n",
            "    accuracy                           0.97    250000\n",
            "   macro avg       0.97      0.97      0.97    250000\n",
            "weighted avg       0.97      0.97      0.97    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.979 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     25889\n",
            "           1       0.99      0.98      0.98     29821\n",
            "           2       0.97      0.97      0.97     26723\n",
            "           3       0.97      0.96      0.97     26539\n",
            "           4       0.97      0.97      0.97     25546\n",
            "           5       0.96      0.97      0.97     22834\n",
            "           6       0.97      0.97      0.97     24910\n",
            "           7       0.96      0.97      0.96     26609\n",
            "           8       0.96      0.97      0.96     25065\n",
            "           9       0.96      0.96      0.96     26064\n",
            "\n",
            "    accuracy                           0.97    260000\n",
            "   macro avg       0.97      0.97      0.97    260000\n",
            "weighted avg       0.97      0.97      0.97    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.977 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     26881\n",
            "           1       0.99      0.98      0.98     30965\n",
            "           2       0.97      0.97      0.97     27742\n",
            "           3       0.97      0.96      0.97     27557\n",
            "           4       0.97      0.97      0.97     26529\n",
            "           5       0.96      0.97      0.97     23734\n",
            "           6       0.97      0.97      0.97     25856\n",
            "           7       0.96      0.97      0.96     27639\n",
            "           8       0.96      0.97      0.96     26027\n",
            "           9       0.96      0.96      0.96     27070\n",
            "\n",
            "    accuracy                           0.97    270000\n",
            "   macro avg       0.97      0.97      0.97    270000\n",
            "weighted avg       0.97      0.97      0.97    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.980 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     27872\n",
            "           1       0.99      0.98      0.98     32108\n",
            "           2       0.97      0.97      0.97     28785\n",
            "           3       0.97      0.96      0.97     28565\n",
            "           4       0.97      0.97      0.97     27511\n",
            "           5       0.96      0.97      0.97     24615\n",
            "           6       0.97      0.97      0.97     26805\n",
            "           7       0.96      0.97      0.96     28670\n",
            "           8       0.96      0.97      0.96     26996\n",
            "           9       0.96      0.97      0.96     28073\n",
            "\n",
            "    accuracy                           0.97    280000\n",
            "   macro avg       0.97      0.97      0.97    280000\n",
            "weighted avg       0.97      0.97      0.97    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     28868\n",
            "           1       0.99      0.98      0.98     33247\n",
            "           2       0.97      0.97      0.97     29828\n",
            "           3       0.97      0.96      0.97     29588\n",
            "           4       0.97      0.97      0.97     28482\n",
            "           5       0.96      0.97      0.97     25493\n",
            "           6       0.97      0.97      0.97     27763\n",
            "           7       0.96      0.97      0.96     29697\n",
            "           8       0.96      0.97      0.96     27961\n",
            "           9       0.96      0.97      0.96     29073\n",
            "\n",
            "    accuracy                           0.97    290000\n",
            "   macro avg       0.97      0.97      0.97    290000\n",
            "weighted avg       0.97      0.97      0.97    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.978 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     29869\n",
            "           1       0.99      0.98      0.99     34388\n",
            "           2       0.97      0.97      0.97     30850\n",
            "           3       0.97      0.96      0.97     30605\n",
            "           4       0.97      0.97      0.97     29456\n",
            "           5       0.96      0.97      0.97     26379\n",
            "           6       0.97      0.97      0.97     28720\n",
            "           7       0.96      0.97      0.97     30734\n",
            "           8       0.96      0.97      0.96     28914\n",
            "           9       0.96      0.97      0.96     30085\n",
            "\n",
            "    accuracy                           0.97    300000\n",
            "   macro avg       0.97      0.97      0.97    300000\n",
            "weighted avg       0.97      0.97      0.97    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.978 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     30861\n",
            "           1       0.99      0.98      0.99     35535\n",
            "           2       0.97      0.97      0.97     31879\n",
            "           3       0.97      0.97      0.97     31627\n",
            "           4       0.97      0.97      0.97     30443\n",
            "           5       0.96      0.98      0.97     27260\n",
            "           6       0.97      0.97      0.97     29681\n",
            "           7       0.96      0.97      0.97     31765\n",
            "           8       0.96      0.97      0.96     29867\n",
            "           9       0.96      0.97      0.96     31082\n",
            "\n",
            "    accuracy                           0.97    310000\n",
            "   macro avg       0.97      0.97      0.97    310000\n",
            "weighted avg       0.97      0.97      0.97    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.979 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     31854\n",
            "           1       0.99      0.98      0.99     36670\n",
            "           2       0.97      0.97      0.97     32918\n",
            "           3       0.98      0.97      0.97     32651\n",
            "           4       0.97      0.97      0.97     31418\n",
            "           5       0.96      0.98      0.97     28133\n",
            "           6       0.97      0.97      0.97     30644\n",
            "           7       0.96      0.97      0.97     32789\n",
            "           8       0.96      0.97      0.96     30828\n",
            "           9       0.96      0.97      0.96     32095\n",
            "\n",
            "    accuracy                           0.97    320000\n",
            "   macro avg       0.97      0.97      0.97    320000\n",
            "weighted avg       0.97      0.97      0.97    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.979 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     32849\n",
            "           1       0.99      0.98      0.99     37809\n",
            "           2       0.97      0.97      0.97     33938\n",
            "           3       0.98      0.97      0.97     33663\n",
            "           4       0.97      0.97      0.97     32398\n",
            "           5       0.96      0.98      0.97     29030\n",
            "           6       0.97      0.97      0.97     31592\n",
            "           7       0.96      0.97      0.97     33822\n",
            "           8       0.96      0.97      0.96     31787\n",
            "           9       0.96      0.97      0.96     33112\n",
            "\n",
            "    accuracy                           0.97    330000\n",
            "   macro avg       0.97      0.97      0.97    330000\n",
            "weighted avg       0.97      0.97      0.97    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.979 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     33844\n",
            "           1       0.99      0.98      0.99     38949\n",
            "           2       0.97      0.97      0.97     34968\n",
            "           3       0.98      0.97      0.97     34686\n",
            "           4       0.97      0.97      0.97     33381\n",
            "           5       0.96      0.98      0.97     29905\n",
            "           6       0.97      0.97      0.97     32548\n",
            "           7       0.97      0.97      0.97     34862\n",
            "           8       0.96      0.97      0.96     32748\n",
            "           9       0.96      0.97      0.96     34109\n",
            "\n",
            "    accuracy                           0.97    340000\n",
            "   macro avg       0.97      0.97      0.97    340000\n",
            "weighted avg       0.97      0.97      0.97    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.979 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     34836\n",
            "           1       0.99      0.98      0.99     40084\n",
            "           2       0.97      0.97      0.97     36003\n",
            "           3       0.98      0.97      0.97     35713\n",
            "           4       0.97      0.97      0.97     34351\n",
            "           5       0.96      0.98      0.97     30776\n",
            "           6       0.97      0.97      0.97     33511\n",
            "           7       0.97      0.97      0.97     35891\n",
            "           8       0.96      0.97      0.96     33716\n",
            "           9       0.96      0.97      0.96     35119\n",
            "\n",
            "    accuracy                           0.97    350000\n",
            "   macro avg       0.97      0.97      0.97    350000\n",
            "weighted avg       0.97      0.97      0.97    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     35833\n",
            "           1       0.99      0.98      0.99     41225\n",
            "           2       0.97      0.97      0.97     37024\n",
            "           3       0.98      0.97      0.97     36725\n",
            "           4       0.97      0.97      0.97     35338\n",
            "           5       0.96      0.98      0.97     31668\n",
            "           6       0.97      0.97      0.97     34460\n",
            "           7       0.97      0.97      0.97     36929\n",
            "           8       0.96      0.97      0.96     34674\n",
            "           9       0.96      0.97      0.96     36124\n",
            "\n",
            "    accuracy                           0.97    360000\n",
            "   macro avg       0.97      0.97      0.97    360000\n",
            "weighted avg       0.97      0.97      0.97    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.979 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     36830\n",
            "           1       0.99      0.98      0.99     42371\n",
            "           2       0.97      0.97      0.97     38054\n",
            "           3       0.98      0.97      0.97     37743\n",
            "           4       0.97      0.97      0.97     36334\n",
            "           5       0.96      0.98      0.97     32539\n",
            "           6       0.97      0.98      0.97     35413\n",
            "           7       0.97      0.97      0.97     37963\n",
            "           8       0.96      0.97      0.96     35636\n",
            "           9       0.96      0.97      0.96     37117\n",
            "\n",
            "    accuracy                           0.97    370000\n",
            "   macro avg       0.97      0.97      0.97    370000\n",
            "weighted avg       0.97      0.97      0.97    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.979 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     37826\n",
            "           1       0.99      0.98      0.99     43506\n",
            "           2       0.97      0.97      0.97     39089\n",
            "           3       0.98      0.97      0.97     38760\n",
            "           4       0.97      0.97      0.97     37315\n",
            "           5       0.96      0.98      0.97     33425\n",
            "           6       0.97      0.98      0.97     36376\n",
            "           7       0.97      0.97      0.97     38995\n",
            "           8       0.96      0.97      0.97     36587\n",
            "           9       0.96      0.97      0.97     38121\n",
            "\n",
            "    accuracy                           0.97    380000\n",
            "   macro avg       0.97      0.97      0.97    380000\n",
            "weighted avg       0.97      0.97      0.97    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     38825\n",
            "           1       0.99      0.98      0.99     44645\n",
            "           2       0.97      0.97      0.97     40111\n",
            "           3       0.98      0.97      0.97     39776\n",
            "           4       0.97      0.97      0.97     38302\n",
            "           5       0.96      0.98      0.97     34324\n",
            "           6       0.97      0.98      0.97     37331\n",
            "           7       0.97      0.97      0.97     40034\n",
            "           8       0.96      0.97      0.97     37535\n",
            "           9       0.96      0.97      0.97     39117\n",
            "\n",
            "    accuracy                           0.97    390000\n",
            "   macro avg       0.97      0.97      0.97    390000\n",
            "weighted avg       0.97      0.97      0.97    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     39823\n",
            "           1       0.99      0.98      0.99     45779\n",
            "           2       0.97      0.97      0.97     41149\n",
            "           3       0.98      0.97      0.97     40789\n",
            "           4       0.97      0.97      0.97     39295\n",
            "           5       0.96      0.98      0.97     35201\n",
            "           6       0.97      0.98      0.98     38286\n",
            "           7       0.97      0.97      0.97     41066\n",
            "           8       0.96      0.97      0.97     38490\n",
            "           9       0.96      0.97      0.97     40122\n",
            "\n",
            "    accuracy                           0.97    400000\n",
            "   macro avg       0.97      0.97      0.97    400000\n",
            "weighted avg       0.97      0.97      0.97    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.980 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     40821\n",
            "           1       0.99      0.98      0.99     46912\n",
            "           2       0.97      0.97      0.97     42180\n",
            "           3       0.98      0.97      0.97     41813\n",
            "           4       0.97      0.97      0.97     40278\n",
            "           5       0.96      0.98      0.97     36077\n",
            "           6       0.97      0.98      0.98     39243\n",
            "           7       0.97      0.97      0.97     42102\n",
            "           8       0.96      0.97      0.97     39449\n",
            "           9       0.96      0.97      0.97     41125\n",
            "\n",
            "    accuracy                           0.97    410000\n",
            "   macro avg       0.97      0.97      0.97    410000\n",
            "weighted avg       0.97      0.97      0.97    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.979 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     41821\n",
            "           1       0.99      0.98      0.99     48051\n",
            "           2       0.97      0.97      0.97     43206\n",
            "           3       0.98      0.97      0.97     42825\n",
            "           4       0.97      0.97      0.97     41257\n",
            "           5       0.96      0.98      0.97     36966\n",
            "           6       0.97      0.98      0.98     40196\n",
            "           7       0.97      0.97      0.97     43138\n",
            "           8       0.96      0.97      0.97     40409\n",
            "           9       0.96      0.97      0.97     42131\n",
            "\n",
            "    accuracy                           0.97    420000\n",
            "   macro avg       0.97      0.97      0.97    420000\n",
            "weighted avg       0.97      0.97      0.97    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.979 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     42812\n",
            "           1       0.99      0.98      0.99     49196\n",
            "           2       0.97      0.97      0.97     44242\n",
            "           3       0.98      0.97      0.97     43838\n",
            "           4       0.97      0.97      0.97     42233\n",
            "           5       0.96      0.98      0.97     37844\n",
            "           6       0.98      0.98      0.98     41151\n",
            "           7       0.97      0.97      0.97     44167\n",
            "           8       0.96      0.97      0.97     41374\n",
            "           9       0.96      0.97      0.97     43143\n",
            "\n",
            "    accuracy                           0.97    430000\n",
            "   macro avg       0.97      0.97      0.97    430000\n",
            "weighted avg       0.97      0.97      0.97    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.981 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     43806\n",
            "           1       0.99      0.98      0.99     50332\n",
            "           2       0.97      0.97      0.97     45281\n",
            "           3       0.98      0.97      0.97     44855\n",
            "           4       0.97      0.97      0.97     43214\n",
            "           5       0.96      0.98      0.97     38729\n",
            "           6       0.98      0.98      0.98     42111\n",
            "           7       0.97      0.97      0.97     45201\n",
            "           8       0.96      0.97      0.97     42334\n",
            "           9       0.96      0.97      0.97     44137\n",
            "\n",
            "    accuracy                           0.97    440000\n",
            "   macro avg       0.97      0.97      0.97    440000\n",
            "weighted avg       0.97      0.97      0.97    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     44805\n",
            "           1       0.99      0.98      0.99     51474\n",
            "           2       0.97      0.97      0.97     46310\n",
            "           3       0.98      0.97      0.97     45857\n",
            "           4       0.97      0.97      0.97     44201\n",
            "           5       0.97      0.98      0.97     39630\n",
            "           6       0.98      0.98      0.98     43061\n",
            "           7       0.97      0.97      0.97     46240\n",
            "           8       0.96      0.97      0.97     43287\n",
            "           9       0.96      0.97      0.97     45135\n",
            "\n",
            "    accuracy                           0.97    450000\n",
            "   macro avg       0.97      0.97      0.97    450000\n",
            "weighted avg       0.97      0.97      0.97    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.980 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     45800\n",
            "           1       0.99      0.98      0.99     52616\n",
            "           2       0.97      0.97      0.97     47343\n",
            "           3       0.98      0.97      0.97     46875\n",
            "           4       0.97      0.97      0.97     45192\n",
            "           5       0.97      0.98      0.97     40509\n",
            "           6       0.98      0.98      0.98     44009\n",
            "           7       0.97      0.97      0.97     47273\n",
            "           8       0.96      0.97      0.97     44248\n",
            "           9       0.96      0.97      0.97     46135\n",
            "\n",
            "    accuracy                           0.97    460000\n",
            "   macro avg       0.97      0.97      0.97    460000\n",
            "weighted avg       0.97      0.97      0.97    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.981 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     46794\n",
            "           1       0.99      0.98      0.99     53749\n",
            "           2       0.97      0.97      0.97     48387\n",
            "           3       0.98      0.97      0.97     47906\n",
            "           4       0.97      0.97      0.97     46167\n",
            "           5       0.97      0.98      0.97     41388\n",
            "           6       0.98      0.98      0.98     44962\n",
            "           7       0.97      0.97      0.97     48302\n",
            "           8       0.96      0.97      0.97     45205\n",
            "           9       0.96      0.97      0.97     47140\n",
            "\n",
            "    accuracy                           0.97    470000\n",
            "   macro avg       0.97      0.97      0.97    470000\n",
            "weighted avg       0.97      0.97      0.97    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     47792\n",
            "           1       0.99      0.98      0.99     54886\n",
            "           2       0.97      0.97      0.97     49412\n",
            "           3       0.98      0.97      0.97     48924\n",
            "           4       0.97      0.97      0.97     47154\n",
            "           5       0.97      0.98      0.97     42279\n",
            "           6       0.98      0.98      0.98     45909\n",
            "           7       0.97      0.97      0.97     49340\n",
            "           8       0.96      0.97      0.97     46163\n",
            "           9       0.96      0.97      0.97     48141\n",
            "\n",
            "    accuracy                           0.97    480000\n",
            "   macro avg       0.97      0.97      0.97    480000\n",
            "weighted avg       0.97      0.97      0.97    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.981 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     48784\n",
            "           1       0.99      0.98      0.99     56023\n",
            "           2       0.97      0.97      0.97     50448\n",
            "           3       0.98      0.97      0.97     49937\n",
            "           4       0.97      0.97      0.97     48137\n",
            "           5       0.97      0.98      0.97     43163\n",
            "           6       0.98      0.98      0.98     46868\n",
            "           7       0.97      0.97      0.97     50368\n",
            "           8       0.96      0.97      0.97     47127\n",
            "           9       0.96      0.97      0.97     49145\n",
            "\n",
            "    accuracy                           0.97    490000\n",
            "   macro avg       0.97      0.97      0.97    490000\n",
            "weighted avg       0.97      0.97      0.97    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.981 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     49777\n",
            "           1       0.99      0.98      0.99     57158\n",
            "           2       0.97      0.97      0.97     51491\n",
            "           3       0.98      0.97      0.97     50957\n",
            "           4       0.97      0.97      0.97     49120\n",
            "           5       0.97      0.98      0.97     44045\n",
            "           6       0.98      0.98      0.98     47826\n",
            "           7       0.97      0.97      0.97     51391\n",
            "           8       0.96      0.97      0.97     48085\n",
            "           9       0.96      0.97      0.97     50150\n",
            "\n",
            "    accuracy                           0.97    500000\n",
            "   macro avg       0.97      0.97      0.97    500000\n",
            "weighted avg       0.97      0.97      0.97    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.981 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     50770\n",
            "           1       0.99      0.98      0.99     58300\n",
            "           2       0.97      0.97      0.97     52530\n",
            "           3       0.98      0.97      0.97     51967\n",
            "           4       0.97      0.97      0.97     50106\n",
            "           5       0.97      0.98      0.97     44929\n",
            "           6       0.98      0.98      0.98     48782\n",
            "           7       0.97      0.97      0.97     52429\n",
            "           8       0.96      0.97      0.97     49039\n",
            "           9       0.96      0.97      0.97     51148\n",
            "\n",
            "    accuracy                           0.97    510000\n",
            "   macro avg       0.97      0.97      0.97    510000\n",
            "weighted avg       0.97      0.97      0.97    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.980 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     51762\n",
            "           1       0.99      0.98      0.99     59444\n",
            "           2       0.97      0.97      0.97     53573\n",
            "           3       0.98      0.97      0.97     52979\n",
            "           4       0.97      0.97      0.97     51091\n",
            "           5       0.97      0.98      0.97     45804\n",
            "           6       0.98      0.98      0.98     49737\n",
            "           7       0.97      0.97      0.97     53466\n",
            "           8       0.96      0.97      0.97     49998\n",
            "           9       0.96      0.97      0.97     52146\n",
            "\n",
            "    accuracy                           0.97    520000\n",
            "   macro avg       0.97      0.97      0.97    520000\n",
            "weighted avg       0.97      0.97      0.97    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     52759\n",
            "           1       0.99      0.98      0.99     60580\n",
            "           2       0.97      0.97      0.97     54620\n",
            "           3       0.98      0.97      0.97     53998\n",
            "           4       0.97      0.97      0.97     52078\n",
            "           5       0.97      0.98      0.97     46682\n",
            "           6       0.98      0.98      0.98     50696\n",
            "           7       0.97      0.97      0.97     54495\n",
            "           8       0.96      0.97      0.97     50949\n",
            "           9       0.96      0.97      0.97     53143\n",
            "\n",
            "    accuracy                           0.97    530000\n",
            "   macro avg       0.97      0.97      0.97    530000\n",
            "weighted avg       0.97      0.97      0.97    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.981 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     53752\n",
            "           1       0.99      0.98      0.99     61716\n",
            "           2       0.97      0.97      0.97     55654\n",
            "           3       0.98      0.97      0.98     55012\n",
            "           4       0.97      0.97      0.97     53053\n",
            "           5       0.97      0.98      0.97     47576\n",
            "           6       0.98      0.98      0.98     51651\n",
            "           7       0.97      0.97      0.97     55532\n",
            "           8       0.96      0.97      0.97     51912\n",
            "           9       0.96      0.97      0.97     54142\n",
            "\n",
            "    accuracy                           0.97    540000\n",
            "   macro avg       0.97      0.97      0.97    540000\n",
            "weighted avg       0.97      0.97      0.97    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.981 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     54751\n",
            "           1       0.99      0.98      0.99     62857\n",
            "           2       0.97      0.97      0.97     56692\n",
            "           3       0.98      0.97      0.98     56016\n",
            "           4       0.97      0.97      0.97     54042\n",
            "           5       0.97      0.98      0.97     48460\n",
            "           6       0.98      0.98      0.98     52605\n",
            "           7       0.97      0.97      0.97     56566\n",
            "           8       0.96      0.97      0.97     52868\n",
            "           9       0.96      0.97      0.97     55143\n",
            "\n",
            "    accuracy                           0.98    550000\n",
            "   macro avg       0.97      0.97      0.97    550000\n",
            "weighted avg       0.98      0.98      0.98    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.981 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     55741\n",
            "           1       0.99      0.98      0.99     63991\n",
            "           2       0.97      0.97      0.97     57736\n",
            "           3       0.98      0.97      0.98     57031\n",
            "           4       0.97      0.97      0.97     55015\n",
            "           5       0.97      0.98      0.97     49348\n",
            "           6       0.98      0.98      0.98     53562\n",
            "           7       0.97      0.97      0.97     57601\n",
            "           8       0.96      0.97      0.97     53821\n",
            "           9       0.96      0.97      0.97     56154\n",
            "\n",
            "    accuracy                           0.98    560000\n",
            "   macro avg       0.97      0.98      0.97    560000\n",
            "weighted avg       0.98      0.98      0.98    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     56736\n",
            "           1       0.99      0.98      0.99     65135\n",
            "           2       0.97      0.97      0.97     58765\n",
            "           3       0.98      0.97      0.98     58039\n",
            "           4       0.97      0.97      0.97     56001\n",
            "           5       0.97      0.98      0.97     50246\n",
            "           6       0.98      0.98      0.98     54518\n",
            "           7       0.97      0.97      0.97     58635\n",
            "           8       0.96      0.97      0.97     54773\n",
            "           9       0.96      0.97      0.97     57152\n",
            "\n",
            "    accuracy                           0.98    570000\n",
            "   macro avg       0.97      0.98      0.98    570000\n",
            "weighted avg       0.98      0.98      0.98    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     57731\n",
            "           1       0.99      0.98      0.99     66273\n",
            "           2       0.97      0.97      0.97     59802\n",
            "           3       0.98      0.97      0.98     59049\n",
            "           4       0.97      0.97      0.97     56983\n",
            "           5       0.97      0.98      0.97     51126\n",
            "           6       0.98      0.98      0.98     55476\n",
            "           7       0.97      0.97      0.97     59672\n",
            "           8       0.96      0.97      0.97     55729\n",
            "           9       0.97      0.97      0.97     58159\n",
            "\n",
            "    accuracy                           0.98    580000\n",
            "   macro avg       0.98      0.98      0.98    580000\n",
            "weighted avg       0.98      0.98      0.98    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.980 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     58723\n",
            "           1       0.99      0.98      0.99     67406\n",
            "           2       0.97      0.97      0.97     60839\n",
            "           3       0.98      0.97      0.98     60062\n",
            "           4       0.97      0.97      0.97     57962\n",
            "           5       0.97      0.98      0.97     52002\n",
            "           6       0.98      0.98      0.98     56437\n",
            "           7       0.97      0.97      0.97     60700\n",
            "           8       0.96      0.97      0.97     56689\n",
            "           9       0.97      0.97      0.97     59180\n",
            "\n",
            "    accuracy                           0.98    590000\n",
            "   macro avg       0.98      0.98      0.98    590000\n",
            "weighted avg       0.98      0.98      0.98    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.980 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     59715\n",
            "           1       0.99      0.98      0.99     68543\n",
            "           2       0.97      0.97      0.97     61875\n",
            "           3       0.98      0.97      0.98     61069\n",
            "           4       0.97      0.97      0.97     58935\n",
            "           5       0.97      0.98      0.97     52896\n",
            "           6       0.98      0.98      0.98     57382\n",
            "           7       0.97      0.97      0.97     61738\n",
            "           8       0.96      0.98      0.97     57649\n",
            "           9       0.97      0.97      0.97     60198\n",
            "\n",
            "    accuracy                           0.98    600000\n",
            "   macro avg       0.98      0.98      0.98    600000\n",
            "weighted avg       0.98      0.98      0.98    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.981 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     60717\n",
            "           1       0.99      0.98      0.99     69683\n",
            "           2       0.97      0.97      0.97     62916\n",
            "           3       0.98      0.97      0.98     62080\n",
            "           4       0.97      0.97      0.97     59925\n",
            "           5       0.97      0.98      0.97     53774\n",
            "           6       0.98      0.98      0.98     58336\n",
            "           7       0.97      0.97      0.97     62771\n",
            "           8       0.96      0.98      0.97     58609\n",
            "           9       0.97      0.97      0.97     61189\n",
            "\n",
            "    accuracy                           0.98    610000\n",
            "   macro avg       0.98      0.98      0.98    610000\n",
            "weighted avg       0.98      0.98      0.98    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.981 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     61709\n",
            "           1       0.99      0.98      0.99     70825\n",
            "           2       0.97      0.97      0.97     63953\n",
            "           3       0.98      0.97      0.98     63093\n",
            "           4       0.97      0.97      0.97     60919\n",
            "           5       0.97      0.98      0.97     54651\n",
            "           6       0.98      0.98      0.98     59295\n",
            "           7       0.97      0.97      0.97     63802\n",
            "           8       0.96      0.98      0.97     59565\n",
            "           9       0.97      0.97      0.97     62188\n",
            "\n",
            "    accuracy                           0.98    620000\n",
            "   macro avg       0.98      0.98      0.98    620000\n",
            "weighted avg       0.98      0.98      0.98    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.981 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     62705\n",
            "           1       0.99      0.98      0.99     71964\n",
            "           2       0.97      0.97      0.97     64987\n",
            "           3       0.98      0.97      0.98     64100\n",
            "           4       0.97      0.97      0.97     61896\n",
            "           5       0.97      0.98      0.97     55537\n",
            "           6       0.98      0.98      0.98     60247\n",
            "           7       0.97      0.97      0.97     64838\n",
            "           8       0.96      0.98      0.97     60524\n",
            "           9       0.97      0.97      0.97     63202\n",
            "\n",
            "    accuracy                           0.98    630000\n",
            "   macro avg       0.98      0.98      0.98    630000\n",
            "weighted avg       0.98      0.98      0.98    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.980 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     63699\n",
            "           1       0.99      0.98      0.99     73104\n",
            "           2       0.97      0.97      0.97     66026\n",
            "           3       0.98      0.97      0.98     65108\n",
            "           4       0.97      0.97      0.97     62886\n",
            "           5       0.97      0.98      0.97     56417\n",
            "           6       0.98      0.98      0.98     61201\n",
            "           7       0.97      0.97      0.97     65872\n",
            "           8       0.96      0.98      0.97     61483\n",
            "           9       0.97      0.97      0.97     64204\n",
            "\n",
            "    accuracy                           0.98    640000\n",
            "   macro avg       0.98      0.98      0.98    640000\n",
            "weighted avg       0.98      0.98      0.98    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.981 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     64690\n",
            "           1       0.99      0.98      0.99     74237\n",
            "           2       0.97      0.97      0.97     67066\n",
            "           3       0.98      0.97      0.98     66126\n",
            "           4       0.97      0.97      0.97     63871\n",
            "           5       0.97      0.98      0.97     57300\n",
            "           6       0.98      0.98      0.98     62163\n",
            "           7       0.97      0.97      0.97     66904\n",
            "           8       0.96      0.98      0.97     62436\n",
            "           9       0.97      0.97      0.97     65207\n",
            "\n",
            "    accuracy                           0.98    650000\n",
            "   macro avg       0.98      0.98      0.98    650000\n",
            "weighted avg       0.98      0.98      0.98    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.981 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     65685\n",
            "           1       0.99      0.99      0.99     75376\n",
            "           2       0.97      0.97      0.97     68101\n",
            "           3       0.98      0.97      0.98     67137\n",
            "           4       0.97      0.97      0.97     64843\n",
            "           5       0.97      0.98      0.97     58187\n",
            "           6       0.98      0.98      0.98     63125\n",
            "           7       0.97      0.97      0.97     67937\n",
            "           8       0.96      0.98      0.97     63384\n",
            "           9       0.97      0.97      0.97     66225\n",
            "\n",
            "    accuracy                           0.98    660000\n",
            "   macro avg       0.98      0.98      0.98    660000\n",
            "weighted avg       0.98      0.98      0.98    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.980 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     66683\n",
            "           1       0.99      0.98      0.99     76522\n",
            "           2       0.97      0.97      0.97     69143\n",
            "           3       0.98      0.97      0.98     68158\n",
            "           4       0.97      0.97      0.97     65827\n",
            "           5       0.97      0.98      0.97     59064\n",
            "           6       0.98      0.98      0.98     64082\n",
            "           7       0.97      0.97      0.97     68958\n",
            "           8       0.96      0.98      0.97     64339\n",
            "           9       0.97      0.97      0.97     67224\n",
            "\n",
            "    accuracy                           0.98    670000\n",
            "   macro avg       0.98      0.98      0.98    670000\n",
            "weighted avg       0.98      0.98      0.98    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.981 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     67676\n",
            "           1       0.99      0.99      0.99     77656\n",
            "           2       0.98      0.97      0.97     70177\n",
            "           3       0.98      0.97      0.98     69185\n",
            "           4       0.97      0.97      0.97     66811\n",
            "           5       0.97      0.98      0.97     59935\n",
            "           6       0.98      0.98      0.98     65042\n",
            "           7       0.97      0.97      0.97     69990\n",
            "           8       0.96      0.98      0.97     65302\n",
            "           9       0.97      0.97      0.97     68226\n",
            "\n",
            "    accuracy                           0.98    680000\n",
            "   macro avg       0.98      0.98      0.98    680000\n",
            "weighted avg       0.98      0.98      0.98    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.980 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     68672\n",
            "           1       0.99      0.99      0.99     78799\n",
            "           2       0.98      0.98      0.98     71201\n",
            "           3       0.98      0.97      0.98     70194\n",
            "           4       0.97      0.97      0.97     67790\n",
            "           5       0.97      0.98      0.97     60823\n",
            "           6       0.98      0.98      0.98     66001\n",
            "           7       0.97      0.97      0.97     71020\n",
            "           8       0.96      0.98      0.97     66260\n",
            "           9       0.97      0.97      0.97     69240\n",
            "\n",
            "    accuracy                           0.98    690000\n",
            "   macro avg       0.98      0.98      0.98    690000\n",
            "weighted avg       0.98      0.98      0.98    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     69666\n",
            "           1       0.99      0.99      0.99     79942\n",
            "           2       0.98      0.98      0.98     72235\n",
            "           3       0.98      0.97      0.98     71203\n",
            "           4       0.97      0.97      0.97     68769\n",
            "           5       0.97      0.98      0.97     61709\n",
            "           6       0.98      0.98      0.98     66959\n",
            "           7       0.97      0.97      0.97     72057\n",
            "           8       0.96      0.98      0.97     67216\n",
            "           9       0.97      0.97      0.97     70244\n",
            "\n",
            "    accuracy                           0.98    700000\n",
            "   macro avg       0.98      0.98      0.98    700000\n",
            "weighted avg       0.98      0.98      0.98    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.980 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     70656\n",
            "           1       0.99      0.99      0.99     81080\n",
            "           2       0.98      0.98      0.98     73275\n",
            "           3       0.98      0.97      0.98     72222\n",
            "           4       0.97      0.97      0.97     69754\n",
            "           5       0.97      0.98      0.98     62590\n",
            "           6       0.98      0.98      0.98     67917\n",
            "           7       0.97      0.97      0.97     73097\n",
            "           8       0.96      0.98      0.97     68170\n",
            "           9       0.97      0.97      0.97     71239\n",
            "\n",
            "    accuracy                           0.98    710000\n",
            "   macro avg       0.98      0.98      0.98    710000\n",
            "weighted avg       0.98      0.98      0.98    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.981 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     71655\n",
            "           1       0.99      0.99      0.99     82219\n",
            "           2       0.98      0.98      0.98     74306\n",
            "           3       0.98      0.97      0.98     73235\n",
            "           4       0.97      0.97      0.97     70727\n",
            "           5       0.97      0.98      0.98     63477\n",
            "           6       0.98      0.98      0.98     68869\n",
            "           7       0.97      0.97      0.97     74134\n",
            "           8       0.96      0.98      0.97     69127\n",
            "           9       0.97      0.97      0.97     72251\n",
            "\n",
            "    accuracy                           0.98    720000\n",
            "   macro avg       0.98      0.98      0.98    720000\n",
            "weighted avg       0.98      0.98      0.98    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.981 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     72651\n",
            "           1       0.99      0.99      0.99     83360\n",
            "           2       0.98      0.98      0.98     75343\n",
            "           3       0.98      0.97      0.98     74245\n",
            "           4       0.97      0.97      0.97     71705\n",
            "           5       0.97      0.98      0.98     64356\n",
            "           6       0.98      0.98      0.98     69828\n",
            "           7       0.97      0.97      0.97     75174\n",
            "           8       0.96      0.98      0.97     70079\n",
            "           9       0.97      0.97      0.97     73259\n",
            "\n",
            "    accuracy                           0.98    730000\n",
            "   macro avg       0.98      0.98      0.98    730000\n",
            "weighted avg       0.98      0.98      0.98    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.982 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     73644\n",
            "           1       0.99      0.99      0.99     84496\n",
            "           2       0.98      0.98      0.98     76376\n",
            "           3       0.98      0.97      0.98     75263\n",
            "           4       0.97      0.97      0.97     72685\n",
            "           5       0.97      0.98      0.98     65234\n",
            "           6       0.98      0.98      0.98     70787\n",
            "           7       0.97      0.97      0.97     76215\n",
            "           8       0.96      0.98      0.97     71039\n",
            "           9       0.97      0.97      0.97     74261\n",
            "\n",
            "    accuracy                           0.98    740000\n",
            "   macro avg       0.98      0.98      0.98    740000\n",
            "weighted avg       0.98      0.98      0.98    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.981 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     74641\n",
            "           1       0.99      0.99      0.99     85634\n",
            "           2       0.98      0.98      0.98     77409\n",
            "           3       0.98      0.97      0.98     76269\n",
            "           4       0.97      0.97      0.97     73667\n",
            "           5       0.97      0.98      0.98     66122\n",
            "           6       0.98      0.98      0.98     71743\n",
            "           7       0.97      0.97      0.97     77251\n",
            "           8       0.96      0.98      0.97     71991\n",
            "           9       0.97      0.97      0.97     75273\n",
            "\n",
            "    accuracy                           0.98    750000\n",
            "   macro avg       0.98      0.98      0.98    750000\n",
            "weighted avg       0.98      0.98      0.98    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.981 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     75636\n",
            "           1       0.99      0.99      0.99     86775\n",
            "           2       0.98      0.98      0.98     78444\n",
            "           3       0.98      0.97      0.98     77280\n",
            "           4       0.97      0.97      0.97     74659\n",
            "           5       0.97      0.98      0.98     67008\n",
            "           6       0.98      0.98      0.98     72697\n",
            "           7       0.97      0.97      0.97     78285\n",
            "           8       0.96      0.98      0.97     72940\n",
            "           9       0.97      0.97      0.97     76276\n",
            "\n",
            "    accuracy                           0.98    760000\n",
            "   macro avg       0.98      0.98      0.98    760000\n",
            "weighted avg       0.98      0.98      0.98    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.982 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     76630\n",
            "           1       0.99      0.99      0.99     87914\n",
            "           2       0.98      0.98      0.98     79482\n",
            "           3       0.98      0.97      0.98     78297\n",
            "           4       0.97      0.97      0.97     75639\n",
            "           5       0.97      0.98      0.98     67891\n",
            "           6       0.98      0.98      0.98     73654\n",
            "           7       0.97      0.97      0.97     79308\n",
            "           8       0.96      0.98      0.97     73899\n",
            "           9       0.97      0.97      0.97     77286\n",
            "\n",
            "    accuracy                           0.98    770000\n",
            "   macro avg       0.98      0.98      0.98    770000\n",
            "weighted avg       0.98      0.98      0.98    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.982 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     77628\n",
            "           1       0.99      0.99      0.99     89054\n",
            "           2       0.98      0.98      0.98     80513\n",
            "           3       0.98      0.97      0.98     79316\n",
            "           4       0.97      0.97      0.97     76619\n",
            "           5       0.97      0.98      0.98     68782\n",
            "           6       0.98      0.98      0.98     74607\n",
            "           7       0.97      0.97      0.97     80350\n",
            "           8       0.96      0.98      0.97     74845\n",
            "           9       0.97      0.97      0.97     78286\n",
            "\n",
            "    accuracy                           0.98    780000\n",
            "   macro avg       0.98      0.98      0.98    780000\n",
            "weighted avg       0.98      0.98      0.98    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.980 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     78623\n",
            "           1       0.99      0.99      0.99     90199\n",
            "           2       0.98      0.98      0.98     81551\n",
            "           3       0.98      0.97      0.98     80329\n",
            "           4       0.98      0.97      0.97     77604\n",
            "           5       0.97      0.98      0.98     69666\n",
            "           6       0.98      0.98      0.98     75562\n",
            "           7       0.97      0.97      0.97     81388\n",
            "           8       0.96      0.98      0.97     75792\n",
            "           9       0.97      0.97      0.97     79286\n",
            "\n",
            "    accuracy                           0.98    790000\n",
            "   macro avg       0.98      0.98      0.98    790000\n",
            "weighted avg       0.98      0.98      0.98    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.981 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     79621\n",
            "           1       0.99      0.99      0.99     91336\n",
            "           2       0.98      0.98      0.98     82591\n",
            "           3       0.98      0.97      0.98     81348\n",
            "           4       0.98      0.97      0.97     78587\n",
            "           5       0.97      0.98      0.98     70550\n",
            "           6       0.98      0.98      0.98     76522\n",
            "           7       0.97      0.97      0.97     82419\n",
            "           8       0.96      0.98      0.97     76738\n",
            "           9       0.97      0.97      0.97     80288\n",
            "\n",
            "    accuracy                           0.98    800000\n",
            "   macro avg       0.98      0.98      0.98    800000\n",
            "weighted avg       0.98      0.98      0.98    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.981 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     80610\n",
            "           1       0.99      0.99      0.99     92476\n",
            "           2       0.98      0.98      0.98     83626\n",
            "           3       0.98      0.97      0.98     82360\n",
            "           4       0.98      0.97      0.97     79567\n",
            "           5       0.97      0.98      0.98     71435\n",
            "           6       0.98      0.98      0.98     77475\n",
            "           7       0.97      0.97      0.97     83454\n",
            "           8       0.96      0.98      0.97     77697\n",
            "           9       0.97      0.97      0.97     81300\n",
            "\n",
            "    accuracy                           0.98    810000\n",
            "   macro avg       0.98      0.98      0.98    810000\n",
            "weighted avg       0.98      0.98      0.98    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.980 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     81606\n",
            "           1       0.99      0.99      0.99     93617\n",
            "           2       0.98      0.98      0.98     84663\n",
            "           3       0.98      0.97      0.98     83374\n",
            "           4       0.98      0.97      0.98     80555\n",
            "           5       0.97      0.98      0.98     72317\n",
            "           6       0.98      0.98      0.98     78429\n",
            "           7       0.97      0.97      0.97     84484\n",
            "           8       0.96      0.98      0.97     78648\n",
            "           9       0.97      0.97      0.97     82307\n",
            "\n",
            "    accuracy                           0.98    820000\n",
            "   macro avg       0.98      0.98      0.98    820000\n",
            "weighted avg       0.98      0.98      0.98    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     82597\n",
            "           1       0.99      0.99      0.99     94751\n",
            "           2       0.98      0.98      0.98     85706\n",
            "           3       0.98      0.97      0.98     84398\n",
            "           4       0.98      0.97      0.98     81533\n",
            "           5       0.97      0.98      0.98     73199\n",
            "           6       0.98      0.98      0.98     79382\n",
            "           7       0.97      0.97      0.97     85514\n",
            "           8       0.96      0.98      0.97     79602\n",
            "           9       0.97      0.97      0.97     83318\n",
            "\n",
            "    accuracy                           0.98    830000\n",
            "   macro avg       0.98      0.98      0.98    830000\n",
            "weighted avg       0.98      0.98      0.98    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.981 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     83594\n",
            "           1       0.99      0.99      0.99     95888\n",
            "           2       0.98      0.98      0.98     86734\n",
            "           3       0.98      0.97      0.98     85412\n",
            "           4       0.98      0.97      0.98     82510\n",
            "           5       0.97      0.98      0.98     74093\n",
            "           6       0.98      0.98      0.98     80333\n",
            "           7       0.97      0.97      0.97     86549\n",
            "           8       0.96      0.98      0.97     80556\n",
            "           9       0.97      0.97      0.97     84331\n",
            "\n",
            "    accuracy                           0.98    840000\n",
            "   macro avg       0.98      0.98      0.98    840000\n",
            "weighted avg       0.98      0.98      0.98    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.980 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     84587\n",
            "           1       0.99      0.99      0.99     97030\n",
            "           2       0.98      0.98      0.98     87773\n",
            "           3       0.98      0.97      0.98     86423\n",
            "           4       0.98      0.97      0.98     83495\n",
            "           5       0.97      0.98      0.98     74972\n",
            "           6       0.98      0.98      0.98     81291\n",
            "           7       0.97      0.97      0.97     87588\n",
            "           8       0.96      0.98      0.97     81508\n",
            "           9       0.97      0.97      0.97     85333\n",
            "\n",
            "    accuracy                           0.98    850000\n",
            "   macro avg       0.98      0.98      0.98    850000\n",
            "weighted avg       0.98      0.98      0.98    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.981 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     85579\n",
            "           1       0.99      0.99      0.99     98170\n",
            "           2       0.98      0.98      0.98     88810\n",
            "           3       0.98      0.97      0.98     87441\n",
            "           4       0.98      0.97      0.98     84484\n",
            "           5       0.97      0.98      0.98     75853\n",
            "           6       0.98      0.98      0.98     82247\n",
            "           7       0.97      0.97      0.97     88622\n",
            "           8       0.96      0.98      0.97     82458\n",
            "           9       0.97      0.97      0.97     86336\n",
            "\n",
            "    accuracy                           0.98    860000\n",
            "   macro avg       0.98      0.98      0.98    860000\n",
            "weighted avg       0.98      0.98      0.98    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.981 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     86571\n",
            "           1       0.99      0.99      0.99     99313\n",
            "           2       0.98      0.98      0.98     89837\n",
            "           3       0.98      0.97      0.98     88455\n",
            "           4       0.98      0.98      0.98     85454\n",
            "           5       0.97      0.98      0.98     76740\n",
            "           6       0.98      0.98      0.98     83205\n",
            "           7       0.97      0.97      0.97     89657\n",
            "           8       0.96      0.98      0.97     83420\n",
            "           9       0.97      0.97      0.97     87348\n",
            "\n",
            "    accuracy                           0.98    870000\n",
            "   macro avg       0.98      0.98      0.98    870000\n",
            "weighted avg       0.98      0.98      0.98    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.980 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     87564\n",
            "           1       0.99      0.99      0.99    100456\n",
            "           2       0.98      0.98      0.98     90873\n",
            "           3       0.98      0.97      0.98     89469\n",
            "           4       0.98      0.98      0.98     86431\n",
            "           5       0.97      0.98      0.98     77625\n",
            "           6       0.98      0.98      0.98     84164\n",
            "           7       0.97      0.97      0.97     90695\n",
            "           8       0.96      0.98      0.97     84371\n",
            "           9       0.97      0.97      0.97     88352\n",
            "\n",
            "    accuracy                           0.98    880000\n",
            "   macro avg       0.98      0.98      0.98    880000\n",
            "weighted avg       0.98      0.98      0.98    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.980 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     88555\n",
            "           1       0.99      0.99      0.99    101589\n",
            "           2       0.98      0.98      0.98     91913\n",
            "           3       0.98      0.97      0.98     90487\n",
            "           4       0.98      0.98      0.98     87422\n",
            "           5       0.97      0.98      0.98     78500\n",
            "           6       0.98      0.98      0.98     85122\n",
            "           7       0.97      0.97      0.97     91733\n",
            "           8       0.96      0.98      0.97     85327\n",
            "           9       0.97      0.97      0.97     89352\n",
            "\n",
            "    accuracy                           0.98    890000\n",
            "   macro avg       0.98      0.98      0.98    890000\n",
            "weighted avg       0.98      0.98      0.98    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.980 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     89550\n",
            "           1       0.99      0.99      0.99    102725\n",
            "           2       0.98      0.98      0.98     92939\n",
            "           3       0.98      0.97      0.98     91496\n",
            "           4       0.98      0.98      0.98     88399\n",
            "           5       0.97      0.98      0.98     79399\n",
            "           6       0.98      0.98      0.98     86077\n",
            "           7       0.97      0.97      0.97     92768\n",
            "           8       0.96      0.98      0.97     86284\n",
            "           9       0.97      0.97      0.97     90363\n",
            "\n",
            "    accuracy                           0.98    900000\n",
            "   macro avg       0.98      0.98      0.98    900000\n",
            "weighted avg       0.98      0.98      0.98    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(28*28, 128)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.linear3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.batch_norm1(self.linear1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.batch_norm2(self.linear2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "model = MnistModel()"
      ],
      "metadata": {
        "id": "-oDR00Iaosqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-b5F4DBAowu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8utd9eMwoy0D",
        "outputId": "5a4476be-3768-4aab-a84b-b499eb2f05a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.931 Loss: 0.267\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96      1011\n",
            "           1       0.99      0.94      0.96      1194\n",
            "           2       0.92      0.94      0.93      1012\n",
            "           3       0.91      0.92      0.92      1009\n",
            "           4       0.95      0.92      0.94      1008\n",
            "           5       0.90      0.91      0.90       882\n",
            "           6       0.95      0.96      0.96       954\n",
            "           7       0.92      0.92      0.92      1026\n",
            "           8       0.87      0.94      0.90       900\n",
            "           9       0.91      0.91      0.91      1004\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.950 Loss: 0.175\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      2022\n",
            "           1       0.99      0.95      0.97      2350\n",
            "           2       0.93      0.94      0.94      2044\n",
            "           3       0.93      0.91      0.92      2056\n",
            "           4       0.94      0.94      0.94      1969\n",
            "           5       0.90      0.93      0.91      1740\n",
            "           6       0.96      0.96      0.96      1927\n",
            "           7       0.93      0.94      0.93      2035\n",
            "           8       0.90      0.95      0.92      1845\n",
            "           9       0.92      0.93      0.92      2012\n",
            "\n",
            "    accuracy                           0.94     20000\n",
            "   macro avg       0.94      0.94      0.94     20000\n",
            "weighted avg       0.94      0.94      0.94     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.954 Loss: 0.154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      3032\n",
            "           1       0.99      0.96      0.97      3500\n",
            "           2       0.93      0.95      0.94      3038\n",
            "           3       0.94      0.92      0.93      3077\n",
            "           4       0.94      0.95      0.95      2937\n",
            "           5       0.92      0.93      0.92      2666\n",
            "           6       0.96      0.96      0.96      2866\n",
            "           7       0.93      0.95      0.94      3041\n",
            "           8       0.91      0.94      0.93      2815\n",
            "           9       0.93      0.93      0.93      3028\n",
            "\n",
            "    accuracy                           0.94     30000\n",
            "   macro avg       0.94      0.94      0.94     30000\n",
            "weighted avg       0.95      0.94      0.94     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.965 Loss: 0.122\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      4026\n",
            "           1       0.99      0.96      0.97      4652\n",
            "           2       0.94      0.95      0.95      4058\n",
            "           3       0.95      0.93      0.94      4116\n",
            "           4       0.95      0.95      0.95      3920\n",
            "           5       0.93      0.94      0.93      3529\n",
            "           6       0.96      0.96      0.96      3822\n",
            "           7       0.94      0.95      0.95      4068\n",
            "           8       0.92      0.95      0.93      3771\n",
            "           9       0.94      0.94      0.94      4038\n",
            "\n",
            "    accuracy                           0.95     40000\n",
            "   macro avg       0.95      0.95      0.95     40000\n",
            "weighted avg       0.95      0.95      0.95     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.967 Loss: 0.107\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97      5021\n",
            "           1       0.99      0.97      0.98      5798\n",
            "           2       0.94      0.96      0.95      5097\n",
            "           3       0.95      0.93      0.94      5145\n",
            "           4       0.95      0.96      0.95      4898\n",
            "           5       0.93      0.95      0.94      4366\n",
            "           6       0.96      0.97      0.97      4785\n",
            "           7       0.94      0.95      0.95      5093\n",
            "           8       0.93      0.95      0.94      4749\n",
            "           9       0.94      0.94      0.94      5048\n",
            "\n",
            "    accuracy                           0.95     50000\n",
            "   macro avg       0.95      0.95      0.95     50000\n",
            "weighted avg       0.95      0.95      0.95     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.967 Loss: 0.109\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98      6019\n",
            "           1       0.99      0.97      0.98      6950\n",
            "           2       0.95      0.96      0.95      6098\n",
            "           3       0.95      0.94      0.94      6164\n",
            "           4       0.96      0.96      0.96      5885\n",
            "           5       0.93      0.95      0.94      5267\n",
            "           6       0.96      0.97      0.97      5730\n",
            "           7       0.95      0.96      0.95      6114\n",
            "           8       0.93      0.95      0.94      5725\n",
            "           9       0.94      0.94      0.94      6048\n",
            "\n",
            "    accuracy                           0.96     60000\n",
            "   macro avg       0.96      0.96      0.96     60000\n",
            "weighted avg       0.96      0.96      0.96     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.971 Loss: 0.097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      7015\n",
            "           1       0.99      0.97      0.98      8104\n",
            "           2       0.95      0.96      0.96      7119\n",
            "           3       0.96      0.94      0.95      7191\n",
            "           4       0.96      0.96      0.96      6880\n",
            "           5       0.94      0.95      0.95      6144\n",
            "           6       0.97      0.97      0.97      6680\n",
            "           7       0.95      0.96      0.95      7135\n",
            "           8       0.94      0.96      0.95      6686\n",
            "           9       0.94      0.95      0.95      7046\n",
            "\n",
            "    accuracy                           0.96     70000\n",
            "   macro avg       0.96      0.96      0.96     70000\n",
            "weighted avg       0.96      0.96      0.96     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.972 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      8008\n",
            "           1       0.99      0.97      0.98      9250\n",
            "           2       0.95      0.96      0.96      8168\n",
            "           3       0.96      0.94      0.95      8221\n",
            "           4       0.96      0.96      0.96      7855\n",
            "           5       0.94      0.96      0.95      7012\n",
            "           6       0.97      0.97      0.97      7643\n",
            "           7       0.95      0.96      0.96      8143\n",
            "           8       0.94      0.96      0.95      7648\n",
            "           9       0.95      0.95      0.95      8052\n",
            "\n",
            "    accuracy                           0.96     80000\n",
            "   macro avg       0.96      0.96      0.96     80000\n",
            "weighted avg       0.96      0.96      0.96     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.971 Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      9010\n",
            "           1       0.99      0.97      0.98     10399\n",
            "           2       0.95      0.96      0.96      9184\n",
            "           3       0.96      0.94      0.95      9237\n",
            "           4       0.96      0.96      0.96      8834\n",
            "           5       0.94      0.96      0.95      7927\n",
            "           6       0.97      0.97      0.97      8576\n",
            "           7       0.95      0.96      0.96      9163\n",
            "           8       0.94      0.96      0.95      8601\n",
            "           9       0.95      0.95      0.95      9069\n",
            "\n",
            "    accuracy                           0.96     90000\n",
            "   macro avg       0.96      0.96      0.96     90000\n",
            "weighted avg       0.96      0.96      0.96     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.975 Loss: 0.083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     10000\n",
            "           1       0.99      0.97      0.98     11545\n",
            "           2       0.96      0.97      0.96     10211\n",
            "           3       0.96      0.95      0.95     10255\n",
            "           4       0.96      0.96      0.96      9821\n",
            "           5       0.95      0.96      0.95      8803\n",
            "           6       0.97      0.97      0.97      9527\n",
            "           7       0.95      0.96      0.96     10186\n",
            "           8       0.94      0.96      0.95      9577\n",
            "           9       0.95      0.95      0.95     10075\n",
            "\n",
            "    accuracy                           0.96    100000\n",
            "   macro avg       0.96      0.96      0.96    100000\n",
            "weighted avg       0.96      0.96      0.96    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.976 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     10996\n",
            "           1       0.99      0.97      0.98     12686\n",
            "           2       0.96      0.97      0.96     11239\n",
            "           3       0.96      0.95      0.96     11273\n",
            "           4       0.96      0.96      0.96     10799\n",
            "           5       0.95      0.96      0.95      9679\n",
            "           6       0.97      0.97      0.97     10480\n",
            "           7       0.95      0.96      0.96     11205\n",
            "           8       0.95      0.96      0.95     10557\n",
            "           9       0.95      0.95      0.95     11086\n",
            "\n",
            "    accuracy                           0.96    110000\n",
            "   macro avg       0.96      0.96      0.96    110000\n",
            "weighted avg       0.96      0.96      0.96    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.973 Loss: 0.090\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     11995\n",
            "           1       0.99      0.97      0.98     13826\n",
            "           2       0.96      0.97      0.96     12261\n",
            "           3       0.96      0.95      0.96     12284\n",
            "           4       0.96      0.96      0.96     11785\n",
            "           5       0.95      0.96      0.96     10588\n",
            "           6       0.97      0.97      0.97     11416\n",
            "           7       0.96      0.96      0.96     12235\n",
            "           8       0.95      0.96      0.95     11504\n",
            "           9       0.95      0.95      0.95     12106\n",
            "\n",
            "    accuracy                           0.96    120000\n",
            "   macro avg       0.96      0.96      0.96    120000\n",
            "weighted avg       0.96      0.96      0.96    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.977 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     12991\n",
            "           1       0.99      0.98      0.98     14970\n",
            "           2       0.96      0.97      0.96     13292\n",
            "           3       0.96      0.95      0.96     13286\n",
            "           4       0.97      0.96      0.97     12784\n",
            "           5       0.95      0.96      0.96     11465\n",
            "           6       0.97      0.98      0.97     12369\n",
            "           7       0.96      0.96      0.96     13260\n",
            "           8       0.95      0.96      0.96     12475\n",
            "           9       0.95      0.96      0.96     13108\n",
            "\n",
            "    accuracy                           0.97    130000\n",
            "   macro avg       0.96      0.97      0.96    130000\n",
            "weighted avg       0.97      0.97      0.97    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.977 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     13982\n",
            "           1       0.99      0.98      0.98     16108\n",
            "           2       0.96      0.97      0.96     14333\n",
            "           3       0.96      0.95      0.96     14302\n",
            "           4       0.97      0.97      0.97     13756\n",
            "           5       0.95      0.96      0.96     12344\n",
            "           6       0.97      0.98      0.97     13330\n",
            "           7       0.96      0.97      0.96     14272\n",
            "           8       0.95      0.96      0.96     13445\n",
            "           9       0.96      0.96      0.96     14128\n",
            "\n",
            "    accuracy                           0.97    140000\n",
            "   macro avg       0.97      0.97      0.97    140000\n",
            "weighted avg       0.97      0.97      0.97    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.975 Loss: 0.080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     14973\n",
            "           1       0.99      0.98      0.98     17246\n",
            "           2       0.96      0.97      0.97     15355\n",
            "           3       0.97      0.95      0.96     15324\n",
            "           4       0.97      0.97      0.97     14732\n",
            "           5       0.95      0.96      0.96     13241\n",
            "           6       0.97      0.98      0.97     14275\n",
            "           7       0.96      0.97      0.96     15295\n",
            "           8       0.95      0.97      0.96     14403\n",
            "           9       0.96      0.96      0.96     15156\n",
            "\n",
            "    accuracy                           0.97    150000\n",
            "   macro avg       0.97      0.97      0.97    150000\n",
            "weighted avg       0.97      0.97      0.97    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.978 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     15963\n",
            "           1       0.99      0.98      0.98     18391\n",
            "           2       0.96      0.97      0.97     16396\n",
            "           3       0.97      0.96      0.96     16330\n",
            "           4       0.97      0.97      0.97     15717\n",
            "           5       0.95      0.97      0.96     14119\n",
            "           6       0.97      0.98      0.97     15229\n",
            "           7       0.96      0.97      0.96     16321\n",
            "           8       0.95      0.97      0.96     15369\n",
            "           9       0.96      0.96      0.96     16165\n",
            "\n",
            "    accuracy                           0.97    160000\n",
            "   macro avg       0.97      0.97      0.97    160000\n",
            "weighted avg       0.97      0.97      0.97    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.979 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     16957\n",
            "           1       0.99      0.98      0.98     19531\n",
            "           2       0.96      0.97      0.97     17433\n",
            "           3       0.97      0.96      0.96     17363\n",
            "           4       0.97      0.97      0.97     16690\n",
            "           5       0.96      0.97      0.96     14996\n",
            "           6       0.97      0.98      0.97     16184\n",
            "           7       0.96      0.97      0.96     17332\n",
            "           8       0.95      0.97      0.96     16344\n",
            "           9       0.96      0.96      0.96     17170\n",
            "\n",
            "    accuracy                           0.97    170000\n",
            "   macro avg       0.97      0.97      0.97    170000\n",
            "weighted avg       0.97      0.97      0.97    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.975 Loss: 0.082\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     17957\n",
            "           1       0.99      0.98      0.98     20679\n",
            "           2       0.96      0.97      0.97     18448\n",
            "           3       0.97      0.96      0.96     18390\n",
            "           4       0.97      0.97      0.97     17666\n",
            "           5       0.96      0.97      0.96     15892\n",
            "           6       0.97      0.98      0.97     17127\n",
            "           7       0.96      0.97      0.96     18370\n",
            "           8       0.95      0.97      0.96     17290\n",
            "           9       0.96      0.96      0.96     18181\n",
            "\n",
            "    accuracy                           0.97    180000\n",
            "   macro avg       0.97      0.97      0.97    180000\n",
            "weighted avg       0.97      0.97      0.97    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.979 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     18947\n",
            "           1       0.99      0.98      0.98     21817\n",
            "           2       0.96      0.97      0.97     19477\n",
            "           3       0.97      0.96      0.96     19406\n",
            "           4       0.97      0.97      0.97     18675\n",
            "           5       0.96      0.97      0.96     16778\n",
            "           6       0.97      0.98      0.97     18081\n",
            "           7       0.96      0.97      0.96     19402\n",
            "           8       0.95      0.97      0.96     18242\n",
            "           9       0.96      0.96      0.96     19175\n",
            "\n",
            "    accuracy                           0.97    190000\n",
            "   macro avg       0.97      0.97      0.97    190000\n",
            "weighted avg       0.97      0.97      0.97    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     19947\n",
            "           1       0.99      0.98      0.98     22953\n",
            "           2       0.97      0.97      0.97     20520\n",
            "           3       0.97      0.96      0.96     20422\n",
            "           4       0.97      0.97      0.97     19649\n",
            "           5       0.96      0.97      0.96     17665\n",
            "           6       0.97      0.98      0.97     19036\n",
            "           7       0.96      0.97      0.97     20417\n",
            "           8       0.96      0.97      0.96     19204\n",
            "           9       0.96      0.96      0.96     20187\n",
            "\n",
            "    accuracy                           0.97    200000\n",
            "   macro avg       0.97      0.97      0.97    200000\n",
            "weighted avg       0.97      0.97      0.97    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.978 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     20948\n",
            "           1       0.99      0.98      0.98     24103\n",
            "           2       0.97      0.97      0.97     21545\n",
            "           3       0.97      0.96      0.97     21432\n",
            "           4       0.97      0.97      0.97     20627\n",
            "           5       0.96      0.97      0.96     18566\n",
            "           6       0.97      0.98      0.97     19980\n",
            "           7       0.96      0.97      0.97     21438\n",
            "           8       0.96      0.97      0.96     20157\n",
            "           9       0.96      0.96      0.96     21204\n",
            "\n",
            "    accuracy                           0.97    210000\n",
            "   macro avg       0.97      0.97      0.97    210000\n",
            "weighted avg       0.97      0.97      0.97    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.979 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     21937\n",
            "           1       0.99      0.98      0.98     25244\n",
            "           2       0.97      0.97      0.97     22574\n",
            "           3       0.97      0.96      0.97     22459\n",
            "           4       0.97      0.97      0.97     21604\n",
            "           5       0.96      0.97      0.96     19450\n",
            "           6       0.97      0.98      0.97     20926\n",
            "           7       0.96      0.97      0.97     22468\n",
            "           8       0.96      0.97      0.96     21121\n",
            "           9       0.96      0.96      0.96     22217\n",
            "\n",
            "    accuracy                           0.97    220000\n",
            "   macro avg       0.97      0.97      0.97    220000\n",
            "weighted avg       0.97      0.97      0.97    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.979 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     22931\n",
            "           1       0.99      0.98      0.98     26383\n",
            "           2       0.97      0.97      0.97     23599\n",
            "           3       0.97      0.96      0.97     23487\n",
            "           4       0.97      0.97      0.97     22573\n",
            "           5       0.96      0.97      0.96     20331\n",
            "           6       0.97      0.98      0.97     21887\n",
            "           7       0.96      0.97      0.97     23481\n",
            "           8       0.96      0.97      0.96     22098\n",
            "           9       0.96      0.96      0.96     23230\n",
            "\n",
            "    accuracy                           0.97    230000\n",
            "   macro avg       0.97      0.97      0.97    230000\n",
            "weighted avg       0.97      0.97      0.97    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.977 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     23923\n",
            "           1       0.99      0.98      0.98     27526\n",
            "           2       0.97      0.97      0.97     24631\n",
            "           3       0.97      0.96      0.97     24502\n",
            "           4       0.97      0.97      0.97     23548\n",
            "           5       0.96      0.97      0.96     21233\n",
            "           6       0.97      0.98      0.98     22835\n",
            "           7       0.96      0.97      0.97     24506\n",
            "           8       0.96      0.97      0.96     23053\n",
            "           9       0.96      0.96      0.96     24243\n",
            "\n",
            "    accuracy                           0.97    240000\n",
            "   macro avg       0.97      0.97      0.97    240000\n",
            "weighted avg       0.97      0.97      0.97    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     24915\n",
            "           1       0.99      0.98      0.98     28665\n",
            "           2       0.97      0.97      0.97     25657\n",
            "           3       0.97      0.96      0.97     25520\n",
            "           4       0.97      0.97      0.97     24540\n",
            "           5       0.96      0.97      0.97     22117\n",
            "           6       0.97      0.98      0.98     23788\n",
            "           7       0.96      0.97      0.97     25541\n",
            "           8       0.96      0.97      0.96     24013\n",
            "           9       0.96      0.96      0.96     25244\n",
            "\n",
            "    accuracy                           0.97    250000\n",
            "   macro avg       0.97      0.97      0.97    250000\n",
            "weighted avg       0.97      0.97      0.97    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.978 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     25909\n",
            "           1       0.99      0.98      0.99     29804\n",
            "           2       0.97      0.97      0.97     26702\n",
            "           3       0.97      0.96      0.97     26543\n",
            "           4       0.97      0.97      0.97     25518\n",
            "           5       0.96      0.97      0.97     22999\n",
            "           6       0.97      0.98      0.98     24748\n",
            "           7       0.96      0.97      0.97     26554\n",
            "           8       0.96      0.97      0.97     24978\n",
            "           9       0.96      0.96      0.96     26245\n",
            "\n",
            "    accuracy                           0.97    260000\n",
            "   macro avg       0.97      0.97      0.97    260000\n",
            "weighted avg       0.97      0.97      0.97    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.977 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     26906\n",
            "           1       0.99      0.98      0.99     30949\n",
            "           2       0.97      0.97      0.97     27722\n",
            "           3       0.97      0.96      0.97     27558\n",
            "           4       0.97      0.97      0.97     26506\n",
            "           5       0.96      0.97      0.97     23900\n",
            "           6       0.97      0.98      0.98     25685\n",
            "           7       0.96      0.97      0.97     27582\n",
            "           8       0.96      0.97      0.97     25937\n",
            "           9       0.96      0.96      0.96     27255\n",
            "\n",
            "    accuracy                           0.97    270000\n",
            "   macro avg       0.97      0.97      0.97    270000\n",
            "weighted avg       0.97      0.97      0.97    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.978 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     27904\n",
            "           1       0.99      0.98      0.99     32087\n",
            "           2       0.97      0.97      0.97     28746\n",
            "           3       0.97      0.96      0.97     28567\n",
            "           4       0.97      0.97      0.97     27498\n",
            "           5       0.96      0.97      0.97     24782\n",
            "           6       0.97      0.98      0.98     26636\n",
            "           7       0.96      0.97      0.97     28616\n",
            "           8       0.96      0.97      0.97     26899\n",
            "           9       0.97      0.96      0.96     28265\n",
            "\n",
            "    accuracy                           0.97    280000\n",
            "   macro avg       0.97      0.97      0.97    280000\n",
            "weighted avg       0.97      0.97      0.97    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     28904\n",
            "           1       0.99      0.98      0.99     33228\n",
            "           2       0.97      0.97      0.97     29785\n",
            "           3       0.97      0.96      0.97     29590\n",
            "           4       0.97      0.97      0.97     28481\n",
            "           5       0.96      0.97      0.97     25664\n",
            "           6       0.97      0.98      0.98     27591\n",
            "           7       0.96      0.97      0.97     29628\n",
            "           8       0.96      0.97      0.97     27860\n",
            "           9       0.97      0.97      0.97     29269\n",
            "\n",
            "    accuracy                           0.97    290000\n",
            "   macro avg       0.97      0.97      0.97    290000\n",
            "weighted avg       0.97      0.97      0.97    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.977 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     29903\n",
            "           1       0.99      0.98      0.99     34372\n",
            "           2       0.97      0.97      0.97     30807\n",
            "           3       0.97      0.96      0.97     30607\n",
            "           4       0.97      0.97      0.97     29466\n",
            "           5       0.96      0.97      0.97     26558\n",
            "           6       0.97      0.98      0.98     28534\n",
            "           7       0.97      0.97      0.97     30654\n",
            "           8       0.96      0.97      0.97     28814\n",
            "           9       0.97      0.97      0.97     30285\n",
            "\n",
            "    accuracy                           0.97    300000\n",
            "   macro avg       0.97      0.97      0.97    300000\n",
            "weighted avg       0.97      0.97      0.97    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.978 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     30901\n",
            "           1       0.99      0.98      0.99     35511\n",
            "           2       0.97      0.97      0.97     31830\n",
            "           3       0.97      0.96      0.97     31636\n",
            "           4       0.97      0.97      0.97     30446\n",
            "           5       0.96      0.97      0.97     27427\n",
            "           6       0.97      0.98      0.98     29494\n",
            "           7       0.97      0.97      0.97     31679\n",
            "           8       0.96      0.97      0.97     29780\n",
            "           9       0.97      0.97      0.97     31296\n",
            "\n",
            "    accuracy                           0.97    310000\n",
            "   macro avg       0.97      0.97      0.97    310000\n",
            "weighted avg       0.97      0.97      0.97    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.979 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     31896\n",
            "           1       0.99      0.98      0.99     36651\n",
            "           2       0.97      0.97      0.97     32868\n",
            "           3       0.97      0.96      0.97     32652\n",
            "           4       0.97      0.97      0.97     31420\n",
            "           5       0.96      0.97      0.97     28310\n",
            "           6       0.97      0.98      0.98     30453\n",
            "           7       0.97      0.97      0.97     32702\n",
            "           8       0.96      0.97      0.97     30735\n",
            "           9       0.97      0.97      0.97     32313\n",
            "\n",
            "    accuracy                           0.97    320000\n",
            "   macro avg       0.97      0.97      0.97    320000\n",
            "weighted avg       0.97      0.97      0.97    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.977 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     32894\n",
            "           1       0.99      0.98      0.99     37797\n",
            "           2       0.97      0.97      0.97     33878\n",
            "           3       0.97      0.96      0.97     33673\n",
            "           4       0.97      0.97      0.97     32399\n",
            "           5       0.96      0.97      0.97     29212\n",
            "           6       0.97      0.98      0.98     31401\n",
            "           7       0.97      0.97      0.97     33738\n",
            "           8       0.96      0.97      0.97     31679\n",
            "           9       0.97      0.97      0.97     33329\n",
            "\n",
            "    accuracy                           0.97    330000\n",
            "   macro avg       0.97      0.97      0.97    330000\n",
            "weighted avg       0.97      0.97      0.97    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     33884\n",
            "           1       0.99      0.98      0.99     38935\n",
            "           2       0.97      0.97      0.97     34913\n",
            "           3       0.97      0.96      0.97     34694\n",
            "           4       0.97      0.97      0.97     33383\n",
            "           5       0.96      0.97      0.97     30088\n",
            "           6       0.97      0.98      0.98     32355\n",
            "           7       0.97      0.97      0.97     34766\n",
            "           8       0.96      0.97      0.97     32635\n",
            "           9       0.97      0.97      0.97     34347\n",
            "\n",
            "    accuracy                           0.97    340000\n",
            "   macro avg       0.97      0.97      0.97    340000\n",
            "weighted avg       0.97      0.97      0.97    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.981 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     34879\n",
            "           1       0.99      0.98      0.99     40069\n",
            "           2       0.97      0.97      0.97     35952\n",
            "           3       0.98      0.96      0.97     35724\n",
            "           4       0.97      0.97      0.97     34361\n",
            "           5       0.96      0.97      0.97     30977\n",
            "           6       0.97      0.98      0.98     33309\n",
            "           7       0.97      0.97      0.97     35785\n",
            "           8       0.96      0.97      0.97     33590\n",
            "           9       0.97      0.97      0.97     35354\n",
            "\n",
            "    accuracy                           0.97    350000\n",
            "   macro avg       0.97      0.97      0.97    350000\n",
            "weighted avg       0.97      0.97      0.97    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     35873\n",
            "           1       0.99      0.98      0.99     41211\n",
            "           2       0.97      0.97      0.97     36974\n",
            "           3       0.98      0.97      0.97     36736\n",
            "           4       0.97      0.97      0.97     35342\n",
            "           5       0.96      0.97      0.97     31875\n",
            "           6       0.97      0.98      0.98     34264\n",
            "           7       0.97      0.97      0.97     36817\n",
            "           8       0.96      0.97      0.97     34542\n",
            "           9       0.97      0.97      0.97     36366\n",
            "\n",
            "    accuracy                           0.97    360000\n",
            "   macro avg       0.97      0.97      0.97    360000\n",
            "weighted avg       0.97      0.97      0.97    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     36866\n",
            "           1       0.99      0.98      0.99     42348\n",
            "           2       0.97      0.97      0.97     38006\n",
            "           3       0.98      0.97      0.97     37745\n",
            "           4       0.97      0.97      0.97     36330\n",
            "           5       0.97      0.97      0.97     32757\n",
            "           6       0.97      0.98      0.98     35219\n",
            "           7       0.97      0.97      0.97     37855\n",
            "           8       0.96      0.98      0.97     35493\n",
            "           9       0.97      0.97      0.97     37381\n",
            "\n",
            "    accuracy                           0.97    370000\n",
            "   macro avg       0.97      0.97      0.97    370000\n",
            "weighted avg       0.97      0.97      0.97    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.979 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     37861\n",
            "           1       0.99      0.98      0.99     43491\n",
            "           2       0.97      0.97      0.97     39046\n",
            "           3       0.98      0.97      0.97     38764\n",
            "           4       0.97      0.97      0.97     37318\n",
            "           5       0.97      0.97      0.97     33640\n",
            "           6       0.97      0.98      0.98     36171\n",
            "           7       0.97      0.97      0.97     38869\n",
            "           8       0.96      0.98      0.97     36453\n",
            "           9       0.97      0.97      0.97     38387\n",
            "\n",
            "    accuracy                           0.97    380000\n",
            "   macro avg       0.97      0.97      0.97    380000\n",
            "weighted avg       0.97      0.97      0.97    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.978 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     38855\n",
            "           1       0.99      0.98      0.99     44630\n",
            "           2       0.97      0.97      0.97     40075\n",
            "           3       0.98      0.97      0.97     39773\n",
            "           4       0.97      0.97      0.97     38304\n",
            "           5       0.97      0.97      0.97     34552\n",
            "           6       0.97      0.98      0.98     37114\n",
            "           7       0.97      0.97      0.97     39904\n",
            "           8       0.96      0.98      0.97     37398\n",
            "           9       0.97      0.97      0.97     39395\n",
            "\n",
            "    accuracy                           0.97    390000\n",
            "   macro avg       0.97      0.97      0.97    390000\n",
            "weighted avg       0.97      0.97      0.97    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     39845\n",
            "           1       0.99      0.98      0.99     45770\n",
            "           2       0.97      0.97      0.97     41119\n",
            "           3       0.98      0.97      0.97     40782\n",
            "           4       0.97      0.97      0.97     39288\n",
            "           5       0.97      0.97      0.97     35439\n",
            "           6       0.97      0.98      0.98     38066\n",
            "           7       0.97      0.97      0.97     40935\n",
            "           8       0.96      0.98      0.97     38355\n",
            "           9       0.97      0.97      0.97     40401\n",
            "\n",
            "    accuracy                           0.97    400000\n",
            "   macro avg       0.97      0.97      0.97    400000\n",
            "weighted avg       0.97      0.97      0.97    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.980 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     40838\n",
            "           1       0.99      0.98      0.99     46906\n",
            "           2       0.97      0.97      0.97     42161\n",
            "           3       0.98      0.97      0.97     41805\n",
            "           4       0.97      0.97      0.97     40271\n",
            "           5       0.97      0.97      0.97     36325\n",
            "           6       0.97      0.98      0.98     39017\n",
            "           7       0.97      0.97      0.97     41957\n",
            "           8       0.96      0.98      0.97     39313\n",
            "           9       0.97      0.97      0.97     41407\n",
            "\n",
            "    accuracy                           0.97    410000\n",
            "   macro avg       0.97      0.97      0.97    410000\n",
            "weighted avg       0.97      0.97      0.97    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.980 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     41829\n",
            "           1       0.99      0.98      0.99     48044\n",
            "           2       0.97      0.97      0.97     43189\n",
            "           3       0.98      0.97      0.97     42814\n",
            "           4       0.97      0.97      0.97     41242\n",
            "           5       0.97      0.97      0.97     37222\n",
            "           6       0.97      0.98      0.98     39977\n",
            "           7       0.97      0.97      0.97     42992\n",
            "           8       0.96      0.98      0.97     40257\n",
            "           9       0.97      0.97      0.97     42434\n",
            "\n",
            "    accuracy                           0.97    420000\n",
            "   macro avg       0.97      0.97      0.97    420000\n",
            "weighted avg       0.97      0.97      0.97    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.979 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     42819\n",
            "           1       0.99      0.98      0.99     49181\n",
            "           2       0.97      0.97      0.97     44220\n",
            "           3       0.98      0.97      0.97     43828\n",
            "           4       0.97      0.97      0.97     42219\n",
            "           5       0.97      0.97      0.97     38097\n",
            "           6       0.97      0.98      0.98     40937\n",
            "           7       0.97      0.97      0.97     44032\n",
            "           8       0.96      0.98      0.97     41215\n",
            "           9       0.97      0.97      0.97     43452\n",
            "\n",
            "    accuracy                           0.97    430000\n",
            "   macro avg       0.97      0.97      0.97    430000\n",
            "weighted avg       0.97      0.97      0.97    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.981 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     43808\n",
            "           1       0.99      0.98      0.99     50323\n",
            "           2       0.97      0.97      0.97     45263\n",
            "           3       0.98      0.97      0.97     44835\n",
            "           4       0.97      0.97      0.97     43201\n",
            "           5       0.97      0.97      0.97     38988\n",
            "           6       0.97      0.98      0.98     41900\n",
            "           7       0.97      0.97      0.97     45055\n",
            "           8       0.96      0.98      0.97     42173\n",
            "           9       0.97      0.97      0.97     44454\n",
            "\n",
            "    accuracy                           0.97    440000\n",
            "   macro avg       0.97      0.97      0.97    440000\n",
            "weighted avg       0.97      0.97      0.97    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.978 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     44804\n",
            "           1       0.99      0.98      0.99     51459\n",
            "           2       0.97      0.98      0.97     46285\n",
            "           3       0.98      0.97      0.97     45853\n",
            "           4       0.97      0.97      0.97     44182\n",
            "           5       0.97      0.97      0.97     39877\n",
            "           6       0.97      0.98      0.98     42855\n",
            "           7       0.97      0.97      0.97     46093\n",
            "           8       0.96      0.98      0.97     43122\n",
            "           9       0.97      0.97      0.97     45470\n",
            "\n",
            "    accuracy                           0.97    450000\n",
            "   macro avg       0.97      0.97      0.97    450000\n",
            "weighted avg       0.97      0.97      0.97    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.978 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     45798\n",
            "           1       0.99      0.98      0.99     52599\n",
            "           2       0.97      0.98      0.97     47328\n",
            "           3       0.98      0.97      0.97     46861\n",
            "           4       0.97      0.97      0.97     45161\n",
            "           5       0.97      0.97      0.97     40760\n",
            "           6       0.97      0.98      0.98     43806\n",
            "           7       0.97      0.97      0.97     47126\n",
            "           8       0.96      0.98      0.97     44087\n",
            "           9       0.97      0.97      0.97     46474\n",
            "\n",
            "    accuracy                           0.97    460000\n",
            "   macro avg       0.97      0.97      0.97    460000\n",
            "weighted avg       0.97      0.97      0.97    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.979 Loss: 0.069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     46787\n",
            "           1       0.99      0.98      0.99     53736\n",
            "           2       0.97      0.98      0.97     48358\n",
            "           3       0.98      0.97      0.97     47879\n",
            "           4       0.97      0.97      0.97     46148\n",
            "           5       0.97      0.97      0.97     41637\n",
            "           6       0.97      0.98      0.98     44763\n",
            "           7       0.97      0.97      0.97     48158\n",
            "           8       0.96      0.98      0.97     45052\n",
            "           9       0.97      0.97      0.97     47482\n",
            "\n",
            "    accuracy                           0.97    470000\n",
            "   macro avg       0.97      0.97      0.97    470000\n",
            "weighted avg       0.97      0.97      0.97    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.978 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     47781\n",
            "           1       0.99      0.98      0.99     54884\n",
            "           2       0.97      0.98      0.97     49382\n",
            "           3       0.98      0.97      0.97     48889\n",
            "           4       0.97      0.97      0.97     47127\n",
            "           5       0.97      0.97      0.97     42546\n",
            "           6       0.97      0.98      0.98     45705\n",
            "           7       0.97      0.97      0.97     49192\n",
            "           8       0.96      0.98      0.97     46001\n",
            "           9       0.97      0.97      0.97     48493\n",
            "\n",
            "    accuracy                           0.97    480000\n",
            "   macro avg       0.97      0.97      0.97    480000\n",
            "weighted avg       0.97      0.97      0.97    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.979 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     48771\n",
            "           1       0.99      0.98      0.99     56026\n",
            "           2       0.97      0.98      0.97     50417\n",
            "           3       0.98      0.97      0.97     49899\n",
            "           4       0.97      0.97      0.97     48106\n",
            "           5       0.97      0.97      0.97     43428\n",
            "           6       0.97      0.98      0.98     46664\n",
            "           7       0.97      0.97      0.97     50226\n",
            "           8       0.96      0.98      0.97     46959\n",
            "           9       0.97      0.97      0.97     49504\n",
            "\n",
            "    accuracy                           0.97    490000\n",
            "   macro avg       0.97      0.97      0.97    490000\n",
            "weighted avg       0.98      0.97      0.97    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     49766\n",
            "           1       0.99      0.98      0.99     57174\n",
            "           2       0.97      0.98      0.97     51451\n",
            "           3       0.98      0.97      0.97     50905\n",
            "           4       0.97      0.98      0.97     49079\n",
            "           5       0.97      0.97      0.97     44322\n",
            "           6       0.97      0.98      0.98     47624\n",
            "           7       0.97      0.97      0.97     51249\n",
            "           8       0.96      0.98      0.97     47917\n",
            "           9       0.97      0.97      0.97     50513\n",
            "\n",
            "    accuracy                           0.98    500000\n",
            "   macro avg       0.97      0.97      0.97    500000\n",
            "weighted avg       0.98      0.98      0.98    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.978 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     50756\n",
            "           1       0.99      0.98      0.99     58314\n",
            "           2       0.97      0.98      0.97     52478\n",
            "           3       0.98      0.97      0.97     51909\n",
            "           4       0.97      0.98      0.97     50066\n",
            "           5       0.97      0.97      0.97     45240\n",
            "           6       0.97      0.98      0.98     48566\n",
            "           7       0.97      0.97      0.97     52278\n",
            "           8       0.96      0.98      0.97     48868\n",
            "           9       0.97      0.97      0.97     51525\n",
            "\n",
            "    accuracy                           0.98    510000\n",
            "   macro avg       0.97      0.98      0.97    510000\n",
            "weighted avg       0.98      0.98      0.98    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.979 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     51747\n",
            "           1       0.99      0.98      0.99     59455\n",
            "           2       0.97      0.98      0.97     53511\n",
            "           3       0.98      0.97      0.97     52931\n",
            "           4       0.97      0.98      0.98     51050\n",
            "           5       0.97      0.97      0.97     46120\n",
            "           6       0.97      0.98      0.98     49521\n",
            "           7       0.97      0.97      0.97     53305\n",
            "           8       0.96      0.98      0.97     49826\n",
            "           9       0.97      0.97      0.97     52534\n",
            "\n",
            "    accuracy                           0.98    520000\n",
            "   macro avg       0.97      0.98      0.97    520000\n",
            "weighted avg       0.98      0.98      0.98    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.980 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     52733\n",
            "           1       0.99      0.98      0.99     60596\n",
            "           2       0.97      0.98      0.97     54551\n",
            "           3       0.98      0.97      0.97     53955\n",
            "           4       0.98      0.98      0.98     52034\n",
            "           5       0.97      0.97      0.97     47005\n",
            "           6       0.97      0.98      0.98     50481\n",
            "           7       0.97      0.97      0.97     54334\n",
            "           8       0.96      0.98      0.97     50786\n",
            "           9       0.97      0.97      0.97     53525\n",
            "\n",
            "    accuracy                           0.98    530000\n",
            "   macro avg       0.97      0.98      0.98    530000\n",
            "weighted avg       0.98      0.98      0.98    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.978 Loss: 0.079\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     53729\n",
            "           1       0.99      0.98      0.99     61741\n",
            "           2       0.97      0.98      0.97     55582\n",
            "           3       0.98      0.97      0.97     54963\n",
            "           4       0.98      0.98      0.98     53019\n",
            "           5       0.97      0.97      0.97     47911\n",
            "           6       0.97      0.98      0.98     51416\n",
            "           7       0.97      0.97      0.97     55376\n",
            "           8       0.96      0.98      0.97     51727\n",
            "           9       0.97      0.97      0.97     54536\n",
            "\n",
            "    accuracy                           0.98    540000\n",
            "   macro avg       0.98      0.98      0.98    540000\n",
            "weighted avg       0.98      0.98      0.98    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.980 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     54718\n",
            "           1       0.99      0.98      0.99     62886\n",
            "           2       0.97      0.98      0.97     56615\n",
            "           3       0.98      0.97      0.97     55980\n",
            "           4       0.98      0.98      0.98     54002\n",
            "           5       0.97      0.97      0.97     48792\n",
            "           6       0.97      0.98      0.98     52375\n",
            "           7       0.97      0.97      0.97     56408\n",
            "           8       0.96      0.98      0.97     52682\n",
            "           9       0.97      0.97      0.97     55542\n",
            "\n",
            "    accuracy                           0.98    550000\n",
            "   macro avg       0.98      0.98      0.98    550000\n",
            "weighted avg       0.98      0.98      0.98    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.980 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     55705\n",
            "           1       0.99      0.98      0.99     64032\n",
            "           2       0.97      0.98      0.97     57660\n",
            "           3       0.98      0.97      0.97     57004\n",
            "           4       0.98      0.98      0.98     54980\n",
            "           5       0.97      0.97      0.97     49679\n",
            "           6       0.97      0.98      0.98     53332\n",
            "           7       0.97      0.97      0.97     57421\n",
            "           8       0.96      0.98      0.97     53636\n",
            "           9       0.97      0.97      0.97     56551\n",
            "\n",
            "    accuracy                           0.98    560000\n",
            "   macro avg       0.98      0.98      0.98    560000\n",
            "weighted avg       0.98      0.98      0.98    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.980 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     56698\n",
            "           1       0.99      0.98      0.99     65175\n",
            "           2       0.97      0.98      0.97     58682\n",
            "           3       0.98      0.97      0.97     58028\n",
            "           4       0.98      0.98      0.98     55959\n",
            "           5       0.97      0.97      0.97     50570\n",
            "           6       0.97      0.98      0.98     54283\n",
            "           7       0.97      0.97      0.97     58450\n",
            "           8       0.96      0.98      0.97     54587\n",
            "           9       0.97      0.97      0.97     57568\n",
            "\n",
            "    accuracy                           0.98    570000\n",
            "   macro avg       0.98      0.98      0.98    570000\n",
            "weighted avg       0.98      0.98      0.98    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.982 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     57686\n",
            "           1       0.99      0.98      0.99     66315\n",
            "           2       0.97      0.98      0.97     59718\n",
            "           3       0.98      0.97      0.97     59044\n",
            "           4       0.98      0.98      0.98     56936\n",
            "           5       0.97      0.97      0.97     51451\n",
            "           6       0.98      0.98      0.98     55245\n",
            "           7       0.97      0.97      0.97     59481\n",
            "           8       0.96      0.98      0.97     55549\n",
            "           9       0.97      0.97      0.97     58575\n",
            "\n",
            "    accuracy                           0.98    580000\n",
            "   macro avg       0.98      0.98      0.98    580000\n",
            "weighted avg       0.98      0.98      0.98    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.981 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     58675\n",
            "           1       0.99      0.98      0.99     67458\n",
            "           2       0.97      0.98      0.97     60753\n",
            "           3       0.98      0.97      0.97     60071\n",
            "           4       0.98      0.98      0.98     57920\n",
            "           5       0.97      0.97      0.97     52333\n",
            "           6       0.98      0.98      0.98     56205\n",
            "           7       0.97      0.97      0.97     60500\n",
            "           8       0.96      0.98      0.97     56502\n",
            "           9       0.97      0.97      0.97     59583\n",
            "\n",
            "    accuracy                           0.98    590000\n",
            "   macro avg       0.98      0.98      0.98    590000\n",
            "weighted avg       0.98      0.98      0.98    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.979 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     59672\n",
            "           1       0.99      0.98      0.99     68599\n",
            "           2       0.97      0.98      0.97     61783\n",
            "           3       0.98      0.97      0.97     61097\n",
            "           4       0.98      0.98      0.98     58890\n",
            "           5       0.97      0.97      0.97     53233\n",
            "           6       0.98      0.98      0.98     57154\n",
            "           7       0.97      0.97      0.97     61527\n",
            "           8       0.96      0.98      0.97     57447\n",
            "           9       0.97      0.97      0.97     60598\n",
            "\n",
            "    accuracy                           0.98    600000\n",
            "   macro avg       0.98      0.98      0.98    600000\n",
            "weighted avg       0.98      0.98      0.98    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.979 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     60665\n",
            "           1       0.99      0.98      0.99     69737\n",
            "           2       0.97      0.98      0.98     62811\n",
            "           3       0.98      0.97      0.97     62119\n",
            "           4       0.98      0.98      0.98     59869\n",
            "           5       0.97      0.97      0.97     54121\n",
            "           6       0.98      0.98      0.98     58112\n",
            "           7       0.97      0.97      0.97     62559\n",
            "           8       0.96      0.98      0.97     58405\n",
            "           9       0.97      0.97      0.97     61602\n",
            "\n",
            "    accuracy                           0.98    610000\n",
            "   macro avg       0.98      0.98      0.98    610000\n",
            "weighted avg       0.98      0.98      0.98    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.981 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     61650\n",
            "           1       0.99      0.98      0.99     70877\n",
            "           2       0.97      0.98      0.98     63849\n",
            "           3       0.98      0.97      0.97     63144\n",
            "           4       0.98      0.98      0.98     60847\n",
            "           5       0.97      0.97      0.97     55002\n",
            "           6       0.98      0.98      0.98     59073\n",
            "           7       0.97      0.97      0.97     63582\n",
            "           8       0.96      0.98      0.97     59362\n",
            "           9       0.97      0.97      0.97     62614\n",
            "\n",
            "    accuracy                           0.98    620000\n",
            "   macro avg       0.98      0.98      0.98    620000\n",
            "weighted avg       0.98      0.98      0.98    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.979 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     62647\n",
            "           1       0.99      0.98      0.99     72019\n",
            "           2       0.97      0.98      0.98     64873\n",
            "           3       0.98      0.97      0.97     64167\n",
            "           4       0.98      0.98      0.98     61825\n",
            "           5       0.97      0.97      0.97     55899\n",
            "           6       0.98      0.98      0.98     60024\n",
            "           7       0.97      0.97      0.97     64616\n",
            "           8       0.96      0.98      0.97     60307\n",
            "           9       0.97      0.97      0.97     63623\n",
            "\n",
            "    accuracy                           0.98    630000\n",
            "   macro avg       0.98      0.98      0.98    630000\n",
            "weighted avg       0.98      0.98      0.98    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.980 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     63634\n",
            "           1       0.99      0.98      0.99     73164\n",
            "           2       0.97      0.98      0.98     65903\n",
            "           3       0.98      0.97      0.97     65178\n",
            "           4       0.98      0.98      0.98     62813\n",
            "           5       0.97      0.97      0.97     56790\n",
            "           6       0.98      0.98      0.98     60981\n",
            "           7       0.97      0.97      0.97     65644\n",
            "           8       0.96      0.98      0.97     61262\n",
            "           9       0.97      0.97      0.97     64631\n",
            "\n",
            "    accuracy                           0.98    640000\n",
            "   macro avg       0.98      0.98      0.98    640000\n",
            "weighted avg       0.98      0.98      0.98    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.981 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     64623\n",
            "           1       0.99      0.98      0.99     74302\n",
            "           2       0.97      0.98      0.98     66933\n",
            "           3       0.98      0.97      0.97     66206\n",
            "           4       0.98      0.98      0.98     63791\n",
            "           5       0.97      0.97      0.97     57680\n",
            "           6       0.98      0.98      0.98     61935\n",
            "           7       0.97      0.97      0.97     66671\n",
            "           8       0.96      0.98      0.97     62223\n",
            "           9       0.97      0.97      0.97     65636\n",
            "\n",
            "    accuracy                           0.98    650000\n",
            "   macro avg       0.98      0.98      0.98    650000\n",
            "weighted avg       0.98      0.98      0.98    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.979 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     65617\n",
            "           1       0.99      0.98      0.99     75443\n",
            "           2       0.97      0.98      0.98     67967\n",
            "           3       0.98      0.97      0.97     67229\n",
            "           4       0.98      0.98      0.98     64772\n",
            "           5       0.97      0.97      0.97     58583\n",
            "           6       0.98      0.98      0.98     62877\n",
            "           7       0.97      0.97      0.97     67696\n",
            "           8       0.96      0.98      0.97     63170\n",
            "           9       0.97      0.97      0.97     66646\n",
            "\n",
            "    accuracy                           0.98    660000\n",
            "   macro avg       0.98      0.98      0.98    660000\n",
            "weighted avg       0.98      0.98      0.98    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.979 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     66606\n",
            "           1       0.99      0.98      0.99     76581\n",
            "           2       0.97      0.98      0.98     68998\n",
            "           3       0.98      0.97      0.97     68252\n",
            "           4       0.98      0.98      0.98     65751\n",
            "           5       0.97      0.97      0.97     59467\n",
            "           6       0.98      0.98      0.98     63839\n",
            "           7       0.97      0.97      0.97     68731\n",
            "           8       0.96      0.98      0.97     64125\n",
            "           9       0.97      0.97      0.97     67650\n",
            "\n",
            "    accuracy                           0.98    670000\n",
            "   macro avg       0.98      0.98      0.98    670000\n",
            "weighted avg       0.98      0.98      0.98    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.981 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     67595\n",
            "           1       0.99      0.98      0.99     77720\n",
            "           2       0.97      0.98      0.98     70045\n",
            "           3       0.98      0.97      0.97     69271\n",
            "           4       0.98      0.98      0.98     66732\n",
            "           5       0.97      0.97      0.97     60344\n",
            "           6       0.98      0.98      0.98     64804\n",
            "           7       0.97      0.97      0.97     69751\n",
            "           8       0.96      0.98      0.97     65083\n",
            "           9       0.97      0.97      0.97     68655\n",
            "\n",
            "    accuracy                           0.98    680000\n",
            "   macro avg       0.98      0.98      0.98    680000\n",
            "weighted avg       0.98      0.98      0.98    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.980 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     68591\n",
            "           1       0.99      0.98      0.99     78861\n",
            "           2       0.97      0.98      0.98     71069\n",
            "           3       0.98      0.97      0.97     70275\n",
            "           4       0.98      0.98      0.98     67714\n",
            "           5       0.97      0.97      0.97     61249\n",
            "           6       0.98      0.98      0.98     65753\n",
            "           7       0.97      0.97      0.97     70786\n",
            "           8       0.96      0.98      0.97     66038\n",
            "           9       0.97      0.97      0.97     69664\n",
            "\n",
            "    accuracy                           0.98    690000\n",
            "   macro avg       0.98      0.98      0.98    690000\n",
            "weighted avg       0.98      0.98      0.98    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.979 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     69582\n",
            "           1       0.99      0.98      0.99     80001\n",
            "           2       0.97      0.98      0.98     72108\n",
            "           3       0.98      0.97      0.97     71288\n",
            "           4       0.98      0.98      0.98     68696\n",
            "           5       0.97      0.97      0.97     62134\n",
            "           6       0.98      0.98      0.98     66710\n",
            "           7       0.97      0.97      0.97     71818\n",
            "           8       0.96      0.98      0.97     66991\n",
            "           9       0.97      0.97      0.97     70672\n",
            "\n",
            "    accuracy                           0.98    700000\n",
            "   macro avg       0.98      0.98      0.98    700000\n",
            "weighted avg       0.98      0.98      0.98    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.980 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     70574\n",
            "           1       0.99      0.98      0.99     81141\n",
            "           2       0.97      0.98      0.98     73152\n",
            "           3       0.98      0.97      0.97     72301\n",
            "           4       0.98      0.98      0.98     69691\n",
            "           5       0.97      0.97      0.97     63020\n",
            "           6       0.98      0.98      0.98     67666\n",
            "           7       0.97      0.97      0.97     72835\n",
            "           8       0.96      0.98      0.97     67949\n",
            "           9       0.97      0.97      0.97     71671\n",
            "\n",
            "    accuracy                           0.98    710000\n",
            "   macro avg       0.98      0.98      0.98    710000\n",
            "weighted avg       0.98      0.98      0.98    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.980 Loss: 0.073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     71570\n",
            "           1       0.99      0.98      0.99     82283\n",
            "           2       0.97      0.98      0.98     74178\n",
            "           3       0.98      0.97      0.97     73315\n",
            "           4       0.98      0.98      0.98     70675\n",
            "           5       0.97      0.97      0.97     63910\n",
            "           6       0.98      0.98      0.98     68616\n",
            "           7       0.97      0.97      0.97     73873\n",
            "           8       0.96      0.98      0.97     68901\n",
            "           9       0.97      0.97      0.97     72679\n",
            "\n",
            "    accuracy                           0.98    720000\n",
            "   macro avg       0.98      0.98      0.98    720000\n",
            "weighted avg       0.98      0.98      0.98    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.979 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     72561\n",
            "           1       0.99      0.98      0.99     83429\n",
            "           2       0.97      0.98      0.98     75215\n",
            "           3       0.98      0.97      0.97     74328\n",
            "           4       0.98      0.98      0.98     71665\n",
            "           5       0.97      0.97      0.97     64797\n",
            "           6       0.98      0.98      0.98     69568\n",
            "           7       0.97      0.97      0.97     74905\n",
            "           8       0.96      0.98      0.97     69859\n",
            "           9       0.97      0.97      0.97     73673\n",
            "\n",
            "    accuracy                           0.98    730000\n",
            "   macro avg       0.98      0.98      0.98    730000\n",
            "weighted avg       0.98      0.98      0.98    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.982 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     73555\n",
            "           1       0.99      0.98      0.99     84575\n",
            "           2       0.97      0.98      0.98     76253\n",
            "           3       0.98      0.97      0.97     75335\n",
            "           4       0.98      0.98      0.98     72649\n",
            "           5       0.97      0.97      0.97     65692\n",
            "           6       0.98      0.98      0.98     70524\n",
            "           7       0.97      0.97      0.97     75929\n",
            "           8       0.96      0.98      0.97     70813\n",
            "           9       0.97      0.97      0.97     74675\n",
            "\n",
            "    accuracy                           0.98    740000\n",
            "   macro avg       0.98      0.98      0.98    740000\n",
            "weighted avg       0.98      0.98      0.98    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.981 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     74542\n",
            "           1       0.99      0.98      0.99     85716\n",
            "           2       0.98      0.98      0.98     77280\n",
            "           3       0.98      0.97      0.97     76341\n",
            "           4       0.98      0.98      0.98     73627\n",
            "           5       0.97      0.97      0.97     66592\n",
            "           6       0.98      0.98      0.98     71479\n",
            "           7       0.97      0.97      0.97     76955\n",
            "           8       0.96      0.98      0.97     71766\n",
            "           9       0.97      0.97      0.97     75702\n",
            "\n",
            "    accuracy                           0.98    750000\n",
            "   macro avg       0.98      0.98      0.98    750000\n",
            "weighted avg       0.98      0.98      0.98    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.981 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     75530\n",
            "           1       0.99      0.98      0.99     86860\n",
            "           2       0.98      0.98      0.98     78310\n",
            "           3       0.98      0.97      0.97     77353\n",
            "           4       0.98      0.98      0.98     74609\n",
            "           5       0.97      0.97      0.97     67472\n",
            "           6       0.98      0.98      0.98     72442\n",
            "           7       0.97      0.97      0.97     77986\n",
            "           8       0.96      0.98      0.97     72726\n",
            "           9       0.97      0.97      0.97     76712\n",
            "\n",
            "    accuracy                           0.98    760000\n",
            "   macro avg       0.98      0.98      0.98    760000\n",
            "weighted avg       0.98      0.98      0.98    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.982 Loss: 0.072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     76519\n",
            "           1       0.99      0.98      0.99     88002\n",
            "           2       0.98      0.98      0.98     79351\n",
            "           3       0.98      0.97      0.97     78353\n",
            "           4       0.98      0.98      0.98     75585\n",
            "           5       0.97      0.97      0.97     68364\n",
            "           6       0.98      0.98      0.98     73404\n",
            "           7       0.97      0.97      0.97     79011\n",
            "           8       0.96      0.98      0.97     73689\n",
            "           9       0.97      0.97      0.97     77722\n",
            "\n",
            "    accuracy                           0.98    770000\n",
            "   macro avg       0.98      0.98      0.98    770000\n",
            "weighted avg       0.98      0.98      0.98    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.979 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     77509\n",
            "           1       0.99      0.98      0.99     89144\n",
            "           2       0.98      0.98      0.98     80379\n",
            "           3       0.98      0.97      0.97     79357\n",
            "           4       0.98      0.98      0.98     76562\n",
            "           5       0.97      0.97      0.97     69262\n",
            "           6       0.98      0.98      0.98     74357\n",
            "           7       0.97      0.97      0.97     80045\n",
            "           8       0.96      0.98      0.97     74645\n",
            "           9       0.97      0.97      0.97     78740\n",
            "\n",
            "    accuracy                           0.98    780000\n",
            "   macro avg       0.98      0.98      0.98    780000\n",
            "weighted avg       0.98      0.98      0.98    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.980 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     78497\n",
            "           1       0.99      0.98      0.99     90284\n",
            "           2       0.98      0.98      0.98     81415\n",
            "           3       0.98      0.97      0.97     80370\n",
            "           4       0.98      0.98      0.98     77537\n",
            "           5       0.97      0.97      0.97     70152\n",
            "           6       0.98      0.98      0.98     75317\n",
            "           7       0.97      0.97      0.97     81075\n",
            "           8       0.96      0.98      0.97     75599\n",
            "           9       0.97      0.97      0.97     79754\n",
            "\n",
            "    accuracy                           0.98    790000\n",
            "   macro avg       0.98      0.98      0.98    790000\n",
            "weighted avg       0.98      0.98      0.98    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.979 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     79486\n",
            "           1       0.99      0.98      0.99     91425\n",
            "           2       0.98      0.98      0.98     82453\n",
            "           3       0.98      0.97      0.97     81388\n",
            "           4       0.98      0.98      0.98     78511\n",
            "           5       0.97      0.97      0.97     71034\n",
            "           6       0.98      0.98      0.98     76279\n",
            "           7       0.97      0.97      0.97     82100\n",
            "           8       0.96      0.98      0.97     76554\n",
            "           9       0.97      0.97      0.97     80770\n",
            "\n",
            "    accuracy                           0.98    800000\n",
            "   macro avg       0.98      0.98      0.98    800000\n",
            "weighted avg       0.98      0.98      0.98    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.979 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     80475\n",
            "           1       0.99      0.98      0.99     92570\n",
            "           2       0.98      0.98      0.98     83481\n",
            "           3       0.98      0.97      0.97     82408\n",
            "           4       0.98      0.98      0.98     79494\n",
            "           5       0.97      0.97      0.97     71933\n",
            "           6       0.98      0.98      0.98     77220\n",
            "           7       0.97      0.97      0.97     83126\n",
            "           8       0.96      0.98      0.97     77508\n",
            "           9       0.97      0.97      0.97     81785\n",
            "\n",
            "    accuracy                           0.98    810000\n",
            "   macro avg       0.98      0.98      0.98    810000\n",
            "weighted avg       0.98      0.98      0.98    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.979 Loss: 0.080\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     81463\n",
            "           1       0.99      0.98      0.99     93717\n",
            "           2       0.98      0.98      0.98     84519\n",
            "           3       0.98      0.97      0.97     83418\n",
            "           4       0.98      0.98      0.98     80464\n",
            "           5       0.97      0.97      0.97     72823\n",
            "           6       0.98      0.98      0.98     78178\n",
            "           7       0.97      0.97      0.97     84155\n",
            "           8       0.96      0.98      0.97     78466\n",
            "           9       0.97      0.97      0.97     82797\n",
            "\n",
            "    accuracy                           0.98    820000\n",
            "   macro avg       0.98      0.98      0.98    820000\n",
            "weighted avg       0.98      0.98      0.98    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.981 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     82455\n",
            "           1       0.99      0.98      0.99     94861\n",
            "           2       0.98      0.98      0.98     85560\n",
            "           3       0.98      0.97      0.97     84439\n",
            "           4       0.98      0.98      0.98     81435\n",
            "           5       0.97      0.97      0.97     73710\n",
            "           6       0.98      0.98      0.98     79133\n",
            "           7       0.97      0.97      0.97     85180\n",
            "           8       0.96      0.98      0.97     79420\n",
            "           9       0.97      0.97      0.97     83807\n",
            "\n",
            "    accuracy                           0.98    830000\n",
            "   macro avg       0.98      0.98      0.98    830000\n",
            "weighted avg       0.98      0.98      0.98    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.980 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     83444\n",
            "           1       0.99      0.98      0.99     96009\n",
            "           2       0.98      0.98      0.98     86587\n",
            "           3       0.98      0.97      0.97     85438\n",
            "           4       0.98      0.98      0.98     82420\n",
            "           5       0.97      0.97      0.97     74608\n",
            "           6       0.98      0.98      0.98     80091\n",
            "           7       0.97      0.97      0.97     86213\n",
            "           8       0.96      0.98      0.97     80373\n",
            "           9       0.97      0.97      0.97     84817\n",
            "\n",
            "    accuracy                           0.98    840000\n",
            "   macro avg       0.98      0.98      0.98    840000\n",
            "weighted avg       0.98      0.98      0.98    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.979 Loss: 0.078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     84433\n",
            "           1       0.99      0.98      0.99     97154\n",
            "           2       0.98      0.98      0.98     87628\n",
            "           3       0.98      0.97      0.97     86452\n",
            "           4       0.98      0.98      0.98     83398\n",
            "           5       0.97      0.97      0.97     75488\n",
            "           6       0.98      0.98      0.98     81046\n",
            "           7       0.97      0.97      0.97     87246\n",
            "           8       0.96      0.98      0.97     81327\n",
            "           9       0.97      0.97      0.97     85828\n",
            "\n",
            "    accuracy                           0.98    850000\n",
            "   macro avg       0.98      0.98      0.98    850000\n",
            "weighted avg       0.98      0.98      0.98    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.980 Loss: 0.077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     85424\n",
            "           1       0.99      0.98      0.99     98303\n",
            "           2       0.98      0.98      0.98     88667\n",
            "           3       0.98      0.97      0.97     87467\n",
            "           4       0.98      0.98      0.98     84378\n",
            "           5       0.97      0.97      0.97     76377\n",
            "           6       0.98      0.98      0.98     82002\n",
            "           7       0.97      0.97      0.97     88260\n",
            "           8       0.96      0.98      0.97     82287\n",
            "           9       0.97      0.97      0.97     86835\n",
            "\n",
            "    accuracy                           0.98    860000\n",
            "   macro avg       0.98      0.98      0.98    860000\n",
            "weighted avg       0.98      0.98      0.98    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.980 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     86413\n",
            "           1       0.99      0.98      0.99     99446\n",
            "           2       0.98      0.98      0.98     89698\n",
            "           3       0.98      0.97      0.97     88471\n",
            "           4       0.98      0.98      0.98     85363\n",
            "           5       0.97      0.97      0.97     77283\n",
            "           6       0.98      0.98      0.98     82951\n",
            "           7       0.97      0.97      0.97     89295\n",
            "           8       0.96      0.98      0.97     83233\n",
            "           9       0.97      0.97      0.97     87847\n",
            "\n",
            "    accuracy                           0.98    870000\n",
            "   macro avg       0.98      0.98      0.98    870000\n",
            "weighted avg       0.98      0.98      0.98    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.981 Loss: 0.079\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     87405\n",
            "           1       0.99      0.98      0.99    100589\n",
            "           2       0.98      0.98      0.98     90742\n",
            "           3       0.98      0.97      0.97     89482\n",
            "           4       0.98      0.98      0.98     86344\n",
            "           5       0.97      0.97      0.97     78166\n",
            "           6       0.98      0.98      0.98     83905\n",
            "           7       0.97      0.97      0.97     90323\n",
            "           8       0.96      0.98      0.97     84189\n",
            "           9       0.97      0.97      0.97     88855\n",
            "\n",
            "    accuracy                           0.98    880000\n",
            "   macro avg       0.98      0.98      0.98    880000\n",
            "weighted avg       0.98      0.98      0.98    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.981 Loss: 0.074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     88392\n",
            "           1       0.99      0.98      0.99    101727\n",
            "           2       0.98      0.98      0.98     91783\n",
            "           3       0.98      0.97      0.97     90497\n",
            "           4       0.98      0.98      0.98     87324\n",
            "           5       0.97      0.97      0.97     79046\n",
            "           6       0.98      0.98      0.98     84862\n",
            "           7       0.97      0.97      0.97     91350\n",
            "           8       0.96      0.98      0.97     85149\n",
            "           9       0.97      0.97      0.97     89870\n",
            "\n",
            "    accuracy                           0.98    890000\n",
            "   macro avg       0.98      0.98      0.98    890000\n",
            "weighted avg       0.98      0.98      0.98    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.980 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     89383\n",
            "           1       0.99      0.98      0.99    102868\n",
            "           2       0.98      0.98      0.98     92816\n",
            "           3       0.98      0.97      0.97     91507\n",
            "           4       0.98      0.98      0.98     88299\n",
            "           5       0.97      0.97      0.97     79945\n",
            "           6       0.98      0.98      0.98     85817\n",
            "           7       0.97      0.97      0.97     92379\n",
            "           8       0.96      0.98      0.97     86095\n",
            "           9       0.97      0.97      0.97     90891\n",
            "\n",
            "    accuracy                           0.98    900000\n",
            "   macro avg       0.98      0.98      0.98    900000\n",
            "weighted avg       0.98      0.98      0.98    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(28*28, 256)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.linear2 = nn.Linear(256, 128)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.linear3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.leaky_relu(self.batch_norm1(self.linear1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.leaky_relu(self.batch_norm2(self.linear2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "model = MnistModel()\n"
      ],
      "metadata": {
        "id": "Tq1CFiEDrsLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "vPnP4R07ryla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps = 0\n",
        "print_every = 200\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "preds_hist = []\n",
        "labels_hist = []\n",
        "device = 'cpu'\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        steps += 1\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images.type(torch.FloatTensor).to(device))\n",
        "        labels = labels.to(device)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if steps % print_every == 0:\n",
        "            accuracy = 0\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                for images, labels in test_loader:\n",
        "                    preds = model(images.type(torch.FloatTensor).to(device))\n",
        "                    labels = labels.to(device)\n",
        "                    labels_hist.extend(np.squeeze(labels.numpy()))\n",
        "                    test_loss += loss_fn(preds, labels)\n",
        "                    ps = torch.exp(preds)\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    preds_hist.extend(np.squeeze(top_class.numpy()))\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            train_loss_hist.append(running_loss/len(train_loader))\n",
        "            test_loss_hist.append(test_loss/len(test_loader))\n",
        "\n",
        "            print(\"Epoch: {}/{}.. \".format(e + 1, NUM_EPOCHS),\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)),\n",
        "                  \"Loss: {:.3f}\".format(test_loss/len(test_loader)))\n",
        "            print(metrics.classification_report(preds_hist,labels_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZSpBA7Qr014",
        "outputId": "6c375f0e-c6aa-48d2-b9b7-4e1af3902649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30..  Test Accuracy: 0.942 Loss: 0.204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      1018\n",
            "           1       0.99      0.96      0.97      1172\n",
            "           2       0.93      0.94      0.94      1015\n",
            "           3       0.93      0.93      0.93      1007\n",
            "           4       0.96      0.92      0.94      1024\n",
            "           5       0.93      0.93      0.93       893\n",
            "           6       0.95      0.97      0.96       942\n",
            "           7       0.93      0.94      0.94      1019\n",
            "           8       0.90      0.96      0.93       913\n",
            "           9       0.91      0.92      0.92       997\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.954 Loss: 0.146\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97      2026\n",
            "           1       0.99      0.97      0.98      2327\n",
            "           2       0.94      0.94      0.94      2060\n",
            "           3       0.94      0.93      0.93      2062\n",
            "           4       0.96      0.94      0.95      1999\n",
            "           5       0.93      0.94      0.94      1749\n",
            "           6       0.96      0.97      0.96      1900\n",
            "           7       0.93      0.95      0.94      2011\n",
            "           8       0.92      0.96      0.94      1864\n",
            "           9       0.93      0.93      0.93      2002\n",
            "\n",
            "    accuracy                           0.95     20000\n",
            "   macro avg       0.95      0.95      0.95     20000\n",
            "weighted avg       0.95      0.95      0.95     20000\n",
            "\n",
            "Epoch: 1/30..  Test Accuracy: 0.959 Loss: 0.132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97      3037\n",
            "           1       0.99      0.97      0.98      3481\n",
            "           2       0.94      0.95      0.95      3060\n",
            "           3       0.95      0.93      0.94      3093\n",
            "           4       0.95      0.95      0.95      2958\n",
            "           5       0.94      0.94      0.94      2669\n",
            "           6       0.96      0.97      0.96      2828\n",
            "           7       0.94      0.95      0.95      3031\n",
            "           8       0.93      0.96      0.94      2826\n",
            "           9       0.93      0.94      0.94      3017\n",
            "\n",
            "    accuracy                           0.95     30000\n",
            "   macro avg       0.95      0.95      0.95     30000\n",
            "weighted avg       0.95      0.95      0.95     30000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.968 Loss: 0.109\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97      4022\n",
            "           1       0.99      0.97      0.98      4632\n",
            "           2       0.95      0.96      0.95      4080\n",
            "           3       0.95      0.94      0.95      4119\n",
            "           4       0.96      0.95      0.95      3975\n",
            "           5       0.94      0.95      0.95      3536\n",
            "           6       0.96      0.98      0.97      3761\n",
            "           7       0.94      0.96      0.95      4054\n",
            "           8       0.93      0.96      0.95      3806\n",
            "           9       0.94      0.94      0.94      4015\n",
            "\n",
            "    accuracy                           0.96     40000\n",
            "   macro avg       0.96      0.96      0.96     40000\n",
            "weighted avg       0.96      0.96      0.96     40000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.971 Loss: 0.091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      5011\n",
            "           1       0.99      0.97      0.98      5788\n",
            "           2       0.95      0.96      0.95      5124\n",
            "           3       0.96      0.94      0.95      5157\n",
            "           4       0.96      0.95      0.96      4952\n",
            "           5       0.94      0.96      0.95      4394\n",
            "           6       0.96      0.98      0.97      4712\n",
            "           7       0.95      0.96      0.95      5065\n",
            "           8       0.94      0.96      0.95      4774\n",
            "           9       0.94      0.95      0.94      5023\n",
            "\n",
            "    accuracy                           0.96     50000\n",
            "   macro avg       0.96      0.96      0.96     50000\n",
            "weighted avg       0.96      0.96      0.96     50000\n",
            "\n",
            "Epoch: 2/30..  Test Accuracy: 0.970 Loss: 0.095\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      6014\n",
            "           1       0.99      0.97      0.98      6939\n",
            "           2       0.95      0.96      0.96      6135\n",
            "           3       0.96      0.94      0.95      6170\n",
            "           4       0.96      0.96      0.96      5938\n",
            "           5       0.95      0.95      0.95      5316\n",
            "           6       0.96      0.98      0.97      5641\n",
            "           7       0.95      0.96      0.96      6086\n",
            "           8       0.94      0.96      0.95      5729\n",
            "           9       0.95      0.95      0.95      6032\n",
            "\n",
            "    accuracy                           0.96     60000\n",
            "   macro avg       0.96      0.96      0.96     60000\n",
            "weighted avg       0.96      0.96      0.96     60000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.974 Loss: 0.088\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      6999\n",
            "           1       0.99      0.97      0.98      8090\n",
            "           2       0.96      0.96      0.96      7159\n",
            "           3       0.96      0.94      0.95      7210\n",
            "           4       0.97      0.96      0.96      6934\n",
            "           5       0.95      0.96      0.95      6185\n",
            "           6       0.96      0.98      0.97      6593\n",
            "           7       0.95      0.96      0.96      7113\n",
            "           8       0.94      0.96      0.95      6690\n",
            "           9       0.95      0.95      0.95      7027\n",
            "\n",
            "    accuracy                           0.96     70000\n",
            "   macro avg       0.96      0.96      0.96     70000\n",
            "weighted avg       0.96      0.96      0.96     70000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.977 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      7992\n",
            "           1       0.99      0.98      0.98      9231\n",
            "           2       0.96      0.96      0.96      8203\n",
            "           3       0.97      0.95      0.96      8227\n",
            "           4       0.97      0.96      0.96      7900\n",
            "           5       0.95      0.96      0.96      7066\n",
            "           6       0.96      0.98      0.97      7549\n",
            "           7       0.95      0.96      0.96      8136\n",
            "           8       0.95      0.96      0.96      7658\n",
            "           9       0.95      0.95      0.95      8038\n",
            "\n",
            "    accuracy                           0.96     80000\n",
            "   macro avg       0.96      0.96      0.96     80000\n",
            "weighted avg       0.96      0.96      0.96     80000\n",
            "\n",
            "Epoch: 3/30..  Test Accuracy: 0.974 Loss: 0.081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      8988\n",
            "           1       0.99      0.98      0.98     10377\n",
            "           2       0.96      0.97      0.96      9218\n",
            "           3       0.97      0.95      0.96      9237\n",
            "           4       0.97      0.96      0.97      8873\n",
            "           5       0.95      0.96      0.96      7969\n",
            "           6       0.97      0.98      0.97      8496\n",
            "           7       0.96      0.97      0.96      9162\n",
            "           8       0.95      0.97      0.96      8630\n",
            "           9       0.95      0.96      0.95      9050\n",
            "\n",
            "    accuracy                           0.97     90000\n",
            "   macro avg       0.97      0.97      0.97     90000\n",
            "weighted avg       0.97      0.97      0.97     90000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.976 Loss: 0.075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      9975\n",
            "           1       0.99      0.98      0.98     11515\n",
            "           2       0.96      0.97      0.96     10238\n",
            "           3       0.97      0.95      0.96     10252\n",
            "           4       0.97      0.96      0.97      9863\n",
            "           5       0.96      0.96      0.96      8857\n",
            "           6       0.97      0.98      0.97      9444\n",
            "           7       0.96      0.97      0.96     10194\n",
            "           8       0.95      0.97      0.96      9595\n",
            "           9       0.95      0.96      0.96     10067\n",
            "\n",
            "    accuracy                           0.97    100000\n",
            "   macro avg       0.97      0.97      0.97    100000\n",
            "weighted avg       0.97      0.97      0.97    100000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.978 Loss: 0.071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     10965\n",
            "           1       0.99      0.98      0.99     12656\n",
            "           2       0.96      0.97      0.96     11273\n",
            "           3       0.97      0.95      0.96     11286\n",
            "           4       0.97      0.97      0.97     10832\n",
            "           5       0.96      0.97      0.96      9733\n",
            "           6       0.97      0.98      0.97     10395\n",
            "           7       0.96      0.97      0.96     11207\n",
            "           8       0.95      0.97      0.96     10579\n",
            "           9       0.96      0.96      0.96     11074\n",
            "\n",
            "    accuracy                           0.97    110000\n",
            "   macro avg       0.97      0.97      0.97    110000\n",
            "weighted avg       0.97      0.97      0.97    110000\n",
            "\n",
            "Epoch: 4/30..  Test Accuracy: 0.974 Loss: 0.076\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     11962\n",
            "           1       0.99      0.98      0.99     13795\n",
            "           2       0.96      0.97      0.97     12285\n",
            "           3       0.97      0.96      0.96     12306\n",
            "           4       0.97      0.97      0.97     11819\n",
            "           5       0.96      0.96      0.96     10657\n",
            "           6       0.97      0.98      0.97     11324\n",
            "           7       0.96      0.97      0.96     12233\n",
            "           8       0.95      0.97      0.96     11533\n",
            "           9       0.96      0.96      0.96     12086\n",
            "\n",
            "    accuracy                           0.97    120000\n",
            "   macro avg       0.97      0.97      0.97    120000\n",
            "weighted avg       0.97      0.97      0.97    120000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.978 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     12948\n",
            "           1       0.99      0.98      0.99     14936\n",
            "           2       0.96      0.97      0.97     13312\n",
            "           3       0.97      0.96      0.96     13329\n",
            "           4       0.97      0.97      0.97     12810\n",
            "           5       0.96      0.96      0.96     11553\n",
            "           6       0.97      0.98      0.97     12258\n",
            "           7       0.96      0.97      0.96     13262\n",
            "           8       0.96      0.97      0.96     12505\n",
            "           9       0.96      0.96      0.96     13087\n",
            "\n",
            "    accuracy                           0.97    130000\n",
            "   macro avg       0.97      0.97      0.97    130000\n",
            "weighted avg       0.97      0.97      0.97    130000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.979 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     13942\n",
            "           1       0.99      0.98      0.99     16074\n",
            "           2       0.96      0.97      0.97     14349\n",
            "           3       0.97      0.96      0.97     14357\n",
            "           4       0.97      0.97      0.97     13792\n",
            "           5       0.96      0.97      0.96     12432\n",
            "           6       0.97      0.98      0.97     13211\n",
            "           7       0.96      0.97      0.97     14278\n",
            "           8       0.96      0.97      0.96     13475\n",
            "           9       0.96      0.96      0.96     14090\n",
            "\n",
            "    accuracy                           0.97    140000\n",
            "   macro avg       0.97      0.97      0.97    140000\n",
            "weighted avg       0.97      0.97      0.97    140000\n",
            "\n",
            "Epoch: 5/30..  Test Accuracy: 0.977 Loss: 0.070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     14940\n",
            "           1       0.99      0.98      0.99     17218\n",
            "           2       0.96      0.97      0.97     15366\n",
            "           3       0.97      0.96      0.97     15382\n",
            "           4       0.97      0.97      0.97     14777\n",
            "           5       0.96      0.97      0.96     13327\n",
            "           6       0.97      0.98      0.97     14147\n",
            "           7       0.96      0.97      0.97     15309\n",
            "           8       0.96      0.97      0.96     14439\n",
            "           9       0.96      0.96      0.96     15095\n",
            "\n",
            "    accuracy                           0.97    150000\n",
            "   macro avg       0.97      0.97      0.97    150000\n",
            "weighted avg       0.97      0.97      0.97    150000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.981 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     15922\n",
            "           1       0.99      0.98      0.99     18361\n",
            "           2       0.97      0.97      0.97     16400\n",
            "           3       0.97      0.96      0.97     16399\n",
            "           4       0.97      0.97      0.97     15756\n",
            "           5       0.96      0.97      0.97     14213\n",
            "           6       0.97      0.98      0.98     15095\n",
            "           7       0.96      0.97      0.97     16339\n",
            "           8       0.96      0.97      0.96     15409\n",
            "           9       0.96      0.96      0.96     16106\n",
            "\n",
            "    accuracy                           0.97    160000\n",
            "   macro avg       0.97      0.97      0.97    160000\n",
            "weighted avg       0.97      0.97      0.97    160000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.980 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     16915\n",
            "           1       0.99      0.98      0.99     19500\n",
            "           2       0.97      0.97      0.97     17436\n",
            "           3       0.97      0.96      0.97     17429\n",
            "           4       0.97      0.97      0.97     16730\n",
            "           5       0.96      0.97      0.97     15100\n",
            "           6       0.97      0.98      0.98     16042\n",
            "           7       0.96      0.97      0.97     17350\n",
            "           8       0.96      0.97      0.97     16376\n",
            "           9       0.96      0.96      0.96     17122\n",
            "\n",
            "    accuracy                           0.97    170000\n",
            "   macro avg       0.97      0.97      0.97    170000\n",
            "weighted avg       0.97      0.97      0.97    170000\n",
            "\n",
            "Epoch: 6/30..  Test Accuracy: 0.979 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     17915\n",
            "           1       0.99      0.98      0.99     20634\n",
            "           2       0.97      0.97      0.97     18450\n",
            "           3       0.97      0.96      0.97     18450\n",
            "           4       0.97      0.97      0.97     17707\n",
            "           5       0.97      0.97      0.97     15996\n",
            "           6       0.97      0.98      0.98     16989\n",
            "           7       0.96      0.97      0.97     18376\n",
            "           8       0.96      0.97      0.97     17335\n",
            "           9       0.96      0.96      0.96     18148\n",
            "\n",
            "    accuracy                           0.97    180000\n",
            "   macro avg       0.97      0.97      0.97    180000\n",
            "weighted avg       0.97      0.97      0.97    180000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.980 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98     18896\n",
            "           1       0.99      0.98      0.99     21773\n",
            "           2       0.97      0.97      0.97     19483\n",
            "           3       0.98      0.96      0.97     19466\n",
            "           4       0.97      0.97      0.97     18694\n",
            "           5       0.97      0.97      0.97     16885\n",
            "           6       0.97      0.98      0.98     17937\n",
            "           7       0.96      0.97      0.97     19404\n",
            "           8       0.96      0.97      0.97     18304\n",
            "           9       0.96      0.96      0.96     19158\n",
            "\n",
            "    accuracy                           0.97    190000\n",
            "   macro avg       0.97      0.97      0.97    190000\n",
            "weighted avg       0.97      0.97      0.97    190000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.982 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     19885\n",
            "           1       0.99      0.98      0.99     22912\n",
            "           2       0.97      0.97      0.97     20524\n",
            "           3       0.98      0.96      0.97     20490\n",
            "           4       0.97      0.97      0.97     19667\n",
            "           5       0.97      0.97      0.97     17768\n",
            "           6       0.97      0.98      0.98     18895\n",
            "           7       0.96      0.97      0.97     20419\n",
            "           8       0.96      0.97      0.97     19273\n",
            "           9       0.96      0.96      0.96     20167\n",
            "\n",
            "    accuracy                           0.97    200000\n",
            "   macro avg       0.97      0.97      0.97    200000\n",
            "weighted avg       0.97      0.97      0.97    200000\n",
            "\n",
            "Epoch: 7/30..  Test Accuracy: 0.979 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     20876\n",
            "           1       0.99      0.98      0.99     24052\n",
            "           2       0.97      0.97      0.97     21550\n",
            "           3       0.98      0.96      0.97     21513\n",
            "           4       0.97      0.97      0.97     20657\n",
            "           5       0.97      0.97      0.97     18667\n",
            "           6       0.97      0.98      0.98     19837\n",
            "           7       0.97      0.97      0.97     21440\n",
            "           8       0.96      0.97      0.97     20231\n",
            "           9       0.96      0.96      0.96     21177\n",
            "\n",
            "    accuracy                           0.97    210000\n",
            "   macro avg       0.97      0.97      0.97    210000\n",
            "weighted avg       0.97      0.97      0.97    210000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.981 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     21863\n",
            "           1       0.99      0.98      0.99     25194\n",
            "           2       0.97      0.97      0.97     22579\n",
            "           3       0.98      0.96      0.97     22523\n",
            "           4       0.97      0.97      0.97     21650\n",
            "           5       0.97      0.97      0.97     19557\n",
            "           6       0.97      0.98      0.98     20783\n",
            "           7       0.97      0.97      0.97     22465\n",
            "           8       0.96      0.97      0.97     21202\n",
            "           9       0.96      0.96      0.96     22184\n",
            "\n",
            "    accuracy                           0.97    220000\n",
            "   macro avg       0.97      0.97      0.97    220000\n",
            "weighted avg       0.97      0.97      0.97    220000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.981 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     22850\n",
            "           1       0.99      0.98      0.99     26338\n",
            "           2       0.97      0.97      0.97     23615\n",
            "           3       0.98      0.96      0.97     23541\n",
            "           4       0.97      0.97      0.97     22620\n",
            "           5       0.97      0.97      0.97     20438\n",
            "           6       0.97      0.98      0.98     21752\n",
            "           7       0.97      0.97      0.97     23475\n",
            "           8       0.96      0.97      0.97     22177\n",
            "           9       0.96      0.97      0.96     23194\n",
            "\n",
            "    accuracy                           0.97    230000\n",
            "   macro avg       0.97      0.97      0.97    230000\n",
            "weighted avg       0.97      0.97      0.97    230000\n",
            "\n",
            "Epoch: 8/30..  Test Accuracy: 0.980 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     23848\n",
            "           1       0.99      0.98      0.99     27481\n",
            "           2       0.97      0.98      0.97     24634\n",
            "           3       0.98      0.96      0.97     24550\n",
            "           4       0.97      0.97      0.97     23598\n",
            "           5       0.97      0.97      0.97     21338\n",
            "           6       0.97      0.98      0.98     22698\n",
            "           7       0.97      0.97      0.97     24501\n",
            "           8       0.96      0.97      0.97     23148\n",
            "           9       0.97      0.97      0.97     24204\n",
            "\n",
            "    accuracy                           0.97    240000\n",
            "   macro avg       0.97      0.97      0.97    240000\n",
            "weighted avg       0.97      0.97      0.97    240000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.981 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     24834\n",
            "           1       0.99      0.98      0.99     28620\n",
            "           2       0.97      0.98      0.97     25667\n",
            "           3       0.98      0.97      0.97     25560\n",
            "           4       0.97      0.97      0.97     24584\n",
            "           5       0.97      0.97      0.97     22223\n",
            "           6       0.97      0.98      0.98     23652\n",
            "           7       0.97      0.97      0.97     25529\n",
            "           8       0.96      0.97      0.97     24113\n",
            "           9       0.97      0.97      0.97     25218\n",
            "\n",
            "    accuracy                           0.97    250000\n",
            "   macro avg       0.97      0.97      0.97    250000\n",
            "weighted avg       0.97      0.97      0.97    250000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.982 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     25828\n",
            "           1       0.99      0.98      0.99     29761\n",
            "           2       0.97      0.98      0.97     26704\n",
            "           3       0.98      0.97      0.97     26575\n",
            "           4       0.97      0.97      0.97     25573\n",
            "           5       0.97      0.97      0.97     23111\n",
            "           6       0.97      0.98      0.98     24614\n",
            "           7       0.97      0.97      0.97     26543\n",
            "           8       0.96      0.97      0.97     25074\n",
            "           9       0.97      0.97      0.97     26217\n",
            "\n",
            "    accuracy                           0.97    260000\n",
            "   macro avg       0.97      0.97      0.97    260000\n",
            "weighted avg       0.97      0.97      0.97    260000\n",
            "\n",
            "Epoch: 9/30..  Test Accuracy: 0.981 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     26821\n",
            "           1       0.99      0.98      0.99     30898\n",
            "           2       0.97      0.98      0.97     27725\n",
            "           3       0.98      0.97      0.97     27589\n",
            "           4       0.97      0.97      0.97     26561\n",
            "           5       0.97      0.97      0.97     24006\n",
            "           6       0.97      0.98      0.98     25563\n",
            "           7       0.97      0.97      0.97     27578\n",
            "           8       0.96      0.97      0.97     26038\n",
            "           9       0.97      0.97      0.97     27221\n",
            "\n",
            "    accuracy                           0.97    270000\n",
            "   macro avg       0.97      0.97      0.97    270000\n",
            "weighted avg       0.97      0.97      0.97    270000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.982 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     27807\n",
            "           1       0.99      0.98      0.99     32041\n",
            "           2       0.97      0.98      0.97     28743\n",
            "           3       0.98      0.97      0.97     28610\n",
            "           4       0.97      0.97      0.97     27547\n",
            "           5       0.97      0.97      0.97     24885\n",
            "           6       0.97      0.98      0.98     26518\n",
            "           7       0.97      0.97      0.97     28613\n",
            "           8       0.97      0.97      0.97     27004\n",
            "           9       0.97      0.97      0.97     28232\n",
            "\n",
            "    accuracy                           0.97    280000\n",
            "   macro avg       0.97      0.97      0.97    280000\n",
            "weighted avg       0.98      0.97      0.98    280000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.982 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     28798\n",
            "           1       0.99      0.98      0.99     33182\n",
            "           2       0.97      0.98      0.97     29781\n",
            "           3       0.98      0.97      0.97     29622\n",
            "           4       0.97      0.97      0.97     28513\n",
            "           5       0.97      0.97      0.97     25782\n",
            "           6       0.97      0.98      0.98     27474\n",
            "           7       0.97      0.97      0.97     29638\n",
            "           8       0.97      0.98      0.97     27966\n",
            "           9       0.97      0.97      0.97     29244\n",
            "\n",
            "    accuracy                           0.98    290000\n",
            "   macro avg       0.98      0.98      0.98    290000\n",
            "weighted avg       0.98      0.98      0.98    290000\n",
            "\n",
            "Epoch: 10/30..  Test Accuracy: 0.981 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     29793\n",
            "           1       0.99      0.98      0.99     34322\n",
            "           2       0.97      0.98      0.97     30811\n",
            "           3       0.98      0.97      0.97     30624\n",
            "           4       0.97      0.97      0.97     29496\n",
            "           5       0.97      0.97      0.97     26690\n",
            "           6       0.97      0.98      0.98     28413\n",
            "           7       0.97      0.97      0.97     30669\n",
            "           8       0.97      0.98      0.97     28919\n",
            "           9       0.97      0.97      0.97     30263\n",
            "\n",
            "    accuracy                           0.98    300000\n",
            "   macro avg       0.98      0.98      0.98    300000\n",
            "weighted avg       0.98      0.98      0.98    300000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.982 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     30780\n",
            "           1       0.99      0.98      0.99     35466\n",
            "           2       0.97      0.98      0.97     31848\n",
            "           3       0.98      0.97      0.97     31646\n",
            "           4       0.98      0.97      0.97     30487\n",
            "           5       0.97      0.97      0.97     27577\n",
            "           6       0.97      0.98      0.98     29355\n",
            "           7       0.97      0.97      0.97     31686\n",
            "           8       0.97      0.98      0.97     29882\n",
            "           9       0.97      0.97      0.97     31273\n",
            "\n",
            "    accuracy                           0.98    310000\n",
            "   macro avg       0.98      0.98      0.98    310000\n",
            "weighted avg       0.98      0.98      0.98    310000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.982 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     31773\n",
            "           1       0.99      0.98      0.99     36608\n",
            "           2       0.97      0.98      0.97     32893\n",
            "           3       0.98      0.97      0.97     32665\n",
            "           4       0.98      0.97      0.97     31463\n",
            "           5       0.97      0.97      0.97     28460\n",
            "           6       0.97      0.98      0.98     30310\n",
            "           7       0.97      0.97      0.97     32701\n",
            "           8       0.97      0.98      0.97     30848\n",
            "           9       0.97      0.97      0.97     32279\n",
            "\n",
            "    accuracy                           0.98    320000\n",
            "   macro avg       0.98      0.98      0.98    320000\n",
            "weighted avg       0.98      0.98      0.98    320000\n",
            "\n",
            "Epoch: 11/30..  Test Accuracy: 0.981 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     32771\n",
            "           1       0.99      0.98      0.99     37747\n",
            "           2       0.97      0.98      0.97     33917\n",
            "           3       0.98      0.97      0.97     33677\n",
            "           4       0.98      0.97      0.97     32443\n",
            "           5       0.97      0.97      0.97     29359\n",
            "           6       0.97      0.98      0.98     31254\n",
            "           7       0.97      0.97      0.97     33721\n",
            "           8       0.97      0.98      0.97     31818\n",
            "           9       0.97      0.97      0.97     33293\n",
            "\n",
            "    accuracy                           0.98    330000\n",
            "   macro avg       0.98      0.98      0.98    330000\n",
            "weighted avg       0.98      0.98      0.98    330000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.982 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     33757\n",
            "           1       0.99      0.98      0.99     38889\n",
            "           2       0.97      0.98      0.97     34943\n",
            "           3       0.98      0.97      0.97     34693\n",
            "           4       0.98      0.97      0.98     33430\n",
            "           5       0.97      0.97      0.97     30246\n",
            "           6       0.97      0.98      0.98     32209\n",
            "           7       0.97      0.97      0.97     34743\n",
            "           8       0.97      0.98      0.97     32790\n",
            "           9       0.97      0.97      0.97     34300\n",
            "\n",
            "    accuracy                           0.98    340000\n",
            "   macro avg       0.98      0.98      0.98    340000\n",
            "weighted avg       0.98      0.98      0.98    340000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.983 Loss: 0.058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     34743\n",
            "           1       0.99      0.98      0.99     40027\n",
            "           2       0.97      0.98      0.97     35987\n",
            "           3       0.98      0.97      0.97     35714\n",
            "           4       0.98      0.97      0.98     34411\n",
            "           5       0.97      0.98      0.97     31140\n",
            "           6       0.97      0.98      0.98     33164\n",
            "           7       0.97      0.98      0.97     35756\n",
            "           8       0.97      0.98      0.97     33745\n",
            "           9       0.97      0.97      0.97     35313\n",
            "\n",
            "    accuracy                           0.98    350000\n",
            "   macro avg       0.98      0.98      0.98    350000\n",
            "weighted avg       0.98      0.98      0.98    350000\n",
            "\n",
            "Epoch: 12/30..  Test Accuracy: 0.982 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     35736\n",
            "           1       0.99      0.98      0.99     41167\n",
            "           2       0.97      0.98      0.98     37016\n",
            "           3       0.98      0.97      0.97     36728\n",
            "           4       0.98      0.97      0.98     35392\n",
            "           5       0.97      0.98      0.97     32036\n",
            "           6       0.97      0.98      0.98     34117\n",
            "           7       0.97      0.98      0.97     36787\n",
            "           8       0.97      0.98      0.97     34705\n",
            "           9       0.97      0.97      0.97     36316\n",
            "\n",
            "    accuracy                           0.98    360000\n",
            "   macro avg       0.98      0.98      0.98    360000\n",
            "weighted avg       0.98      0.98      0.98    360000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.982 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     36720\n",
            "           1       0.99      0.98      0.99     42313\n",
            "           2       0.97      0.98      0.98     38056\n",
            "           3       0.98      0.97      0.97     37732\n",
            "           4       0.98      0.97      0.98     36384\n",
            "           5       0.97      0.98      0.97     32926\n",
            "           6       0.97      0.98      0.98     35065\n",
            "           7       0.97      0.98      0.97     37818\n",
            "           8       0.97      0.98      0.97     35670\n",
            "           9       0.97      0.97      0.97     37316\n",
            "\n",
            "    accuracy                           0.98    370000\n",
            "   macro avg       0.98      0.98      0.98    370000\n",
            "weighted avg       0.98      0.98      0.98    370000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.982 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     37713\n",
            "           1       0.99      0.98      0.99     43458\n",
            "           2       0.97      0.98      0.98     39083\n",
            "           3       0.98      0.97      0.97     38749\n",
            "           4       0.98      0.98      0.98     37367\n",
            "           5       0.97      0.98      0.97     33813\n",
            "           6       0.97      0.98      0.98     36019\n",
            "           7       0.97      0.98      0.97     38840\n",
            "           8       0.97      0.98      0.97     36633\n",
            "           9       0.97      0.97      0.97     38325\n",
            "\n",
            "    accuracy                           0.98    380000\n",
            "   macro avg       0.98      0.98      0.98    380000\n",
            "weighted avg       0.98      0.98      0.98    380000\n",
            "\n",
            "Epoch: 13/30..  Test Accuracy: 0.982 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     38708\n",
            "           1       0.99      0.98      0.99     44595\n",
            "           2       0.97      0.98      0.98     40107\n",
            "           3       0.98      0.97      0.97     39755\n",
            "           4       0.98      0.98      0.98     38346\n",
            "           5       0.97      0.98      0.97     34716\n",
            "           6       0.97      0.98      0.98     36966\n",
            "           7       0.97      0.98      0.97     39878\n",
            "           8       0.97      0.98      0.97     37593\n",
            "           9       0.97      0.97      0.97     39336\n",
            "\n",
            "    accuracy                           0.98    390000\n",
            "   macro avg       0.98      0.98      0.98    390000\n",
            "weighted avg       0.98      0.98      0.98    390000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.984 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     39697\n",
            "           1       0.99      0.99      0.99     45728\n",
            "           2       0.97      0.98      0.98     41140\n",
            "           3       0.98      0.97      0.97     40772\n",
            "           4       0.98      0.98      0.98     39323\n",
            "           5       0.97      0.98      0.97     35605\n",
            "           6       0.97      0.98      0.98     37924\n",
            "           7       0.97      0.98      0.97     40907\n",
            "           8       0.97      0.98      0.97     38554\n",
            "           9       0.97      0.97      0.97     40350\n",
            "\n",
            "    accuracy                           0.98    400000\n",
            "   macro avg       0.98      0.98      0.98    400000\n",
            "weighted avg       0.98      0.98      0.98    400000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.983 Loss: 0.057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     40685\n",
            "           1       0.99      0.99      0.99     46870\n",
            "           2       0.97      0.98      0.98     42176\n",
            "           3       0.98      0.97      0.97     41796\n",
            "           4       0.98      0.98      0.98     40312\n",
            "           5       0.97      0.98      0.98     36488\n",
            "           6       0.97      0.98      0.98     38874\n",
            "           7       0.97      0.98      0.97     41929\n",
            "           8       0.97      0.98      0.97     39518\n",
            "           9       0.97      0.97      0.97     41352\n",
            "\n",
            "    accuracy                           0.98    410000\n",
            "   macro avg       0.98      0.98      0.98    410000\n",
            "weighted avg       0.98      0.98      0.98    410000\n",
            "\n",
            "Epoch: 14/30..  Test Accuracy: 0.981 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     41683\n",
            "           1       0.99      0.99      0.99     48007\n",
            "           2       0.97      0.98      0.98     43201\n",
            "           3       0.98      0.97      0.98     42821\n",
            "           4       0.98      0.98      0.98     41292\n",
            "           5       0.97      0.98      0.98     37387\n",
            "           6       0.97      0.98      0.98     39813\n",
            "           7       0.97      0.98      0.97     42956\n",
            "           8       0.97      0.98      0.97     40482\n",
            "           9       0.97      0.97      0.97     42358\n",
            "\n",
            "    accuracy                           0.98    420000\n",
            "   macro avg       0.98      0.98      0.98    420000\n",
            "weighted avg       0.98      0.98      0.98    420000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.983 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     42670\n",
            "           1       0.99      0.99      0.99     49149\n",
            "           2       0.97      0.98      0.98     44232\n",
            "           3       0.98      0.97      0.98     43829\n",
            "           4       0.98      0.98      0.98     42274\n",
            "           5       0.97      0.98      0.98     38271\n",
            "           6       0.97      0.98      0.98     40768\n",
            "           7       0.97      0.98      0.97     43991\n",
            "           8       0.97      0.98      0.97     41448\n",
            "           9       0.97      0.97      0.97     43368\n",
            "\n",
            "    accuracy                           0.98    430000\n",
            "   macro avg       0.98      0.98      0.98    430000\n",
            "weighted avg       0.98      0.98      0.98    430000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.984 Loss: 0.057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     43657\n",
            "           1       0.99      0.99      0.99     50288\n",
            "           2       0.97      0.98      0.98     45269\n",
            "           3       0.98      0.97      0.98     44845\n",
            "           4       0.98      0.98      0.98     43258\n",
            "           5       0.97      0.98      0.98     39154\n",
            "           6       0.97      0.98      0.98     41729\n",
            "           7       0.97      0.98      0.97     45015\n",
            "           8       0.97      0.98      0.97     42414\n",
            "           9       0.97      0.97      0.97     44371\n",
            "\n",
            "    accuracy                           0.98    440000\n",
            "   macro avg       0.98      0.98      0.98    440000\n",
            "weighted avg       0.98      0.98      0.98    440000\n",
            "\n",
            "Epoch: 15/30..  Test Accuracy: 0.982 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     44652\n",
            "           1       0.99      0.99      0.99     51422\n",
            "           2       0.97      0.98      0.98     46295\n",
            "           3       0.98      0.97      0.98     45856\n",
            "           4       0.98      0.98      0.98     44230\n",
            "           5       0.97      0.98      0.98     40048\n",
            "           6       0.98      0.98      0.98     42684\n",
            "           7       0.97      0.98      0.97     46060\n",
            "           8       0.97      0.98      0.97     43373\n",
            "           9       0.97      0.97      0.97     45380\n",
            "\n",
            "    accuracy                           0.98    450000\n",
            "   macro avg       0.98      0.98      0.98    450000\n",
            "weighted avg       0.98      0.98      0.98    450000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.983 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     45648\n",
            "           1       0.99      0.99      0.99     52561\n",
            "           2       0.98      0.98      0.98     47332\n",
            "           3       0.98      0.97      0.98     46863\n",
            "           4       0.98      0.98      0.98     45214\n",
            "           5       0.97      0.98      0.98     40937\n",
            "           6       0.98      0.98      0.98     43644\n",
            "           7       0.97      0.98      0.97     47084\n",
            "           8       0.97      0.98      0.97     44333\n",
            "           9       0.97      0.97      0.97     46384\n",
            "\n",
            "    accuracy                           0.98    460000\n",
            "   macro avg       0.98      0.98      0.98    460000\n",
            "weighted avg       0.98      0.98      0.98    460000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.983 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     46637\n",
            "           1       0.99      0.99      0.99     53698\n",
            "           2       0.98      0.98      0.98     48366\n",
            "           3       0.98      0.97      0.98     47880\n",
            "           4       0.98      0.98      0.98     46186\n",
            "           5       0.97      0.98      0.98     41821\n",
            "           6       0.98      0.98      0.98     44605\n",
            "           7       0.97      0.98      0.97     48103\n",
            "           8       0.97      0.98      0.97     45293\n",
            "           9       0.97      0.97      0.97     47411\n",
            "\n",
            "    accuracy                           0.98    470000\n",
            "   macro avg       0.98      0.98      0.98    470000\n",
            "weighted avg       0.98      0.98      0.98    470000\n",
            "\n",
            "Epoch: 16/30..  Test Accuracy: 0.980 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     47628\n",
            "           1       0.99      0.99      0.99     54836\n",
            "           2       0.98      0.98      0.98     49382\n",
            "           3       0.98      0.97      0.98     48901\n",
            "           4       0.98      0.98      0.98     47179\n",
            "           5       0.97      0.98      0.98     42719\n",
            "           6       0.98      0.98      0.98     45550\n",
            "           7       0.97      0.98      0.97     49127\n",
            "           8       0.97      0.98      0.97     46252\n",
            "           9       0.97      0.97      0.97     48426\n",
            "\n",
            "    accuracy                           0.98    480000\n",
            "   macro avg       0.98      0.98      0.98    480000\n",
            "weighted avg       0.98      0.98      0.98    480000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.983 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     48621\n",
            "           1       0.99      0.99      0.99     55973\n",
            "           2       0.98      0.98      0.98     50420\n",
            "           3       0.98      0.97      0.98     49911\n",
            "           4       0.98      0.98      0.98     48156\n",
            "           5       0.97      0.98      0.98     43603\n",
            "           6       0.98      0.98      0.98     46505\n",
            "           7       0.97      0.98      0.97     50147\n",
            "           8       0.97      0.98      0.97     47223\n",
            "           9       0.97      0.97      0.97     49441\n",
            "\n",
            "    accuracy                           0.98    490000\n",
            "   macro avg       0.98      0.98      0.98    490000\n",
            "weighted avg       0.98      0.98      0.98    490000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.983 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     49612\n",
            "           1       0.99      0.99      0.99     57113\n",
            "           2       0.98      0.98      0.98     51454\n",
            "           3       0.98      0.97      0.98     50930\n",
            "           4       0.98      0.98      0.98     49139\n",
            "           5       0.98      0.98      0.98     44493\n",
            "           6       0.98      0.98      0.98     47460\n",
            "           7       0.97      0.98      0.97     51171\n",
            "           8       0.97      0.98      0.97     48187\n",
            "           9       0.97      0.97      0.97     50441\n",
            "\n",
            "    accuracy                           0.98    500000\n",
            "   macro avg       0.98      0.98      0.98    500000\n",
            "weighted avg       0.98      0.98      0.98    500000\n",
            "\n",
            "Epoch: 17/30..  Test Accuracy: 0.981 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     50604\n",
            "           1       0.99      0.99      0.99     58252\n",
            "           2       0.98      0.98      0.98     52485\n",
            "           3       0.98      0.97      0.98     51950\n",
            "           4       0.98      0.98      0.98     50114\n",
            "           5       0.98      0.98      0.98     45382\n",
            "           6       0.98      0.98      0.98     48414\n",
            "           7       0.97      0.98      0.97     52201\n",
            "           8       0.97      0.98      0.97     49145\n",
            "           9       0.97      0.97      0.97     51453\n",
            "\n",
            "    accuracy                           0.98    510000\n",
            "   macro avg       0.98      0.98      0.98    510000\n",
            "weighted avg       0.98      0.98      0.98    510000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.982 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     51597\n",
            "           1       0.99      0.99      0.99     59391\n",
            "           2       0.98      0.98      0.98     53512\n",
            "           3       0.98      0.97      0.98     52958\n",
            "           4       0.98      0.98      0.98     51087\n",
            "           5       0.98      0.98      0.98     46272\n",
            "           6       0.98      0.98      0.98     49373\n",
            "           7       0.97      0.98      0.97     53237\n",
            "           8       0.97      0.98      0.97     50110\n",
            "           9       0.97      0.97      0.97     52463\n",
            "\n",
            "    accuracy                           0.98    520000\n",
            "   macro avg       0.98      0.98      0.98    520000\n",
            "weighted avg       0.98      0.98      0.98    520000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.982 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98     52590\n",
            "           1       0.99      0.99      0.99     60530\n",
            "           2       0.98      0.98      0.98     54554\n",
            "           3       0.98      0.97      0.98     53980\n",
            "           4       0.98      0.98      0.98     52071\n",
            "           5       0.98      0.98      0.98     47157\n",
            "           6       0.98      0.98      0.98     50335\n",
            "           7       0.97      0.98      0.97     54257\n",
            "           8       0.97      0.98      0.97     51067\n",
            "           9       0.97      0.97      0.97     53459\n",
            "\n",
            "    accuracy                           0.98    530000\n",
            "   macro avg       0.98      0.98      0.98    530000\n",
            "weighted avg       0.98      0.98      0.98    530000\n",
            "\n",
            "Epoch: 18/30..  Test Accuracy: 0.983 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     53577\n",
            "           1       0.99      0.99      0.99     61674\n",
            "           2       0.98      0.98      0.98     55587\n",
            "           3       0.98      0.97      0.98     54990\n",
            "           4       0.98      0.98      0.98     53054\n",
            "           5       0.98      0.98      0.98     48045\n",
            "           6       0.98      0.99      0.98     51282\n",
            "           7       0.97      0.98      0.97     55286\n",
            "           8       0.97      0.98      0.97     52035\n",
            "           9       0.97      0.97      0.97     54470\n",
            "\n",
            "    accuracy                           0.98    540000\n",
            "   macro avg       0.98      0.98      0.98    540000\n",
            "weighted avg       0.98      0.98      0.98    540000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.984 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     54565\n",
            "           1       0.99      0.99      0.99     62813\n",
            "           2       0.98      0.98      0.98     56616\n",
            "           3       0.98      0.97      0.98     56003\n",
            "           4       0.98      0.98      0.98     54031\n",
            "           5       0.98      0.98      0.98     48934\n",
            "           6       0.98      0.99      0.98     52237\n",
            "           7       0.97      0.98      0.97     56312\n",
            "           8       0.97      0.98      0.97     53009\n",
            "           9       0.97      0.97      0.97     55480\n",
            "\n",
            "    accuracy                           0.98    550000\n",
            "   macro avg       0.98      0.98      0.98    550000\n",
            "weighted avg       0.98      0.98      0.98    550000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.984 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     55556\n",
            "           1       0.99      0.99      0.99     63949\n",
            "           2       0.98      0.98      0.98     57659\n",
            "           3       0.98      0.97      0.98     57015\n",
            "           4       0.98      0.98      0.98     55014\n",
            "           5       0.98      0.98      0.98     49825\n",
            "           6       0.98      0.99      0.98     53192\n",
            "           7       0.97      0.98      0.97     57326\n",
            "           8       0.97      0.98      0.97     53980\n",
            "           9       0.97      0.97      0.97     56484\n",
            "\n",
            "    accuracy                           0.98    560000\n",
            "   macro avg       0.98      0.98      0.98    560000\n",
            "weighted avg       0.98      0.98      0.98    560000\n",
            "\n",
            "Epoch: 19/30..  Test Accuracy: 0.983 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     56551\n",
            "           1       0.99      0.99      0.99     65093\n",
            "           2       0.98      0.98      0.98     58681\n",
            "           3       0.98      0.97      0.98     58024\n",
            "           4       0.98      0.98      0.98     55994\n",
            "           5       0.98      0.98      0.98     50720\n",
            "           6       0.98      0.99      0.98     54146\n",
            "           7       0.97      0.98      0.98     58355\n",
            "           8       0.97      0.98      0.97     54946\n",
            "           9       0.97      0.97      0.97     57490\n",
            "\n",
            "    accuracy                           0.98    570000\n",
            "   macro avg       0.98      0.98      0.98    570000\n",
            "weighted avg       0.98      0.98      0.98    570000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.982 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     57543\n",
            "           1       0.99      0.99      0.99     66235\n",
            "           2       0.98      0.98      0.98     59716\n",
            "           3       0.98      0.97      0.98     59036\n",
            "           4       0.98      0.98      0.98     56981\n",
            "           5       0.98      0.98      0.98     51606\n",
            "           6       0.98      0.99      0.98     55095\n",
            "           7       0.97      0.98      0.98     59388\n",
            "           8       0.97      0.98      0.97     55914\n",
            "           9       0.97      0.97      0.97     58486\n",
            "\n",
            "    accuracy                           0.98    580000\n",
            "   macro avg       0.98      0.98      0.98    580000\n",
            "weighted avg       0.98      0.98      0.98    580000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.985 Loss: 0.055\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     58530\n",
            "           1       0.99      0.99      0.99     67371\n",
            "           2       0.98      0.98      0.98     60755\n",
            "           3       0.98      0.97      0.98     60055\n",
            "           4       0.98      0.98      0.98     57961\n",
            "           5       0.98      0.98      0.98     52493\n",
            "           6       0.98      0.99      0.98     56049\n",
            "           7       0.97      0.98      0.98     60403\n",
            "           8       0.97      0.98      0.97     56882\n",
            "           9       0.97      0.97      0.97     59501\n",
            "\n",
            "    accuracy                           0.98    590000\n",
            "   macro avg       0.98      0.98      0.98    590000\n",
            "weighted avg       0.98      0.98      0.98    590000\n",
            "\n",
            "Epoch: 20/30..  Test Accuracy: 0.983 Loss: 0.056\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     59517\n",
            "           1       0.99      0.99      0.99     68508\n",
            "           2       0.98      0.98      0.98     61785\n",
            "           3       0.98      0.97      0.98     61069\n",
            "           4       0.98      0.98      0.98     58947\n",
            "           5       0.98      0.98      0.98     53389\n",
            "           6       0.98      0.99      0.98     56998\n",
            "           7       0.97      0.98      0.98     61432\n",
            "           8       0.97      0.98      0.97     57842\n",
            "           9       0.97      0.97      0.97     60513\n",
            "\n",
            "    accuracy                           0.98    600000\n",
            "   macro avg       0.98      0.98      0.98    600000\n",
            "weighted avg       0.98      0.98      0.98    600000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.983 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     60506\n",
            "           1       0.99      0.99      0.99     69650\n",
            "           2       0.98      0.98      0.98     62823\n",
            "           3       0.98      0.97      0.98     62089\n",
            "           4       0.98      0.98      0.98     59927\n",
            "           5       0.98      0.98      0.98     54280\n",
            "           6       0.98      0.99      0.98     57945\n",
            "           7       0.97      0.98      0.98     62460\n",
            "           8       0.97      0.98      0.97     58804\n",
            "           9       0.97      0.97      0.97     61516\n",
            "\n",
            "    accuracy                           0.98    610000\n",
            "   macro avg       0.98      0.98      0.98    610000\n",
            "weighted avg       0.98      0.98      0.98    610000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.984 Loss: 0.058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     61490\n",
            "           1       0.99      0.99      0.99     70789\n",
            "           2       0.98      0.98      0.98     63862\n",
            "           3       0.98      0.97      0.98     63106\n",
            "           4       0.98      0.98      0.98     60911\n",
            "           5       0.98      0.98      0.98     55171\n",
            "           6       0.98      0.99      0.98     58896\n",
            "           7       0.97      0.98      0.98     63474\n",
            "           8       0.97      0.98      0.97     59766\n",
            "           9       0.97      0.97      0.97     62535\n",
            "\n",
            "    accuracy                           0.98    620000\n",
            "   macro avg       0.98      0.98      0.98    620000\n",
            "weighted avg       0.98      0.98      0.98    620000\n",
            "\n",
            "Epoch: 21/30..  Test Accuracy: 0.983 Loss: 0.058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     62481\n",
            "           1       0.99      0.99      0.99     71927\n",
            "           2       0.98      0.98      0.98     64897\n",
            "           3       0.98      0.97      0.98     64115\n",
            "           4       0.98      0.98      0.98     61884\n",
            "           5       0.98      0.98      0.98     56062\n",
            "           6       0.98      0.99      0.98     59847\n",
            "           7       0.97      0.98      0.98     64507\n",
            "           8       0.97      0.98      0.98     60733\n",
            "           9       0.97      0.97      0.97     63547\n",
            "\n",
            "    accuracy                           0.98    630000\n",
            "   macro avg       0.98      0.98      0.98    630000\n",
            "weighted avg       0.98      0.98      0.98    630000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.982 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     63473\n",
            "           1       0.99      0.99      0.99     73071\n",
            "           2       0.98      0.98      0.98     65938\n",
            "           3       0.98      0.97      0.98     65127\n",
            "           4       0.98      0.98      0.98     62864\n",
            "           5       0.98      0.98      0.98     56947\n",
            "           6       0.98      0.99      0.98     60797\n",
            "           7       0.97      0.98      0.98     65536\n",
            "           8       0.97      0.98      0.98     61697\n",
            "           9       0.97      0.97      0.97     64550\n",
            "\n",
            "    accuracy                           0.98    640000\n",
            "   macro avg       0.98      0.98      0.98    640000\n",
            "weighted avg       0.98      0.98      0.98    640000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.983 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     64467\n",
            "           1       0.99      0.99      0.99     74210\n",
            "           2       0.98      0.98      0.98     66968\n",
            "           3       0.98      0.97      0.98     66149\n",
            "           4       0.98      0.98      0.98     63854\n",
            "           5       0.98      0.98      0.98     57835\n",
            "           6       0.98      0.99      0.98     61746\n",
            "           7       0.97      0.98      0.98     66553\n",
            "           8       0.97      0.98      0.98     62661\n",
            "           9       0.97      0.97      0.97     65557\n",
            "\n",
            "    accuracy                           0.98    650000\n",
            "   macro avg       0.98      0.98      0.98    650000\n",
            "weighted avg       0.98      0.98      0.98    650000\n",
            "\n",
            "Epoch: 22/30..  Test Accuracy: 0.983 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     65462\n",
            "           1       0.99      0.99      0.99     75349\n",
            "           2       0.98      0.98      0.98     67991\n",
            "           3       0.98      0.97      0.98     67159\n",
            "           4       0.98      0.98      0.98     64835\n",
            "           5       0.98      0.98      0.98     58724\n",
            "           6       0.98      0.99      0.98     62700\n",
            "           7       0.97      0.98      0.98     67584\n",
            "           8       0.97      0.98      0.98     63630\n",
            "           9       0.97      0.97      0.97     66566\n",
            "\n",
            "    accuracy                           0.98    660000\n",
            "   macro avg       0.98      0.98      0.98    660000\n",
            "weighted avg       0.98      0.98      0.98    660000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.983 Loss: 0.068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     66446\n",
            "           1       0.99      0.99      0.99     76488\n",
            "           2       0.98      0.98      0.98     69032\n",
            "           3       0.98      0.97      0.98     68167\n",
            "           4       0.98      0.98      0.98     65820\n",
            "           5       0.98      0.98      0.98     59620\n",
            "           6       0.98      0.99      0.98     63648\n",
            "           7       0.97      0.98      0.98     68617\n",
            "           8       0.97      0.98      0.98     64590\n",
            "           9       0.97      0.97      0.97     67572\n",
            "\n",
            "    accuracy                           0.98    670000\n",
            "   macro avg       0.98      0.98      0.98    670000\n",
            "weighted avg       0.98      0.98      0.98    670000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.984 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     67439\n",
            "           1       0.99      0.99      0.99     77626\n",
            "           2       0.98      0.98      0.98     70069\n",
            "           3       0.98      0.97      0.98     69186\n",
            "           4       0.98      0.98      0.98     66793\n",
            "           5       0.98      0.98      0.98     60506\n",
            "           6       0.98      0.99      0.98     64605\n",
            "           7       0.97      0.98      0.98     69641\n",
            "           8       0.97      0.98      0.98     65561\n",
            "           9       0.97      0.97      0.97     68574\n",
            "\n",
            "    accuracy                           0.98    680000\n",
            "   macro avg       0.98      0.98      0.98    680000\n",
            "weighted avg       0.98      0.98      0.98    680000\n",
            "\n",
            "Epoch: 23/30..  Test Accuracy: 0.983 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     68430\n",
            "           1       0.99      0.99      0.99     78766\n",
            "           2       0.98      0.98      0.98     71094\n",
            "           3       0.98      0.97      0.98     70198\n",
            "           4       0.98      0.98      0.98     67776\n",
            "           5       0.98      0.98      0.98     61408\n",
            "           6       0.98      0.99      0.98     65562\n",
            "           7       0.97      0.98      0.98     70673\n",
            "           8       0.97      0.98      0.98     66520\n",
            "           9       0.97      0.97      0.97     69573\n",
            "\n",
            "    accuracy                           0.98    690000\n",
            "   macro avg       0.98      0.98      0.98    690000\n",
            "weighted avg       0.98      0.98      0.98    690000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.983 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     69418\n",
            "           1       0.99      0.99      0.99     79906\n",
            "           2       0.98      0.98      0.98     72129\n",
            "           3       0.98      0.97      0.98     71208\n",
            "           4       0.98      0.98      0.98     68761\n",
            "           5       0.98      0.98      0.98     62294\n",
            "           6       0.98      0.99      0.98     66519\n",
            "           7       0.97      0.98      0.98     71716\n",
            "           8       0.97      0.98      0.98     67475\n",
            "           9       0.97      0.97      0.97     70574\n",
            "\n",
            "    accuracy                           0.98    700000\n",
            "   macro avg       0.98      0.98      0.98    700000\n",
            "weighted avg       0.98      0.98      0.98    700000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.985 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     70403\n",
            "           1       0.99      0.99      0.99     81043\n",
            "           2       0.98      0.98      0.98     73163\n",
            "           3       0.98      0.97      0.98     72223\n",
            "           4       0.98      0.98      0.98     69743\n",
            "           5       0.98      0.98      0.98     63184\n",
            "           6       0.98      0.99      0.98     67477\n",
            "           7       0.97      0.98      0.98     72745\n",
            "           8       0.97      0.98      0.98     68446\n",
            "           9       0.97      0.97      0.97     71573\n",
            "\n",
            "    accuracy                           0.98    710000\n",
            "   macro avg       0.98      0.98      0.98    710000\n",
            "weighted avg       0.98      0.98      0.98    710000\n",
            "\n",
            "Epoch: 24/30..  Test Accuracy: 0.984 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     71395\n",
            "           1       0.99      0.99      0.99     82176\n",
            "           2       0.98      0.98      0.98     74189\n",
            "           3       0.98      0.97      0.98     73241\n",
            "           4       0.98      0.98      0.98     70730\n",
            "           5       0.98      0.98      0.98     64073\n",
            "           6       0.98      0.99      0.98     68421\n",
            "           7       0.97      0.98      0.98     73783\n",
            "           8       0.97      0.98      0.98     69411\n",
            "           9       0.97      0.97      0.97     72581\n",
            "\n",
            "    accuracy                           0.98    720000\n",
            "   macro avg       0.98      0.98      0.98    720000\n",
            "weighted avg       0.98      0.98      0.98    720000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.984 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     72383\n",
            "           1       0.99      0.99      0.99     83315\n",
            "           2       0.98      0.98      0.98     75230\n",
            "           3       0.98      0.97      0.98     74242\n",
            "           4       0.98      0.98      0.98     71702\n",
            "           5       0.98      0.98      0.98     64967\n",
            "           6       0.98      0.99      0.98     69376\n",
            "           7       0.97      0.98      0.98     74809\n",
            "           8       0.97      0.98      0.98     70380\n",
            "           9       0.97      0.97      0.97     73596\n",
            "\n",
            "    accuracy                           0.98    730000\n",
            "   macro avg       0.98      0.98      0.98    730000\n",
            "weighted avg       0.98      0.98      0.98    730000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.985 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     73373\n",
            "           1       0.99      0.99      0.99     84456\n",
            "           2       0.98      0.98      0.98     76264\n",
            "           3       0.98      0.97      0.98     75258\n",
            "           4       0.98      0.98      0.98     72681\n",
            "           5       0.98      0.98      0.98     65856\n",
            "           6       0.98      0.99      0.98     70340\n",
            "           7       0.97      0.98      0.98     75830\n",
            "           8       0.97      0.98      0.98     71343\n",
            "           9       0.97      0.97      0.97     74599\n",
            "\n",
            "    accuracy                           0.98    740000\n",
            "   macro avg       0.98      0.98      0.98    740000\n",
            "weighted avg       0.98      0.98      0.98    740000\n",
            "\n",
            "Epoch: 25/30..  Test Accuracy: 0.983 Loss: 0.064\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     74367\n",
            "           1       0.99      0.99      0.99     85603\n",
            "           2       0.98      0.98      0.98     77293\n",
            "           3       0.98      0.97      0.98     76279\n",
            "           4       0.98      0.98      0.98     73666\n",
            "           5       0.98      0.98      0.98     66747\n",
            "           6       0.98      0.99      0.98     71290\n",
            "           7       0.97      0.98      0.98     76856\n",
            "           8       0.97      0.98      0.98     72299\n",
            "           9       0.97      0.97      0.97     75600\n",
            "\n",
            "    accuracy                           0.98    750000\n",
            "   macro avg       0.98      0.98      0.98    750000\n",
            "weighted avg       0.98      0.98      0.98    750000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.985 Loss: 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     75355\n",
            "           1       0.99      0.99      0.99     86739\n",
            "           2       0.98      0.98      0.98     78327\n",
            "           3       0.98      0.97      0.98     77293\n",
            "           4       0.98      0.98      0.98     74642\n",
            "           5       0.98      0.98      0.98     67633\n",
            "           6       0.98      0.99      0.98     72247\n",
            "           7       0.97      0.98      0.98     77890\n",
            "           8       0.97      0.98      0.98     73264\n",
            "           9       0.97      0.97      0.97     76610\n",
            "\n",
            "    accuracy                           0.98    760000\n",
            "   macro avg       0.98      0.98      0.98    760000\n",
            "weighted avg       0.98      0.98      0.98    760000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.985 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     76346\n",
            "           1       0.99      0.99      0.99     87877\n",
            "           2       0.98      0.98      0.98     79353\n",
            "           3       0.98      0.97      0.98     78309\n",
            "           4       0.98      0.98      0.98     75617\n",
            "           5       0.98      0.98      0.98     68526\n",
            "           6       0.98      0.99      0.98     73205\n",
            "           7       0.97      0.98      0.98     78925\n",
            "           8       0.97      0.98      0.98     74225\n",
            "           9       0.97      0.97      0.97     77617\n",
            "\n",
            "    accuracy                           0.98    770000\n",
            "   macro avg       0.98      0.98      0.98    770000\n",
            "weighted avg       0.98      0.98      0.98    770000\n",
            "\n",
            "Epoch: 26/30..  Test Accuracy: 0.984 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     77343\n",
            "           1       0.99      0.99      0.99     89016\n",
            "           2       0.98      0.98      0.98     80379\n",
            "           3       0.98      0.98      0.98     79317\n",
            "           4       0.98      0.98      0.98     76603\n",
            "           5       0.98      0.98      0.98     69418\n",
            "           6       0.98      0.99      0.98     74162\n",
            "           7       0.98      0.98      0.98     79953\n",
            "           8       0.97      0.98      0.98     75190\n",
            "           9       0.97      0.97      0.97     78619\n",
            "\n",
            "    accuracy                           0.98    780000\n",
            "   macro avg       0.98      0.98      0.98    780000\n",
            "weighted avg       0.98      0.98      0.98    780000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.984 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     78329\n",
            "           1       0.99      0.99      0.99     90159\n",
            "           2       0.98      0.98      0.98     81409\n",
            "           3       0.98      0.98      0.98     80324\n",
            "           4       0.98      0.98      0.98     77580\n",
            "           5       0.98      0.98      0.98     70310\n",
            "           6       0.98      0.99      0.98     75115\n",
            "           7       0.98      0.98      0.98     80981\n",
            "           8       0.97      0.98      0.98     76163\n",
            "           9       0.97      0.97      0.97     79630\n",
            "\n",
            "    accuracy                           0.98    790000\n",
            "   macro avg       0.98      0.98      0.98    790000\n",
            "weighted avg       0.98      0.98      0.98    790000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.984 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     79314\n",
            "           1       0.99      0.99      0.99     91303\n",
            "           2       0.98      0.98      0.98     82437\n",
            "           3       0.98      0.98      0.98     81344\n",
            "           4       0.98      0.98      0.98     78558\n",
            "           5       0.98      0.98      0.98     71197\n",
            "           6       0.98      0.99      0.98     76071\n",
            "           7       0.98      0.98      0.98     82009\n",
            "           8       0.97      0.98      0.98     77135\n",
            "           9       0.97      0.97      0.97     80632\n",
            "\n",
            "    accuracy                           0.98    800000\n",
            "   macro avg       0.98      0.98      0.98    800000\n",
            "weighted avg       0.98      0.98      0.98    800000\n",
            "\n",
            "Epoch: 27/30..  Test Accuracy: 0.982 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     80314\n",
            "           1       0.99      0.99      0.99     92449\n",
            "           2       0.98      0.98      0.98     83467\n",
            "           3       0.98      0.98      0.98     82355\n",
            "           4       0.98      0.98      0.98     79535\n",
            "           5       0.98      0.98      0.98     72094\n",
            "           6       0.98      0.99      0.98     77020\n",
            "           7       0.98      0.98      0.98     83036\n",
            "           8       0.97      0.98      0.98     78092\n",
            "           9       0.97      0.97      0.97     81638\n",
            "\n",
            "    accuracy                           0.98    810000\n",
            "   macro avg       0.98      0.98      0.98    810000\n",
            "weighted avg       0.98      0.98      0.98    810000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.983 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     81301\n",
            "           1       0.99      0.99      0.99     93590\n",
            "           2       0.98      0.98      0.98     84492\n",
            "           3       0.98      0.98      0.98     83361\n",
            "           4       0.98      0.98      0.98     80509\n",
            "           5       0.98      0.98      0.98     72984\n",
            "           6       0.98      0.99      0.98     77975\n",
            "           7       0.98      0.98      0.98     84075\n",
            "           8       0.97      0.98      0.98     79060\n",
            "           9       0.97      0.97      0.97     82653\n",
            "\n",
            "    accuracy                           0.98    820000\n",
            "   macro avg       0.98      0.98      0.98    820000\n",
            "weighted avg       0.98      0.98      0.98    820000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.985 Loss: 0.060\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     82289\n",
            "           1       0.99      0.99      0.99     94733\n",
            "           2       0.98      0.98      0.98     85523\n",
            "           3       0.98      0.98      0.98     84380\n",
            "           4       0.98      0.98      0.98     81491\n",
            "           5       0.98      0.98      0.98     73862\n",
            "           6       0.98      0.99      0.98     78935\n",
            "           7       0.98      0.98      0.98     85104\n",
            "           8       0.97      0.98      0.98     80024\n",
            "           9       0.97      0.97      0.97     83659\n",
            "\n",
            "    accuracy                           0.98    830000\n",
            "   macro avg       0.98      0.98      0.98    830000\n",
            "weighted avg       0.98      0.98      0.98    830000\n",
            "\n",
            "Epoch: 28/30..  Test Accuracy: 0.982 Loss: 0.065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     83283\n",
            "           1       0.99      0.99      0.99     95874\n",
            "           2       0.98      0.98      0.98     86554\n",
            "           3       0.98      0.98      0.98     85398\n",
            "           4       0.98      0.98      0.98     82460\n",
            "           5       0.98      0.98      0.98     74747\n",
            "           6       0.98      0.99      0.98     79890\n",
            "           7       0.98      0.98      0.98     86132\n",
            "           8       0.97      0.98      0.98     80987\n",
            "           9       0.97      0.97      0.97     84675\n",
            "\n",
            "    accuracy                           0.98    840000\n",
            "   macro avg       0.98      0.98      0.98    840000\n",
            "weighted avg       0.98      0.98      0.98    840000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.983 Loss: 0.067\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     84275\n",
            "           1       0.99      0.99      0.99     97019\n",
            "           2       0.98      0.98      0.98     87585\n",
            "           3       0.98      0.98      0.98     86400\n",
            "           4       0.98      0.98      0.98     83427\n",
            "           5       0.98      0.98      0.98     75642\n",
            "           6       0.98      0.99      0.98     80849\n",
            "           7       0.98      0.98      0.98     87163\n",
            "           8       0.97      0.98      0.98     81950\n",
            "           9       0.97      0.97      0.97     85690\n",
            "\n",
            "    accuracy                           0.98    850000\n",
            "   macro avg       0.98      0.98      0.98    850000\n",
            "weighted avg       0.98      0.98      0.98    850000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.984 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     85265\n",
            "           1       0.99      0.99      0.99     98157\n",
            "           2       0.98      0.98      0.98     88613\n",
            "           3       0.98      0.98      0.98     87417\n",
            "           4       0.98      0.98      0.98     84415\n",
            "           5       0.98      0.98      0.98     76533\n",
            "           6       0.98      0.99      0.98     81804\n",
            "           7       0.98      0.98      0.98     88188\n",
            "           8       0.97      0.98      0.98     82915\n",
            "           9       0.97      0.97      0.97     86693\n",
            "\n",
            "    accuracy                           0.98    860000\n",
            "   macro avg       0.98      0.98      0.98    860000\n",
            "weighted avg       0.98      0.98      0.98    860000\n",
            "\n",
            "Epoch: 29/30..  Test Accuracy: 0.983 Loss: 0.059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     86256\n",
            "           1       0.99      0.99      0.99     99298\n",
            "           2       0.98      0.98      0.98     89639\n",
            "           3       0.98      0.98      0.98     88434\n",
            "           4       0.98      0.98      0.98     85398\n",
            "           5       0.98      0.98      0.98     77419\n",
            "           6       0.98      0.99      0.98     82756\n",
            "           7       0.98      0.98      0.98     89217\n",
            "           8       0.97      0.98      0.98     83882\n",
            "           9       0.97      0.97      0.97     87701\n",
            "\n",
            "    accuracy                           0.98    870000\n",
            "   macro avg       0.98      0.98      0.98    870000\n",
            "weighted avg       0.98      0.98      0.98    870000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.983 Loss: 0.063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     87246\n",
            "           1       0.99      0.99      0.99    100441\n",
            "           2       0.98      0.98      0.98     90676\n",
            "           3       0.98      0.98      0.98     89441\n",
            "           4       0.98      0.98      0.98     86377\n",
            "           5       0.98      0.98      0.98     78306\n",
            "           6       0.98      0.99      0.98     83711\n",
            "           7       0.98      0.98      0.98     90242\n",
            "           8       0.97      0.98      0.98     84850\n",
            "           9       0.97      0.97      0.97     88710\n",
            "\n",
            "    accuracy                           0.98    880000\n",
            "   macro avg       0.98      0.98      0.98    880000\n",
            "weighted avg       0.98      0.98      0.98    880000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.984 Loss: 0.062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     88238\n",
            "           1       0.99      0.99      0.99    101574\n",
            "           2       0.98      0.98      0.98     91703\n",
            "           3       0.98      0.98      0.98     90463\n",
            "           4       0.98      0.98      0.98     87361\n",
            "           5       0.98      0.98      0.98     79192\n",
            "           6       0.98      0.99      0.98     84665\n",
            "           7       0.98      0.98      0.98     91277\n",
            "           8       0.97      0.98      0.98     85811\n",
            "           9       0.97      0.97      0.97     89716\n",
            "\n",
            "    accuracy                           0.98    890000\n",
            "   macro avg       0.98      0.98      0.98    890000\n",
            "weighted avg       0.98      0.98      0.98    890000\n",
            "\n",
            "Epoch: 30/30..  Test Accuracy: 0.982 Loss: 0.066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99     89233\n",
            "           1       0.99      0.99      0.99    102715\n",
            "           2       0.98      0.98      0.98     92730\n",
            "           3       0.98      0.98      0.98     91476\n",
            "           4       0.98      0.98      0.98     88342\n",
            "           5       0.98      0.98      0.98     80090\n",
            "           6       0.98      0.99      0.98     85616\n",
            "           7       0.98      0.98      0.98     92307\n",
            "           8       0.97      0.98      0.98     86778\n",
            "           9       0.97      0.97      0.97     90713\n",
            "\n",
            "    accuracy                           0.98    900000\n",
            "   macro avg       0.98      0.98      0.98    900000\n",
            "weighted avg       0.98      0.98      0.98    900000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выводы\n",
        "![image.png](data:image/png;base64,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)\n",
        "В первом номере не использовались dropout и batchnorm, в 7 и далее номерах я изменил количество input И output layers на 128 и 64 соответственно. Во время исследования были испробованы 9 различных моделей с различными видами оптимизаторов, архитектур, значений dropout и количеств параметров. Наилучший результат показал 9 номер с Relu+Relu, dropout 0.3 и оптимизатором Adam. Значения Accuracy и Precision - 0.984 и 0.062 соответственно. Остальные метрики имеют значения 0.98. Это наилучшая получившаяся модель нейронной сети с двумя скрытыми слоями. Табличка Эксель есть в файлах"
      ],
      "metadata": {
        "id": "BrKXGylitlOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я считаю, что эта архитектура стала наилучшей, потому что в ней есть ряд преимуществ. Первое это наличие batchnorm, что нормализует и стабилизирует процесс обучения. Второе это наличие dropout с значением 0.3, так как оно предотвращает переобучение. Третье это количество скрытых слоев - 2, конечно можно сделать их больше и тем самым сделать модель более сложной, однако даже два слоя отлично справляются с этой задачей. Четвертое это функция активации LeakyReLU, потому что она избегает появление мертвых нейронов за счет того, что LeakyReLu допускает небольшой градиент когда нейрон не активен. Ненулевой наклон для отрицательных входных данных в некоторых случаях может привести к более быстрой сходимости, поскольку позволяет избежать проблемы нулевого градиента для отрицательных входных данных. В заключении, это задача отлично справляется с задачей датасета MNIST. Конечно, она еще далека от идеала, но имеющиеся данные(например акураси) довольно хороши."
      ],
      "metadata": {
        "id": "RUMrQoGi0AYL"
      }
    }
  ]
}